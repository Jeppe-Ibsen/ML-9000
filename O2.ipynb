{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Supergruppe diskussion\n",
    "\n",
    "\n",
    "# § 2 \"End-to-End Machine Learning Project\" [HOML]\n",
    "\n",
    "Genlæs kapitel  § 2 og forbered mundtlig præsentation.\n",
    "\n",
    "# Forberedelse inden lektionen\n",
    "\n",
    "Een eller flere af gruppe medlemmer forbereder et mundtligt resume af § 2:\n",
    "\n",
    "* i skal kunne give et kort mundligt resume af hele § 2 til en anden gruppe (på nær, som nævnt, Create the Workspace og Download the Data),\n",
    "\n",
    "* resume holdes til koncept-plan, dvs. prøv at genfortælle, hvad de overordnede linier er i kapitlerne i [HOML].\n",
    "\n",
    "Lav et kort skriftlig resume af de enkelte underafsnit, ca. 5 til 20 liners tekst, se \"TODO\"-template herunder (MUST, til O2 aflevering).\n",
    "\n",
    "Kapitler (incl. underkapitler):\n",
    "\n",
    "* _Look at the Big Picture,_\n",
    "* _Get the Data,_\n",
    "* _Explore and Visualize the Data to Gain Insights,_ \n",
    "* _Prepare the Data for Machine Learning Algorithms,_\n",
    "* _Select and Train a Model,_\n",
    "* _Fine-Tune Your Model,_\n",
    "* _Launch, Monitor, and Maintain Your System,_\n",
    "* _Try It Out!._\n",
    "\n",
    "# På klassen\n",
    "\n",
    "Supergruppe [SG] resume af § 2 End-to-End, ca. 30 til 45 min.\n",
    "\n",
    "* en supergruppe [SG], sammensættes af to grupper [G], on-the-fly på klassen,\n",
    "\n",
    "* hver gruppe [G] forbereder og giver en anden gruppe [G] et mundtligt resume af § 2 til en anden gruppe,\n",
    "\n",
    "* tid: ca. 30 mim. sammenlagt, den ene grupper genfortæller første halvdel af § 2 i ca. 15 min., hvorefter den anden gruppe genfortæller resten i ca. 15 min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume: Look at the Big Picture\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Get the Data\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Explore and Visualize the Data to Gain Insights,\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Select and Train a Model\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Fine-Tune Your Model\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Launch, Monitor, and Maintain Your System\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Try It Out!.\n",
    "\n",
    "TODO resume.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    "---------||\n",
    "2019-01-28| CEF, initial.\n",
    "2020-02-05| CEF, F20 ITMAL update.\n",
    "2021-08-17| CEF, E21 ITMAL update.\n",
    "2021-09-17| CEF, corrected some spell errors.\n",
    "2022-01-28| CEF, update to F22 SWMAL.\n",
    "2022-09-09| CEF, corrected 'MUST for O1' to 'MUST for O2' in text.\n",
    "2023-02-13| CEF, updated to HOML 3rd, removed exclude subsections in 'Get the Data' in this excercise, since the parts with python environments has been removed in HOML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SWMAL Opgave\n",
    "\n",
    "# Dataanalyse\n",
    "\n",
    "## Qa) Beskrivelse af datasæt til O4 projekt\n",
    "\n",
    "I kurset er slutprojektet et bærende element, som I forventes at arbejde på igennem hele kurset\n",
    "sideløbende med de forskellige undervisningsemner. \n",
    "\n",
    "I skal selv vælge et O4 projekt–det anbefales at I vælger en problemstilling, hvor der allerede er data til rådighed og en god beskrivelse af data, dataopsamlingsmetode og problemstilling.\n",
    "\n",
    "I denne opgave skal I:\n",
    "\n",
    "* a) Give en kort konceptmæssig projektbeskrivelse af Jeres ide til O4 projekt. \n",
    "\n",
    "* b) Beskrive jeres valgte datasæt med en kort forklaring af baggrund og hvor I har fået data fra.\n",
    "\n",
    "* c) Beskrive data–dvs. hvilke features, antal samples, target værdier, evt. fejl/usikkerheder, etc.\n",
    "\n",
    "* d) Forklare hvordan I ønsker at anvende datasættet – vil I fx. bruge det til at prædiktere noget\n",
    "bestemt, lave en regression eller klassifikation, el.lign. \n",
    "\n",
    "I vil nok komme til at anvende data også på andre måder i løbet af undervisningen – men det behøver I ikke nævne. Og det er også ok, hvis I ender med at bruge data på en anden måde end planlagt her.\n",
    "\n",
    "Omfang af beskrivelsen forventes at være 1-2 sider.\n",
    "\n",
    "\n",
    "## Qb) Dataanalyse af eget datasæt\n",
    "\n",
    "Lav data analyse på jeres egne data og projekt.\n",
    "\n",
    "Det indebærer de sædvanlige elementer såsom plotte histogrammer, middelværdi/median/spredning, analysere for outliers/korrupte data, forslag til skalering af data og lignende former for analyse af data.\n",
    "\n",
    "For nogle typer data (fx billed-data), hvor features ikke har en specifik betydning, er det mest\n",
    "histogrammer og lignende, som giver mening – det er helt o.k. \n",
    "\n",
    "\n",
    "## NOTE vdr. billeddatasæts\n",
    "\n",
    "For billeddata fer hver pixel en feature, og alm. analyse beskrevet ovenfor giver ikke indsigt. Prøv i stedet for billeder at beskrive billedformater (JPEG, PNG osv. / RGB, HSV, gråtone, multispektral, etc.), størrelser af billeder, hvordan de er repræsenteret på disk (dirs osv.)\n",
    "\n",
    "Giv også eksempler på billeder og evt. labels i billedesæt.\n",
    "\n",
    "Histogrammer kan udføres på enkelte billeder, men kun i forbindelse med labelede områder---og bedst på billesæt med ens baggrunde.\n",
    "\n",
    "Benytter i lyddata eller video gælder de samme begrænsinger som får billeder her.\n",
    "\n",
    "## NOTE vdr. valg af datasæt til O4\n",
    "\n",
    "I har frie hænder til at vælge O4 projekt og tilhørende datasæt og valg af datasæt og ide til O4 her er ikke endelig. \n",
    "\n",
    "Dvs. at i løbende kan modificere projektbeskrivelse og, evt. om nødvendigt, vælge et andet datasæt senere, hvis jeres nuværende valg viser sig umuligt (men er en dyr proces). \n",
    "\n",
    "Scope af O4 projekt bør også begrænses, så det passer til kurset og til den '_time-box_'ede aflevering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-|\n",
    "2021-08-17| CEF, moved from Word to Notebook.\n",
    "2021-11-08| CEF, elaborated on image based data.\n",
    "2022-01-25| CEF, update to F22 SWMAL.\n",
    "2023-02-19| CEF, updated to F23 SWMAL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "# Pipelines\n",
    "\n",
    "We now try building af ML pipeline. The data for this exercise is the same as in L01, meaning that the OECD data from the 'intro.ipynb' have been save into a Python 'pickle' file. \n",
    "\n",
    "The pickle library is a nifty data preservation method in Python, and from L01 the tuple `(X, y)` have been stored to the pickle file `itmal_l01_data.pkl', try reloading it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def LoadDataFromL01():\n",
    "    filename = \"Data/itmal_l01_data.pkl\"\n",
    "    with open(f\"{filename}\", \"rb\") as f:\n",
    "        (X, y) = pickle.load(f)\n",
    "        return X, y\n",
    "\n",
    "X, y = LoadDataFromL01()\n",
    "\n",
    "print(f\"X.shape={X.shape},  y.shape={y.shape}\")\n",
    "\n",
    "assert X.shape[0] == y.shape[0]\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1  # did a y.ravel() before saving to picke file\n",
    "assert X.shape[0] == 29\n",
    "\n",
    "# re-create plot data (not stored in the Pickel file)\n",
    "m = np.linspace(0, 60000, 1000)\n",
    "M = np.empty([m.shape[0], 1])\n",
    "M[:, 0] = m\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Revisiting the problem with the MLP\n",
    "\n",
    "Using the MLP for the QECD data in Qd) from `intro.ipynb` produced a negative $R^2$, meaning that it was unable to fit the data, and the MPL model was actually _worse_ than the naive $\\hat y$ (mean value of y).\n",
    "\n",
    "Let's just revisit this fact. When running the next cell you should now see an OK $~R^2_{lin.reg}~$ score and a negative $~R^2_{mlp}~$ score.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the MLP and lin. regression again..\n",
    "\n",
    "def isNumpyData(t: np.ndarray, expected_ndim: int):\n",
    "    assert isinstance(expected_ndim, int), f\"input parameter 'expected_ndim' is not an integer but a '{type(expected_ndim)}'\"\n",
    "    assert expected_ndim>=0, f\"expected input parameter 'expected_ndim' to be >=0, got {expected_ndim}\"\n",
    "    if t is None:\n",
    "        print(\"input parameter 't' is None\", file=sys.stderr)\n",
    "        return False\n",
    "    if not isinstance(t, np.ndarray):\n",
    "        print(\"excepted numpy.ndarray got type '{type(t)}'\", file=sys.stderr)\n",
    "        return False\n",
    "    if not t.ndim==expected_ndim:\n",
    "        print(\"expected ndim={expected_ndim} but found {t.ndim}\", file=sys.stderr)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def PlotModels(model1, model2, X: np.ndarray, y: np.ndarray, name_model1: str, name_model2: str):\n",
    "    \n",
    "    # NOTE: local function is such a nifty feature of Python!\n",
    "    def CalcPredAndScore(model, X: np.ndarray, y: np.ndarray,):\n",
    "        assert isNumpyData(X, 2) and isNumpyData(y, 1) and X.shape[0]==y.shape[0]\n",
    "        y_pred_model = model.predict(X)\n",
    "        score_model = r2_score(y, y_pred_model) # call r2\n",
    "        return y_pred_model, score_model    \n",
    "\n",
    "    assert isinstance(name_model1, str) and isinstance(name_model2, str)\n",
    "\n",
    "    y_pred_model1, score_model1 = CalcPredAndScore(model1, X, y)\n",
    "    y_pred_model2, score_model2 = CalcPredAndScore(model2, X, y)\n",
    "\n",
    "    plt.plot(X, y_pred_model1, \"r.-\")\n",
    "    plt.plot(X, y_pred_model2, \"kx-\")\n",
    "    plt.scatter(X, y)\n",
    "    plt.xlabel(\"GDP per capita\")\n",
    "    plt.ylabel(\"Life satisfaction\")\n",
    "    plt.legend([name_model1, name_model2, \"X OECD data\"])\n",
    "\n",
    "    l = max(len(name_model1), len(name_model2))\n",
    "    \n",
    "    print(f\"{(name_model1).rjust(l)}.score(X, y)={score_model1:0.2f}\")\n",
    "    print(f\"{(name_model2).rjust(l)}.score(X, y)={score_model2:0.2f}\")\n",
    "\n",
    "# lets make a linear and MLP regressor and redo the plots\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10, ),\n",
    "                   solver='adam',\n",
    "                   activation='relu',\n",
    "                   tol=1E-5,\n",
    "                   max_iter=100000,\n",
    "                   verbose=False)\n",
    "linreg = LinearRegression()\n",
    "\n",
    "mlp.fit(X, y)\n",
    "linreg.fit(X, y)\n",
    "\n",
    "print(\"The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\\n\")\n",
    "PlotModels(linreg, mlp, X, y, \"lin.reg\", \"MLP\")\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qa) Create a Min/max scaler for the MLP\n",
    "\n",
    "Now, the neurons in neural networks normally expect input data in the range `[0;1]` or sometimes in the range `[-1;1]`, meaning that for value outside this range then the neuron will saturate to its min or max value (also typical `0` or `1`). \n",
    "\n",
    "A concrete value of `X` is, say 22.000 USD, that is far away from what the MLP expects. Af fix to the problem in Qd), from `intro.ipynb`, is to preprocess data by scaling it down to something more sensible.\n",
    "\n",
    "Try to manually scale X to a range of `[0;1]`, re-train the MLP, re-plot and find the new score from the rescaled input. Any better?\n",
    "\n",
    "(If you already made exercise \"Qe) Neural Network with pre-scaling\" in L01, then reuse Your work here!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: rescale X and refit the model(s)..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb) Scikit-learn Pipelines\n",
    "\n",
    "Now, rescale again, but use the `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "When this works put both the MLP and the scaler into a composite construction via `sklearn.pipeline.Pipeline`. This composite is just a new Scikit-learn estimator, and can be used just like any other `fit-predict` models, try it, and document it for the journal.\n",
    "\n",
    "(You could reuse the `PlotModels()` function by also retraining the linear regressor on the scaled data, or just write your own plot code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: put everything into a pipeline..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc) Outliers and the Min-max Scaler vs. the Standard Scaler\n",
    "\n",
    "Explain the fundamental problem with a min-max scaler and outliers. \n",
    "\n",
    "Will a `sklearn.preprocessing.StandardScaler` do better here, in the case of abnormal feature values/outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: research the problem here..\n",
    "assert False, \"TODO: investigate outlier problems and try a StandardScaler..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd) Modify the MLP Hyperparameters\n",
    "\n",
    "Finally, try out some of the hyperparameters associated with the MLP.\n",
    "\n",
    "Specifically, test how few neurons the MLP can do with---still producing a sensible output, i.e. high $R^2$. \n",
    "\n",
    "Also try-out some other activation functions, ala sigmoid, and solvers, like `sgd`.\n",
    "\n",
    "Notice, that the Scikit-learn MLP does not have as many adjustable parameters, as a Keras MLP, for example, the Scikit-learn MLP misses neurons initialization parameters (p.333-334 [HOML,2nd], p.358-359 [HOML,3rd]) and the ELU activation function (p.336 [HOML,2nd], p.363 [HOML,3rd).\n",
    "\n",
    "[OPTIONAL 1]: use a Keras MLP regressor instead of the Scikit-learn MLP (You need to install the  Keras if its not installed as default).\n",
    "\n",
    "[OPTIONAL 2]: try out the `early_stopping` hyperparameter on the `MLPRegressor`. \n",
    "\n",
    "[OPTIONAL 3]: try putting all score-calculations into K-fold cross-validation  methods readily available in Scikit-learn using\n",
    "\n",
    "* `sklearn.model_selection.cross_val_predict`\n",
    "* `sklearn.model_selection.cross_val_score` \n",
    "\n",
    "or similar (this is, in theory, the correct method, but can be hard to use due to the  extremely small number of data points, `n=29`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: test out various hyperparameters for the MLP..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-|\n",
    "2020-10-15| CEF, initial. \n",
    "2020-10-21| CEF, added Standard Scaler Q.\n",
    "2020-11-17| CEF, removed orhpant text in Qa (moded to Qc).\n",
    "2021-02-10| CEF, updated for ITMAL F21.\n",
    "2021-11-08| CEF, updated print info.\n",
    "2021-02-10| CEF, updated for SWMAL F22.\n",
    "2023-02-19| CEF, updated for SWMAL F23, adjuste page numbers for 3rd.ed.\n",
    "2023-02-21| CEF, added types, rewrote CalcPredAndScore and added isNumpyData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Training a Linear Regressor I \n",
    "\n",
    "The goal of the linear regression is to find the argument $w$ that minimizes the sum-of-squares error over all inputs. \n",
    "\n",
    "Given the usual ML input data matrix $\\mathbf X$ of size $(n,d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "    \\renewcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\renewcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\renewcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\renewcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\renewcommand\\st[1]{_{\\textrm{\\scriptsize #1}}}\n",
    "    \\renewcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\renewcommand\\obs[2]{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\renewcommand\\diff[1]{\\mathrm{d}#1}\n",
    "    \\renewcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\renewcommand\\pfrac[2]{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\renewcommand\\dfrac[2]{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "            \\vdots      &             &        & \\vdots \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "        }\n",
    "$$\n",
    "\n",
    "and $\\by$ is the target output column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\by =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1} \\\\\n",
    "     y\\pown{2} \\\\\n",
    "     \\vdots \\\\\n",
    "     y\\pown{n} \\\\\n",
    "  }\n",
    "$$\n",
    "\n",
    "The linear regression model, via its hypothesis function and for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$ (with the additional element $w_0$ being the bias), can now be written as simple as\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  h(\\bx\\powni;\\bw) &= \\bw^\\top \\ac{c}{1\\\\\\bx\\powni} \\\\\n",
    "                   &= w_0 + w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni\n",
    "}\n",
    "$$\n",
    "\n",
    "using the model parameters or weights, $\\bw$, aka $\\btheta$. To ease notation $\\bx$ is assumed to have the 1 element prepended in the following so that $\\bx$ is a $d+1$ column vector\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} &\\mapsto \\bx\\powni, ~~~~\\textrm{by convention in the following...}\\\\\n",
    "  h(\\bx\\powni;\\bw) &= \\bw^\\top \\bx\\powni \n",
    "}\n",
    "$$\n",
    "\n",
    "This is actually the first fully white-box machine learning algorithm, that we see. All the glory details of the algorithm are clearly visible in the internal vector multiplication...quite simple, right? Now we just need to train the weights...\n",
    " \n",
    "## Loss or Objective Function - Formulation for Linear Regression\n",
    "\n",
    "The individual cost (or loss), $L\\powni$, for a single input-vector $\\bx\\powni$ is a measure of how the model is able to fit the data: the higher the $L\\powni$ value the worse it is able to fit. A loss of $L=0$ means a perfect fit.\n",
    "\n",
    "It can be given by, say, the square difference from the calculated output, $h$, to the desired output, $y$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || h(\\bx\\powni;\\bw)      - y\\powni ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - y\\powni ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - y\\powni \\right)^2 \n",
    "}\n",
    "$$\n",
    "when $L$ is based on the $\\norm{2}^2$ norm, and only when $y$ is in one dimension.\n",
    "\n",
    "To minimize all the $L\\powni$ losses (or indirectly also the MSE or RMSE) is to minimize the sum of all the\n",
    "individual costs, via the total cost function $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "\\textrm{MSE}(\\bX,\\by;\\bw) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                    &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni \\right)^2\\\\\n",
    "                    &= \\frac{1}{n} ||\\bX \\bw - \\by||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "here using the squared Euclidean norm, $\\norm{2}^2$, via the $||\\cdot||_2^2$ expressions.\n",
    "\n",
    "Now the factor $\\frac{1}{n}$ is just a constant and can be ignored, yielding the total cost function\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    J &= \\frac{1}{2} ||\\bX \\bw - \\by||_2^2\\\\\n",
    "     &\\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "adding yet another constant, 1/2, to ease later differentiation of $J$.\n",
    "\n",
    "## Training\n",
    "\n",
    "Training the linear regression model now amounts to computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total cost\n",
    "\n",
    "$$\n",
    " \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "where $\\textrm{argmin}_\\bw$ means find the argument of $\\bw$ that minimizes the $J$ function. This minimum (sometimes a maximum, via argmax) is denoted $\\bw^*$ in most ML literature. \n",
    "\n",
    "The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always form a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from server.\" style=\"height:240px\">\n",
    "\n",
    "## Training: The Closed-form Solution\n",
    "\n",
    "To solve for $\\bw^*$ in closed form (i.e. directly, without any numerical approximation), we find the gradient of $J$ with respect to $\\bw$. Taking the partial deriverty $\\partial/\\partial_\\bw$ of the $J$ via the gradient (nabla) operator\n",
    "\n",
    "$$\n",
    "\\rem{\n",
    " \\frac{\\partial}{\\partial \\bw} = \n",
    "   \\ac{c}{\n",
    "     \\frac{\\partial}{\\partial w_1} \\\\\n",
    "     \\frac{\\partial}{\\partial w_2} \\\\\n",
    "     \\vdots\\\\\n",
    "     \\frac{\\partial}{\\partial w_d}\n",
    "   }\n",
    "}    \n",
    " \\nabla_\\bw~J = \n",
    "   \\left[ \\frac{\\partial J}{\\partial w_1}, \\frac{\\partial J}{\\partial w_2}, \\ldots ,  \\frac{\\partial J}{\\partial w_m}   \\right]^\\top\n",
    "$$\n",
    "     \n",
    "and setting it to zero yields the optimal solution for $\\bw$, and ignoring all constant factors of 1/2\n",
    "and $1/n$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J(\\bw) &= \\bX^\\top \\left( \\bX \\bw - \\by \\right) ~=~ 0\\\\\n",
    "                  0 &= \\bX^\\top\\bX \\bw - \\bX^\\top\\by\n",
    "}\n",
    "$$\n",
    "\n",
    "giving the closed-form solution, with $\\by = [y\\pown{1}, y\\pown{2}, \\cdots,\n",
    "y\\pown{n}]^\\top$\n",
    "\n",
    "$$\n",
    "\\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1} \\bX^\\top \\by\n",
    "$$\n",
    "\n",
    "You already know this method from math, finding the extrema for a function, say \n",
    "\n",
    "$$f(w)=w^2-2w-2$$ \n",
    "\n",
    "so is given by finding the place where the gradient $\\mathrm{d}~f(w)/\\mathrm{d}w = 0$\n",
    "\n",
    "$$\n",
    "   \\dfrac{f(w)}{w} = 2w -2 = 0\n",
    "$$\n",
    "\n",
    "so we see that there is an extremum at $w=1$. Checking the second deriverty tells if we are seeing a minimum, maximum or a saddlepoint at that point. In matrix terms, this corresponds to finding the _Hessian_ matrix and gets notational tricky due to the multiple feature dimensions involved.\n",
    "\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Ordinary_least_squares\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Hessian_matrix\n",
    "\n",
    "## Qa Write a Python function that uses the closed-form to find $\\bw^*$\n",
    "\n",
    "Use the test data, `X` and `y` in the code below to find `w` via the closed-form. Use the test vectors for `w` to test your implementation, and remember to add the bias term (concat an all-one vector to `X` before solving). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa...\n",
    "\n",
    "# TEST DATA: \n",
    "import numpy as np\n",
    "from libitmal import utils as itmalutils\n",
    "\n",
    "def GenerateData():\n",
    "    X = np.array([[8.34044009e-01],[1.44064899e+00],[2.28749635e-04],[6.04665145e-01]])\n",
    "    y = np.array([5.97396028, 7.24897834, 4.86609388, 3.51245674])\n",
    "    return X, y\n",
    "\n",
    "X, y = GenerateData()\n",
    "assert False, \"find the least-square solution for X and y, your implementation here, say from [HOML, p.114 2nd./p.134 3rd.]\"\n",
    "\n",
    "# w = ...\n",
    "\n",
    "\n",
    "# TEST VECTOR:\n",
    "w_expected = np.array([4.046879011698, 1.880121487278])\n",
    "itmalutils.PrintMatrix(w, label=\"w=\", precision=12)\n",
    "itmalutils.AssertInRange(w, w_expected, eps=1E-9)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb Find the limits of the least-square method\n",
    "\n",
    "Again find the least-square optimal value for `w` now using then new `X` and `y` as inputs.\n",
    "\n",
    "Describe the problem with the matrix inverse, and for what `M` and `N` combinations do you see, that calculation of the matrix inverse takes up long time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb...\n",
    "def GenerateData(M, N):\n",
    "    # TEST DATA: Matrix, taken from [HOML]\n",
    "    print(f'GenerateData(M={N}, N={N})...')\n",
    "    \n",
    "    assert M>0\n",
    "    assert N>0\n",
    "    assert isinstance(M, int)\n",
    "    assert isinstance(N, int)\n",
    "\n",
    "    # NOTE: not always possible to invert a random matrix; \n",
    "    #       it becomes sigular, hence a more elaborate choice \n",
    "    #       of values below (but still a hack): \n",
    "    X=2 * np.ones([M, N])\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i,0]=i*4\n",
    "    for j in range(X.shape[1]):\n",
    "        X[0,j]=-j*4\n",
    "\n",
    "    y=4 + 3*X + np.random.randn(M,1)\n",
    "    y=y[:,0] # well, could do better here!\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = GenerateData(M=10000, N=20)\n",
    "assert False, \"find the least-square solution for X and y, again\"\n",
    "\n",
    "# w = \n",
    "\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2018-12-18| CEF, initial.\n",
    "2018-02-14| CEF, major update.\n",
    "2018-02-18| CEF, fixed error in nabla expression.\n",
    "2018-02-18| CEF, added minimization plot.\n",
    "2018-02-18| CEF, added note on argmin/max.\n",
    "2018-02-18| CEF, changed concave to convex.\n",
    "2021-09-26| CEF, update for ITMAL E21.\n",
    "2011-10-02| CEF, corrected page numbers for HOML v2 (109=>114).\n",
    "2022-03-09| CEF, elaboreted on code and introduced GetData().\n",
    "2023-02-22| CEF, updated page no to HOML 3rd. ed., updated to SWMAL F23.\n",
    "2023-09-19| CEF, changed LaTeX mbox and newcommand (VSCode error) to textrm/mathrm and renewcommand.\n",
    "2023-09-28| CEF, elaborated on L-expressions from (..)^2 to \\|\\| .. \\|\\|_2^2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Gradient Descent Methods and Training\n",
    "\n",
    "Finding the optimal solution in one-step, via \n",
    "\n",
    "$$\n",
    "    \\renewcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\renewcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\renewcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\renewcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\renewcommand\\st[1]{_{\\textrm{\\scriptsize #1}}}\n",
    "    \\renewcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\renewcommand\\obs[2]{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\renewcommand\\diff[1]{\\mathrm{d}#1}\n",
    "    \\renewcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\renewcommand\\pfrac[2]{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\renewcommand\\dfrac[2]{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "\\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1} \\bX^\\top \\by\n",
    "$$\n",
    "\n",
    "has its downsides: the scaling problem of the matrix inverse. Now, let us look at a numerical solution to the problem of finding the value of $\\bw$  (aka $\\btheta$) that minimizes the objective function $J$.\n",
    "\n",
    "Again, ideally we just want to find places, where the (multi-dimensionally) gradient of $J$ is zero (here using a constant factor $\\frac{2}{m}$)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J(\\bw) &= \\frac{2}{m} \\bX^\\top \\left( \\bX \\bw - \\by \\right)\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "and numerically we calculate $\\nabla_{\\bw} J$ for a point in $\\bw$-space, and then move along in the opposite direction of this gradient, taking a step of size $\\eta$\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)} = \\bw^{(step~N)} - \\eta \\nabla_{\\bw} J(\\bw)\n",
    "$$\n",
    "\n",
    "That's it, pretty simple, right (apart from numerical stability, problem with convergence and regularization, that we will discuss later).\n",
    "\n",
    "So, we begin with some initial $\\bw$, and iterate via the equation above, towards places, where $J$ is smaller, and this can be illustrated as\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from server.\" style=\"height:240px\">\n",
    "\n",
    "\n",
    "If we hit the/a global minimum or just a local minimum (or in extremely rare cases a local saddle point) is another question when not using a simple linear regression model: for non-linear models we will in general not see a nice convex $J$-$\\bw$ surface, as in the figure above.\n",
    "\n",
    "## Qa The Gradient Descent Method (GD)\n",
    "\n",
    "Explain the gradient descent algorithm using the equations in the section _'§ Bach Gradient Descent'_ [HOML, p.121-122 2nd, p.143 3rd], and relate it to the code snippet \n",
    "\n",
    "```python\n",
    "X_b, y = GenerateData()\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "``` \n",
    "in the python code below. \n",
    "\n",
    "As usual, avoid going top much into details of the code that does the plotting.\n",
    "\n",
    "What role does `eta` play, and what happens if you increase/decrease it (explain the three plots)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa...examine the method (without the plotting)\n",
    "\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def GenerateData():\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "    X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "    return X, X_b, y\n",
    "\n",
    "X, X_b, y = GenerateData()\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "print(f'stochastic gradient descent theta={theta.ravel()}')\n",
    "\n",
    "#############################\n",
    "# rest of the code is just for plotting, needs no review\n",
    "\n",
    "def plot_gradient_descent(theta, eta, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        \n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta_path_bgd = []\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb The Stochastic Gradient Descent Method (SGD)\n",
    "\n",
    "Now, introducing the _stochastic_ variant of gradient descent, explain the stochastic nature of the SGD, and comment on the difference to the _normal_ gradient descent method (GD) we just saw.\n",
    "\n",
    "Also explain the role of the calls to `np.random.randint()` in the code, \n",
    "\n",
    "HINT: In detail, the important differences are, that the main loop for SGC is \n",
    "```python\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = ...\n",
    "        theta = ...\n",
    "```\n",
    "where it for the GD method was just\n",
    "```python\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = ..\n",
    "```\n",
    "\n",
    "NOTE: the call `np.random.seed(42)` resets the random generator so that it produces the same random-sequence when re-running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb...run this code\n",
    "\n",
    "# NOTE: code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "theta_path_sgd = []\n",
    "m = len(X_b)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        if epoch == 0 and i < 20:\n",
    "            y_predict = X_new_b.dot(theta) \n",
    "            style = \"b-\" if i > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        \n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)        \n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)                 \n",
    "\n",
    "        plt.plot(X, y, \"b.\")      \n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "print(f'stochastic gradient descent theta={theta.ravel()}')\n",
    "print(f'Scikit-learn SGDRegressor \"thetas\": sgd_reg.intercept_={sgd_reg.intercept_}, sgd_reg.coef_={sgd_reg.coef_}')\n",
    "\n",
    "#############################\n",
    "# rest of the code is just for plotting, needs no review \n",
    "plt.xlabel(\"$x_1$\", fontsize=18)                     \n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)           \n",
    "plt.axis([0, 2, 0, 15])                              \n",
    "\n",
    "plt.show()        \n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc Adaptive learning rate for $\\eta$  \n",
    "\n",
    "There is also an adaptive learning rate method in the demo code for the SGD. \n",
    "\n",
    "Explain the effects of the `learning_schedule()` functions.\n",
    "\n",
    "You can set the learning rate parameter (also known as a hyperparameter) in may ML algorithms, say for SGD regression, to a method of your choice \n",
    "\n",
    "```python\n",
    "SGDRegressor(max_iter=1,\n",
    "             eta0=0.0005,\n",
    "             learning_rate=\"constant\", # or 'adaptive' etc.\n",
    "             random_state=42)\n",
    "```\n",
    "\n",
    "but as usual, there is a bewildering array of possibilities...we will tackle this problem later when searching for the optimal hyperparameters.\n",
    "\n",
    "NOTE: the `learning_schedule()` method could also have been used in the normal SG algorithm; is not directly part of the stochastic method, but a concept in itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc...in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd Mini-batch Gradient Descent Method \n",
    "\n",
    "Finally explain what a __mini-batch__ SG method is, and how it differs from the two others.\n",
    "\n",
    "Again, take a peek into the demo code below, to extract the algorithm details...and explain the __main differences__, compared with the GD and SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd...run this code\n",
    "\n",
    "# NOTE: code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "theta_path_mgd = []\n",
    "\n",
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = 0\n",
    "for epoch in range(n_iterations):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        t += 1\n",
    "        xi = X_b_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        \n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_mgd.append(theta)\n",
    "\n",
    "print(f'mini-batch theta={theta.ravel()}')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qe Choosing a Gradient Descent Method\n",
    "\n",
    "Explain the $θ_0−θ_1$ plot below, and make a comment on when to use GD/SGD/mini-batch gradient descent (pros and cons for the different methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd...run this code\n",
    "\n",
    "# NOTE: code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "theta_path_bgd = np.array(theta_path_bgd)\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "theta_path_mgd = np.array(theta_path_mgd)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
    "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
    "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
    "plt.legend(loc=\"upper left\", fontsize=16)\n",
    "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
    "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
    "plt.show()\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL]  Qf Extend the MyRegressor \n",
    "\n",
    "NOTE: this excercise only possible if `linear_regression_2.ipynb` has been solved.\n",
    "\n",
    "Can you extend the `MyRegressor` class from the previous `linear_regression_2.ipynb` notebook, adding a numerical train method? Choose one of the gradient descent methods above...perhaps starting with a plain SG method.\n",
    "\n",
    "You could add a parameter for the class, indicating it what mode it should be operating: analytical closed-form or numerical, like\n",
    "\n",
    "```python  \n",
    "class MyRegressor(BaseEstimator):\n",
    "    def __init__(self, numerical = False):\n",
    "        self.__w = None\n",
    "        self.__numerical_mode = numerical\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qf...[OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2018-02-14| CEF, initial.\n",
    "2018-02-14| CEF, added optional exe.\n",
    "2018-02-20| CEF, major update.\n",
    "2018-02-20| CEF, fixed revision table malformatting.\n",
    "2018-02-25| CEF, removed =0 in expression.\n",
    "2018-02-25| CEF, minor text updates and made Qc optional.\n",
    "2018-02-25| CEF, minor source code cleanups.\n",
    "2021-09-18| CEF, update to ITMAL E21.\n",
    "2021-10-02| CEF, corrected link to extra material and page numbers for HOML v2 (114/115=>121/122).\n",
    "2022-01-25| CEF, update to SWMAL F22.\n",
    "2023-02-22| CEF, updated page no to HOML 3rd. ed., updated to SWMAL F23\n",
    "2023-09-19| CEF, changed LaTeX mbox and newcommand (VSCode error) to textrm/mathrm and renewcommand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "#  Artificial Neural Networks as Universal Approximators\n",
    "\n",
    "An ANN can in principle approximate any n-dimensional function: given enough neurons (and layers) a ANN is an _universal approximator_.\n",
    "\n",
    "Let us test this by using a very simple ANN consisting of only two neurons in a hidden layer(and an input- and output-layer both with the identity activation function, _I_ ).\n",
    "\n",
    "Given a `tanh` activation function in a neuron, it can only approximate something similar to this monotonic function, but applying two neurons in a pair, they should be able to approximate an up-hill-then-downhill non-monotonic function, which is a simple function with a single maximum. \n",
    "\n",
    "We use Scikit-learns `MLPRegressor` for this part of the exercise. Use the synthetic data, generated by the `GenerateSimpleData()` functions, in the next cells and train the MLP to make it fit the curve. \n",
    "\n",
    "Notice the lack of a train-test split in the exercise; since we only want to look at the approximation capabilities of the MLP, the train-test split is omitted, (and you are welcome to do the split yourself, and also to add noise in the data generators.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One data generator just to test out the MLP..\n",
    "#   An MLP with just two neurons should be able to approximate this simple\n",
    "#   down-up graph using its two non-linear sigmoid or tanh neurons...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def GenerateSimpleData():\n",
    "    X = np.linspace(-10, 10, 100)\n",
    "    y = 2*np.tanh(2*X - 12) - 3*np.tanh(2*X - 4)  \n",
    "    y = 2*np.tanh(2*X + 2)  - 3*np.tanh(2*X - 4)   \n",
    "    X = X.reshape(-1, 1) # Scikit-algorithms needs matrix in (:,1)-format\n",
    "    return X,y\n",
    "\n",
    "X, y_true = GenerateSimpleData()\n",
    "plt.plot(X, y_true, \"r-.\")\n",
    "plt.legend([\"y_true\"])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, Groundtruth data simple\")\n",
    "           \n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qa)\n",
    "\n",
    "Fit the model using the data generator and the MLP in the next cell. \n",
    "\n",
    "Then plot `y_true` and `y_pred` in a graph, and extract the network weights and bias coefficients (remember the `coefs_` and `intercepts_` attributes you found on a linear regressor in an earlier exercise, the MLP is similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP and fit model, just run..\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(activation = 'tanh',      # activation function \n",
    "                   hidden_layer_sizes = [2], # layes and neurons in layers: one hidden layer with two neurons\n",
    "                   alpha = 1e-5,             # regularization parameter\n",
    "                   solver = 'lbfgs',         # quasi-Newton solver\n",
    "                   max_iter=10000,\n",
    "                   verbose = True)\n",
    "\n",
    "mlp.fit(X, y_true)\n",
    "y_pred = mlp.predict(X)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the fit.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb)\n",
    "\n",
    "Draw the ANN with its input-, hidden- and output-layer. Remember the bias input to the input- and hidden-layer (a handmade drawing is fine).\n",
    "\n",
    "Now, add the seven weights extracted from the MLP attributes to the drawing: four w coefficients and three bias coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: extract and print all coefficients.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc)\n",
    "\n",
    "Create a mathematical formula for the network ala\n",
    "\n",
    "    y_math = 0.3* tanh(2 * x + 0.1) - 0.3 * tanh(5 * x + 3) + 0.9\n",
    "\n",
    "with the seven weights found before, two or three decimals should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create formula.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd)\n",
    "\n",
    "Plot the `y_math` function using `np.tanh` and `X` as input similar to  \n",
    "\n",
    "    y_math = 0.3*np.tanh(2 * X + ..\n",
    "   \n",
    "and compare `y_math` with `y_pred` and `y_true` in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the formula.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qe)\n",
    "\n",
    "Plot the first half of the function ala\n",
    "\n",
    "    y_math_first_part = 0.3* tanh(2 * X + 0.1)\n",
    "   \n",
    "and then plot the second part. The sum of these two parts gives the total value of y_math if you also add them with the last bias part.\n",
    "\n",
    "Are the first and second parts similar to a monotonic tanh activation function, and explain the ability of the two-neuron network to be a general approximator for the input function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the first and second half of the formula.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qf)\n",
    "\n",
    "Now we change the data generator to a `sinc`-like function, which is a function that needs a NN with a higher capacity than the previous simple data.\n",
    "\n",
    "Extend the MLP with more neurons and more layers, and plot the result. Can you create a good approximation for the `sinc` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateSincData():\n",
    "    # A Sinc curve, approximation needs more neurons to capture the 'ringing'...\n",
    "    X = np.linspace(-3, 3, 1000) \n",
    "    y = np.sinc(X)\n",
    "    X = X.reshape(-1,1)\n",
    "    return X, y\n",
    "\n",
    "X, y_true = GenerateSincData()\n",
    "plt.plot(X, y_true, \"r-\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, Groundtruth data for Sinc\")\n",
    "\n",
    "# TODO:\n",
    "assert False, \"TODO: instantiate and train an MLP on the sinc data..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [OPTIONAL] Qg) \n",
    "\n",
    "Change the hyperparameters in the MLP, say the `alpha` to `1e5` and `1e-1`, and explain the results (hint: regularization).\n",
    "\n",
    "Also, try out different `activation` functions `learning_rate`s and `solver`s, or other interesting hyperparameters found on the MLP regressor in the documentation.\n",
    "\n",
    "Finally, implement the MLP regressor in `Keras` instead.\n",
    "\n",
    "(Solvers aka. optimizers and regularization will be discussed in a later lecture.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do some experiments.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2021-10-04| CEF, initial, converted from old word format.\n",
    "2021-10-04| CEF, inserted ANN_example.py into Notebook.\n",
    "2023-03-06| CEF, minor table update.\n",
    "2023-03-09| CEF, major update, translated to English, elaborated on NNs as Universal Approximator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fdca30083b677063cb48173af8214056b55c1526d8cb4ee9ed2f266c85962ab6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
