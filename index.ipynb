{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "# Intro\n",
    "\n",
    "\n",
    "We startup by reusing parts of `01_the_machine_learning_landscape.ipynb` from Géron [GITHOML]. So we begin with what Géron says about life satisfactions vs GDP per capita.\n",
    " \n",
    "Halfway down this notebook, a list of questions for SWMAL is presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 – The Machine Learning landscape\n",
    "\n",
    "_This is the code used to generate some of the figures in chapter 1._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"fundamentals\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"IGNORING: Saving figure\", fig_id) # SWMAL: I've disabled saving of figures\n",
    "    #if tight_layout:\n",
    "    #    plt.tight_layout()\n",
    "    #plt.savefig(path, format='png', dpi=300)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example 1-1\n",
    "\n",
    "This function just merges the OECD's life satisfaction data and the IMF's GDP per capita data. It's a bit too long and boring and it's not specific to Machine Learning, which is why I left it out of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_country_stats(oecd_bli, gdp_per_capita):\n",
    "    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "    gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n",
    "                                  left_index=True, right_index=True)\n",
    "    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "    keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the book expects the data files to be located in the current directory. I just tweaked it here to fetch the files in datasets/lifesat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datapath = os.path.join(\"../datasets\", \"lifesat\", \"\")\n",
    "\n",
    "# NOTE: a ! prefix makes us able to run system commands..\n",
    "# (command 'dir' for windows, 'ls' for Linux or Macs)\n",
    "#\n",
    "\n",
    "! dir\n",
    "\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code example\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Load the data\n",
    "try:\n",
    "    oecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\n",
    "    gdp_per_capita = pd.read_csv(datapath + \"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "except Exception as e:\n",
    "    print(f\"SWMAL NOTE: well, you need to have the 'datasets' dir in path, please unzip 'datasets.zip' and make sure that its included in the datapath='{datapath}' setting in the cell above..\")\n",
    "    raise e\n",
    "    \n",
    "# Prepare the data\n",
    "country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.show()\n",
    "\n",
    "# Select a linear model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make a prediction for Cyprus\n",
    "X_new = [[22587]]  # Cyprus' GDP per capita\n",
    "y_pred = model.predict(X_new)\n",
    "print(y_pred) # outputs [[ 5.96242338]]\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL\n",
    "\n",
    "Now we plot the linear regression result.\n",
    "\n",
    "Just ignore all the data plotter code mumbo-jumbo here (code take dirclty from the notebook, [GITHOML])...and see the final plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\n",
    "oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n",
    "oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n",
    "#oecd_bli.head(2)\n",
    "\n",
    "gdp_per_capita = pd.read_csv(datapath+\"gdp_per_capita.csv\", thousands=',', delimiter='\\t',\n",
    "                             encoding='latin1', na_values=\"n/a\")\n",
    "gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n",
    "gdp_per_capita.set_index(\"Country\", inplace=True)\n",
    "#gdp_per_capita.head(2)\n",
    "\n",
    "full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita, left_index=True, right_index=True)\n",
    "full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n",
    "#full_country_stats\n",
    "\n",
    "remove_indices = [0, 1, 6, 8, 33, 34, 35]\n",
    "keep_indices = list(set(range(36)) - set(remove_indices))\n",
    "\n",
    "sample_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]\n",
    "#missing_data = full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[remove_indices]\n",
    "\n",
    "sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "position_text = {\n",
    "    \"Hungary\": (5000, 1),\n",
    "    \"Korea\": (18000, 1.7),\n",
    "    \"France\": (29000, 2.4),\n",
    "    \"Australia\": (40000, 3.0),\n",
    "    \"United States\": (52000, 3.8),\n",
    "}\n",
    "for country, pos_text in position_text.items():\n",
    "    pos_data_x, pos_data_y = sample_data.loc[country]\n",
    "    country = \"U.S.\" if country == \"United States\" else country\n",
    "    plt.annotate(country, xy=(pos_data_x, pos_data_y), xytext=pos_text,\n",
    "            arrowprops=dict(facecolor='black', width=0.5, shrink=0.1, headwidth=5))\n",
    "    plt.plot(pos_data_x, pos_data_y, \"ro\")\n",
    "#save_fig('money_happy_scatterplot')\n",
    "plt.show()\n",
    "\n",
    "from sklearn import linear_model\n",
    "lin1 = linear_model.LinearRegression()\n",
    "Xsample = np.c_[sample_data[\"GDP per capita\"]]\n",
    "ysample = np.c_[sample_data[\"Life satisfaction\"]]\n",
    "lin1.fit(Xsample, ysample)\n",
    "\n",
    "t0 = 4.8530528\n",
    "t1 = 4.91154459e-05\n",
    "\n",
    "sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "M=np.linspace(0, 60000, 1000)\n",
    "plt.plot(M, t0 + t1*M, \"b\")\n",
    "plt.text(5000, 3.1, r\"$\\theta_0 = 4.85$\", fontsize=14, color=\"b\")\n",
    "plt.text(5000, 2.2, r\"$\\theta_1 = 4.91 \\times 10^{-5}$\", fontsize=14, color=\"b\")\n",
    "#save_fig('best_fit_model_plot')\n",
    "plt.show()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultra-brief Intro to the Fit-Predict Interface in Scikit-learn\n",
    "\n",
    "OK, the important lines in the cells above are really just\n",
    "```python\n",
    "#Select a linear model\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make a prediction for Cyprus\n",
    "X_new = [[22587]]  # Cyprus' GDP per capita\n",
    "y_pred = model.predict(X_new)\n",
    "print(y_pred) # outputs [[ 5.96242338]]\n",
    "\n",
    "```\n",
    "\n",
    "What happens here is that we create  model, called LinearRegression (for now just a 100% black-box method), put in our data training $\\mathbf{X}$ matrix and corresponding desired training ground thruth vector $\\mathbf{y}$ (aka $\\mathbf{y}_{true})$, and then train the model. \n",
    "\n",
    "After training we extract a _predicted_ $\\mathbf{y}_{pred}$ vector from the model, for some input scalar $x$=22587. \n",
    "\n",
    "\n",
    "## Supervised Training via Fit-predict\n",
    "\n",
    "The train-predict (or train-fit) process on some data can be visualized as\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L01/Figs/supervised_learning.png\" alt=\"WARNING: could not get image from server.\" style=\"height:250px\">\n",
    "\n",
    "In this figure the untrained model is a `sklearn.linear_model.LinearRegression` python object. When trained via  `model.fit()`, using some know answers for the data, $\\mathbf{y}_{true}~$, it becomes a blue-boxed trained model.\n",
    "\n",
    "The trained model can be used to _predict_ values from new, yet-unseen, data, via the `model.predict()` function. \n",
    "\n",
    "In other words, how high is life-satisfaction for Cyprus' GDP=22587 USD?\n",
    "\n",
    "Just call `model.predict()` on a matrix with one single numerical element, 22587, well, not a matrix really, but a python list-of-lists, `[[22587]]`  \n",
    "\n",
    "```y_pred = model.predict([[22587]])```\n",
    "\n",
    "Apparently 5.96 the models answers!\n",
    "\n",
    "(you get used to the python built-in containers and numpy on the way..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qa) The $\\theta$ parameters and the $R^2$ Score\n",
    "\n",
    "Géron uses some $\\theta$ parameter from this linear regression model, in his examples and plots above.\n",
    "\n",
    "How do you extract the $\\theta_0$ and $\\theta_1$ coefficients in his life-satisfaction figure form the linear regression model, via the models python attributes?\n",
    "\n",
    "Read the documentation for the linear regressor at\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "Extract the score=0.734 for the model using data (X,y) and explain what $R^2$ score measures in broad terms\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcll}\n",
    "    R^2 &=& 1 - u/v\\\\\n",
    "    u   &=& \\sum (y_{true} - y_{pred}~)^2   ~~~&\\small \\mbox{residual sum of squares}\\\\\n",
    "    v   &=& \\sum (y_{true} - \\mu_{true}~)^2 ~~~&\\small \\mbox{total sum of squares}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "with $y_{true}~$ being the true data, $y_{pred}~$ being the predicted data from the model and $\\mu_{true}~$ being the true mean of the data.\n",
    "\n",
    "What are the minimum and maximum values for $R^2~$?\n",
    "\n",
    "Is it best to have a low $R^2$ score or a high $R^2$ score? This means, is $R^2$ a loss/cost function or a function that measures of fitness/goodness? \n",
    "\n",
    "NOTE$_1$: the $R^2$ is just one of many scoring functions used in ML, we will see plenty more other methods later.\n",
    "\n",
    "NOTE$_2$: there are different definitions of the $R^2$, 'coefficient of determination', in linear algebra. We stricly use the formulation above. \n",
    "\n",
    "OPTIONAL: Read the additional in-depth literature on $R^2~$:\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Coefficient_of_determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: solve Qa, and remove me..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Merits of the Fit-Predict Interface\n",
    "\n",
    "Now comes the really fun part: all methods in Scikit-learn have this fit-predict interface, and you can easily interchange models in your code just by instantiating a new and perhaps better ML model.\n",
    "\n",
    "There are still a lot of per-model parameters to tune, but fortunately, the built-in default values provide you with a good initial guess for good model setup.\n",
    "\n",
    "Later on, you might want to go into the parameter detail trying to optimize some params (opening the lid of the black-box ML algo), but for now, we pretty much stick to the default values.\n",
    "\n",
    "Let's try to replace the linear regression now, let's test a _k-nearest neighbour algorithm_ instead (still black boxed algorithm-wise)...\n",
    "\n",
    "\n",
    "## Qb) Using k-Nearest Neighbors\n",
    "\n",
    "Change the linear regression model to a `sklearn.neighbors.KNeighborsRegressor` with k=3 (as in [HOML:p.22,bottom]), and rerun the `fit` and `predict` using this new model.\n",
    "\n",
    "What do the k-nearest neighbours estimate for Cyprus, compared to the linear regression (it should yield=5.77)?\n",
    "\n",
    "What _score-method_ does the k-nearest model use, and is it comparable to the linear regression model? \n",
    "\n",
    "Seek out the documentation in Scikit-learn, if the scoring methods are not equal, can they be compared to each other at all then?\n",
    "\n",
    "Remember to put pointer/text from the Sckikit-learn documentation in the journal...(did you find the right kNN model etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our raw data set:\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this is our preprocessed data\n",
    "country_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = np.c_[country_stats[\"GDP per capita\"]]\n",
    "y = np.c_[country_stats[\"Life satisfaction\"]]\n",
    "\n",
    "print(\"X.shape=\",X.shape)\n",
    "print(\"y.shape=\",y.shape)\n",
    "\n",
    "# Visualize the data\n",
    "country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n",
    "plt.show()\n",
    "\n",
    "# Select and train a model\n",
    "\n",
    "# TODO: add your code here..\n",
    "assert False, \"TODO: add you instatiation and training of the knn model here..\"\n",
    "# knn = .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc) Tuning Parameter for k-Nearest Neighbors and A Sanity Check\n",
    "\n",
    "But that not the full story. Try plotting the prediction for both models in the same graph and tune the `k_neighbor` parameter of the `KNeighborsRegressor` model.  \n",
    "\n",
    "Choosing `k_neighbor=1` produces a nice `score=1`, that seems optimal...but is it really so good?\n",
    "\n",
    "Plotting the two models in a 'Life Satisfaction-vs-GDP capita' 2D plot by creating an array in the range 0 to 60000 (USD) (the `M` matrix below) and then predict the corresponding y value will sheed some light to this. \n",
    "\n",
    "Now reusing the plots stubs below, try to explain why the k-nearest neighbour with `k_neighbor=1` has such a good score.\n",
    "\n",
    "Does a score=1 with `k_neighbor=1`also mean that this would be the prefered estimator for the job?\n",
    "\n",
    "Hint here is a similar plot of a KNN for a small set of different k's:\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L01/Figs/regression_with_knn.png\"  alt=\"WARNING: could not get image from server.\" style=\"height:150px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction', figsize=(5,3))\n",
    "plt.axis([0, 60000, 0, 10])\n",
    "\n",
    "# create an test matrix M, with the same dimensionality as X, and in the range [0;60000] \n",
    "# and a step size of your choice\n",
    "m=np.linspace(0, 60000, 1000)\n",
    "M=np.empty([m.shape[0],1])\n",
    "M[:,0]=m\n",
    "\n",
    "# from this test M data, predict the y values via the lin.reg. and k-nearest models\n",
    "y_pred_lin = model.predict(M)\n",
    "y_pred_knn = knn.predict(M)   # ASSUMING the variable name 'knn' of your KNeighborsRegressor \n",
    "\n",
    "# use plt.plot to plot x-y into the sample_data plot..\n",
    "plt.plot(m, y_pred_lin, \"r\")\n",
    "plt.plot(m, y_pred_knn, \"b\")\n",
    "\n",
    "# TODO: add your code here..\n",
    "assert False, \"TODO: try knn with different k_neighbor params, that is re-instantiate knn, refit and replot..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd) Trying out a Neural Network\n",
    "\n",
    "Let us then try a Neural Network on the data, using the fit-predict interface allows us to replug a new model into our existing code.\n",
    "\n",
    "There are a number of different NN's available, let's just hook into Scikit-learns Multi-Layer Perceptron for regression, that is an 'MLPRegressor'. \n",
    "\n",
    "Now, the data-set for training the MLP is really not well scaled, so we need to tweak a lot of parameters in the MLP just to get it to produce some sensible output: with out preprocessing and scaling of the input data, `X`, the MLP is really a bad choice of model for the job since it so easily produces garbage output. \n",
    "\n",
    "Try training the `mlp` regression model below, predict the value for Cyprus, and find the `score` value for the training set...just as we did for the linear and KNN models.\n",
    "\n",
    "Can the `MLPRegressor` score function be compared with the linear and KNN-scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Setup MLPRegressor\n",
    "mlp = MLPRegressor( hidden_layer_sizes=(10,), solver='adam', activation='relu', tol=1E-5, max_iter=100000, verbose=True)\n",
    "mlp.fit(X, y.ravel())\n",
    "\n",
    "# lets make a MLP regressor prediction and redo the plots\n",
    "y_pred_mlp = mlp.predict(M) \n",
    "\n",
    "plt.plot(m, y_pred_lin, \"r\")\n",
    "plt.plot(m, y_pred_knn, \"b\")\n",
    "plt.plot(m, y_pred_mlp, \"k\")\n",
    "\n",
    "# TODO: add your code here..\n",
    "assert False, \"TODO: predict value for Cyprus and fetch the score() from the fitting.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [OPTIONAL] Qe) Neural Network with pre-scaling\n",
    "\n",
    "Now, the neurons in neural networks normally expects input data in the range `[0;1]` or sometimes in the range `[-1;1]`, meaning that for value outside this range the you put of the neuron will saturate to it's min or max value (also typical `0` or `1`). \n",
    "\n",
    "A concrete value of `X` is, say 22.000 USD, that is far away from what the MLP expects. To af fix to the problem in Qd) is to preprocess data by scaling it down to something more sensible.\n",
    "\n",
    "Try to scale X to a range of `[0;1]`, re-train the MLP, re-plot and find the new score from the rescaled input. Any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: try prescale data for the MPL...any better?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :-\n",
    "2018-12-18| CEF, initial.                  \n",
    "2019-01-24| CEF, spell checked and update. \n",
    "2019-01-30| CEF, removed reset -f, did not work on all PC's. \n",
    "2019-08-20| CEF, E19 ITMAL update. \n",
    "2019-08-26| CEF, minor mod to NN exercise.\n",
    "2019-08-28| CEF, fixed dataset dir issue, datapath\"../datasets\" changed to \"./datasets\".\n",
    "2020-01-25| CEF, F20 ITMAL update.\n",
    "2020-08-06| CEF, E20 ITMAL update, minor fix of ls to dir and added exception to datasets load, udpated figs paths.\n",
    "2020-09-24| CEF, updated text to R2, Qa exe.\n",
    "2020-09-28| CEF, updated R2 and theta extraction, use python attributes, moved revision table. Added comment about MLP.\n",
    "2021-01-12| CEF, updated Qe.\n",
    "2021-02-08| CEF, added ls for Mac/Linux to dir command cell. \n",
    "2021-08-02| CEF, update to E21 ITMAL.\n",
    "2021-08-03| CEF, fixed ref to p21 => p.22.\n",
    "2022-01-25| CEF, update to F22 SWMAL.\n",
    "2022-08-30| CEF, update to v1 changes.\n",
    "2023-08-30| CEF, minor table update for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "# Python Basics\n",
    "\n",
    "## Modules and Packages in Python\n",
    "\n",
    "Reuse of code in Jupyter notebooks can be done by either including a raw python source as a magic command\n",
    "\n",
    "```python\n",
    "%load filename.py\n",
    "```\n",
    "but this just pastes the source into the notebook and creates all kinds of pains regarding code maintenance.\n",
    "\n",
    "A better way is to use a python __module__. A module consists simply (and pythonic) of a directory with a module init file in it (possibly empty) \n",
    "```python\n",
    "libitmal/__init__.py\n",
    "```\n",
    "To this directory you can add modules in form of plain python files, say\n",
    "```python\n",
    "libitmal/utils.py\n",
    "```\n",
    "That's about it! The `libitmal` file tree should now look like\n",
    "```\n",
    "libitmal/\n",
    "├── __init__.py\n",
    "├── __pycache__\n",
    "│   ├── __init__.cpython-36.pyc\n",
    "│   └── utils.cpython-36.pyc\n",
    "├── utils.py\n",
    "```\n",
    "with the cache part only being present once the module has been initialized.\n",
    "\n",
    "You should now be able to use the `libitmal` unit via an import directive, like\n",
    "```python\n",
    "import numpy as np\n",
    "from libitmal import utils as itmalutils\n",
    "\n",
    "print(dir(itmalutils))\n",
    "print(itmalutils.__file__)\n",
    "\n",
    "X = np.array([[1,2],[3,-100]])\n",
    "itmalutils.PrintMatrix(X,\"mylabel=\")\n",
    "itmalutils.TestAll()\n",
    "```\n",
    "\n",
    "## Qa Load and test the `libitmal` module\n",
    "\n",
    "Try out the `libitmal` module from [GITMAL]. Load this module and run the function\n",
    "\n",
    "```python\n",
    "from libitmal import utils as itmalutils\n",
    "itmalutils.TestAll()\n",
    "```\n",
    "from this module.\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "Note that there is a python module ___include___ search path, that you may have to investigate and modify. For my Linux setup I have an export or declare statement in my .bashrc file, like\n",
    "\n",
    "```bash\n",
    "declare -x PYTHONPATH=~/ASE/ML/itmal:$PYTHONPATH\n",
    "```\n",
    "but your ```itmal```, the [GITMAL] root dir, may be placed elsewhere.\n",
    "\n",
    "For ___Windows___, you have to add `PYTHONPATH` to your user environment variables...see screenshot below (enlarge by modding the image width-tag or find the original png in the Figs directory).\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L01/Figs/Screenshot_windows_enviroment_variables.png\" alt=\"WARNING: could not get image from server.\" style=\"height:250px\" style=\"width:350px\">\n",
    "\n",
    "or if you, like me, hate setting up things in a GUI, and prefer a console, try in a CMD on windows\n",
    "\n",
    "```bash\n",
    "CMD> setx.exe PYTHONPATH \"C:\\Users\\auXXYYZZ\\itmal\"\n",
    "```\n",
    "\n",
    "replacing the username and path with whatever you have. Note that some Windows installations have various security settings enables, so that running `setx.exe` fails. Setting up a MAC should be similar to Linux; just modify your `PYTHONPATH` setting (still to be proven correct?, CEF). \n",
    "\n",
    "\n",
    "If everything fails you could programmatically add your path to the libitmal directory as\n",
    "\n",
    "```python\n",
    "import sys,os\n",
    "sys.path.append(os.path.expanduser('~/itmal'))\n",
    "\n",
    "from libitmal import utils as itmalutils\n",
    "print(dir(itmalutils))\n",
    "print(itmalutils.__file__)\n",
    "```\n",
    "\n",
    "For the journal: remember to document your particular PATH setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb Create your own module, with some functions, and test it\n",
    "\n",
    "Now create your own module, with some dummy functionality. Load it and run you dummy function in a Jupyter Notebook.\n",
    "\n",
    "Keep this module at hand, when coding, and try to capture reusable python functions in it as you invent them!\n",
    "\n",
    "For the journal: remember to document your particular library setup (where did you place files, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc How do you 'recompile' a module?\n",
    "\n",
    "When changing the module code, Jupyter will keep running on the old module. How do you force the Jupyter notebook to re-load the module changes? \n",
    "\n",
    "NOTE: There is a surprising issue regarding module reloads in Jupyter notebooks. If you use another development framework, like Spyder or Visual Studio Code, module reloading works out-of-the-box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL] Qd Write a Howto on Python Modules a Packages\n",
    "\n",
    "Write a short description of how to use modules in Python (notes on modules path, import directives, directory structure, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes in Python\n",
    "\n",
    "Good news: Python got classes. Bad news: they are somewhat obscure compared to C++ classes. \n",
    "\n",
    "Though we will not use object-oriented programming in Python intensively, we still need some basic understanding of Python classes. Let's just dig into a class-demo, here is `MyClass` in Python\n",
    "\n",
    "```python\n",
    "class MyClass:\n",
    "   \n",
    "    def myfun(self):\n",
    "        self.myvar = \"blah\" # NOTE: a per class-instance variable.\n",
    "        print(f\"This is a message inside the class, myvar={self.myvar}.\")\n",
    "\n",
    "myobjectx = MyClass()\n",
    "```\n",
    "\n",
    "NOTE: The following exercise assumes some C++ knowledge, in particular the OPRG and OOP courses. If you are an EE-student, or from another Faculty, then ignore the cryptic C++ comments, and jump directly to some Python code instead. It's the Python solution here, that is important!\n",
    "\n",
    "## Qe Extend the class with some public and private functions and member variables\n",
    "\n",
    "How are private function and member variables represented in python classes? \n",
    "\n",
    "What is the meaning of `self` in python classes?\n",
    "\n",
    "What happens to a function inside a class if you forget `self` in the parameter list, like `def myfun():` instead of `def myfun(self):` and you try to call it like `myobjectx.myfun()`? Remember to document the demo code and result.\n",
    "\n",
    "\n",
    "[OPTIONAL] What does 'class' and 'instance variables' in python correspond to in C++? Maybe you can figure it out, I did not really get it reading, say this tutorial\n",
    "\n",
    "> https://www.digitalocean.com/community/tutorials/understanding-class-and-instance-variables-in-python-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qe..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qf Extend the class with a Constructor\n",
    "\n",
    "Figure a way to declare/define a constructor (CTOR) in a python class. How is it done in python?\n",
    "\n",
    "Is there a class destructor in python (DTOR)? Give a textual reason why/why-not python has a DTOR?\n",
    "\n",
    "Hint: python is garbage collection like in C#, and do not go into the details of `__del__, ___enter__, __exit__` functions...unless you find it irresistible to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qf..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qg Extend the class with a to-string function\n",
    "\n",
    "Then find a way to serialize a class, that is to make some `tostring()` functionality similar to a C++ \n",
    "\n",
    "```C++\n",
    "friend ostream& operator<<(ostream& s,const MyClass& x)\n",
    "{\n",
    "    return os << ..\n",
    "}\n",
    "```\n",
    "\n",
    "If you do not know C++, you might be aware of the C# way to string serialize\n",
    "```\n",
    "    string s=myobject.tostring()\n",
    "```\n",
    "that is a per-class buildin function `tostring()`, now what is the pythonic way of 'printing' a class instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qg..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL] Qh Write a Howto on Python Classes \n",
    "\n",
    "Write a _How-To use Classes Pythonically_, including a description of public/privacy, constructors/destructors, the meaning of `self`, and inheritance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qh..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Administration\n",
    "\n",
    "REVISIONS||\n",
    ":- | :- |\n",
    "2018-12-19| CEF, initial.                  \n",
    "2018-02-06| CEF, updated and spell checked. \n",
    "2018-02-07| CEF, made Qh optional.\n",
    "2018-02-08| CEF, added PYTHONPATH for windows.\n",
    "2018-02-12| CEF, small mod in itmalutils/utils.\n",
    "2019-08-20| CEF, E19 ITMAL update.\n",
    "2020-01-25| CEF, F20 ITMAL update.\n",
    "2020-08-06| CEF, E20 ITMAL update, udpated figs paths.\n",
    "2020-09-07| CEF, added text on OPRG and OOP for EE's\n",
    "2020-09-29| CEF, added elaboration for journal in Qa+b.\n",
    "2021-02-06| CEF, fixed itmalutils.TestAll() in markdown cell.\n",
    "2021-08-02| CEF, update to E21 ITMAL.\n",
    "2022-01-25| CEF, update to F22 SWMAL.\n",
    "2022-02-25| CEF, elaborated on setx.exe on Windows and MAC PYTHONPATH.\n",
    "2022-08-30| CEF, updated to v1 changes.\n",
    "2022-09-16| CEF, added comment on module reloading when not using notebooks.\n",
    "2023-08-30| CEF, minor table and text update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "# Mathematical Foundation\n",
    "\n",
    "## Vector and matrix representation in python\n",
    "\n",
    "Say, we have $d$ features for a given sample point. This $d$-sized feature column vector for a data-sample $i$ is then given by\n",
    "\n",
    "$$\n",
    "    \\newcommand\\rem[1]{}\n",
    "    \\rem{SWMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\newcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\newcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\newcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\newcommand\\st[1]{_{\\scriptsize #1}}\n",
    "    \\newcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\newcommand\\obs[2]{#1_{\\mbox{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\newcommand\\diff[1]{\\mbox{d}#1}\n",
    "    \\newcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\mbox{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\mbox{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "\\bx\\powni = \n",
    "    \\ac{c}{\n",
    "        x_1\\powni \\\\\n",
    "        x_2\\powni \\\\ \n",
    "        \\vdots \\\\\n",
    "        x_d\\powni\n",
    "     }  \n",
    "$$\n",
    "\n",
    "or typically written transposed to save as\n",
    "\n",
    "$$\n",
    "    \\bx\\powni = \\left[  x_1\\powni~~ x_2\\powni~~ \\cdots~~ x_d\\powni\\right]^T\n",
    "$$\n",
    "\n",
    "such that $\\bX$ can be constructed of the full set of $n$ samples of these feature vectors\n",
    "\n",
    "$$\n",
    "    \\bX = \n",
    "      \\ac{c}{\n",
    "        (\\bx\\pown{1})^T \\\\\n",
    "        (\\bx\\pown{2})^T \\\\\n",
    "        \\vdots \\\\\n",
    "        (\\bx\\pownn)^T\n",
    "      }\n",
    "$$\n",
    "\n",
    "or by explicitly writing out the full data matrix $\\bX$ consisting of scalars \n",
    "\n",
    "$$\n",
    "    \\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "            \\vdots      &             &        & \\vdots \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "        }\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "but sometimes the notation is a little more fuzzy, leaving out the transpose operator for $\\mathbf x$ and in doing so just interpreting the $\\mathbf{x}^{(i)}$'s to be row vectors instead of column vectors.\n",
    "\n",
    "The target column vector, $\\mathbf y$, also has the dimension $n$ \n",
    "\n",
    "$$\n",
    "    \\by = \\ac{c}{\n",
    "            y\\pown{1} \\\\\n",
    "            y\\pown{2} \\\\\n",
    "            \\vdots \\\\\n",
    "            y\\pownn \\\\\n",
    "          }\n",
    "$$\n",
    "\n",
    "## Qa Given the following $\\mathbf{x}^{(i)}$'s, construct and print the $\\mathbf X$ matrix in python.\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "      \\bx\\pown{1} &= \\ac{c}{ 1, 2, 3}^T \\\\\n",
    "      \\bx\\pown{2} &= \\ac{c}{ 4, 2, 1}^T \\\\\n",
    "      \\bx\\pown{3} &= \\ac{c}{ 3, 8, 5}^T \\\\\n",
    "      \\bx\\pown{4} &= \\ac{c}{-9,-1, 0}^T\n",
    "    }\n",
    "$$\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "Notice that the ```np.matrix``` class is getting deprecated! So, we use numpy's ```np.array``` as matrix container. Also, __do not__ use the built-in python lists or the numpy matrix subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qa\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y = np.array([1,2,3,4]) # NOTE:  you'll need this later\n",
    "\n",
    "# TODO..create and print the full matrix\n",
    "assert False, \"TODO: solve Qa, and remove me..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norms, metrics or distances\n",
    "\n",
    "The $\\norm{2}$ Euclidian distance, or norm, for a vector of size $n$ is defined as \n",
    "\n",
    "$$\n",
    "    \\norm{2}:~~ ||\\bx||_2 = \\left( \\sum_{i=1}^{n} |x_i|^2 \\right)^{1/2}\\\\\n",
    "$$\n",
    "\n",
    "and the distance between two vectors is given by\n",
    "\n",
    "$$\n",
    "    \\ar{ll}{      \n",
    "          \\mbox{d}(\\bx,\\by) &= ||\\bx-\\by||_2\\\\\n",
    "                     &= \\left( \\sum_{i=1}^n \\left| x_{i}-y_{i} \\right|^2 \\right)^{1/2}\n",
    "    }\n",
    "$$ \n",
    "\n",
    "This Euclidian norm is sometimes also just denoted as $||\\bx||$, leaving out the 2 in the subscript.\n",
    "\n",
    "The squared $\\norm{2}$ for a vector can compactly be expressed via \n",
    "\n",
    "$$\n",
    "    \\norm{2}^2: ||\\bx||_2^2 = \\bx^\\top\\bx\n",
    "$$\n",
    "\n",
    "\n",
    "The $\\norm{1}$ 'City-block' norm is given by\n",
    "\n",
    "$$\n",
    "    \\norm{1}:~~ ||\\bx||_1 = \\sum_i |x_i|\n",
    "$$\n",
    "\n",
    "but $\\norm{1}$ is not used as intensive as its more popular $\\norm{2}$ cousin. \n",
    "\n",
    "Notice that $|x|$ in code means ```fabs(x)```.\n",
    "\n",
    "## Qb Implement the $\\norm{1}$ and $\\norm{2}$ norms for vectors in python.\n",
    "\n",
    "First implementation must be a 'low-level'/explicit implementation---using primitive/build-in functions, like ```+```, ```*``` and power ```**``` only! The square-root function can be achieved via power like ```x**0.5```.\n",
    "\n",
    "Do NOT use any methods from libraries, like ```math.sqrt```, ```math.abs```, ```numpy.linalg.inner```, ```numpy.dot()``` or similar. Yes, using such libraries is an efficient way of building python software, but in this exercise we want to explicitly map the mathematichal formulaes to python code.\n",
    "\n",
    "Name your functions L1 and L2 respectively, they both take one vector as input argument.\n",
    "\n",
    "But test your implementation against some built-in functions, say  ```numpy.linalg.norm```\n",
    "\n",
    "When this works, and passes the tests below, optimize the $\\norm{2}$, such that it uses np.numpy's dot operator instead of an explicit sum, call this function ```L2Dot```. This implementation, ```L2Dot```, must be pythonic, i.e. it must not contain explicit for- or while-loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solve Qb...implement the L1, L2 and L2Dot functions...\n",
    "assert False, \"TODO: solve Qb, and remove me..\"\n",
    "\n",
    "# TEST vectors: here I test your implementation...calling your L1() and L2() functions\n",
    "tx=np.array([1, 2, 3, -1])\n",
    "ty=np.array([3,-1, 4,  1])\n",
    "\n",
    "expected_d1=8.0\n",
    "expected_d2=4.242640687119285\n",
    "\n",
    "d1=L1(tx-ty)\n",
    "d2=L2(tx-ty)\n",
    "\n",
    "print(f\"tx-ty={tx-ty}, d1-expected_d1={d1-expected_d1}, d2-expected_d2={d2-expected_d2}\")\n",
    "\n",
    "eps=1E-9 \n",
    "# NOTE: remember to import 'math' for fabs for the next two lines..\n",
    "assert math.fabs(d1-expected_d1)<eps, \"L1 dist seems to be wrong\" \n",
    "assert math.fabs(d2-expected_d2)<eps, \"L2 dist seems to be wrong\" \n",
    "\n",
    "print(\"OK(part-1)\")\n",
    "\n",
    "# comment-in once your L2Dot fun is ready...\n",
    "#d2dot=L2Dot(tx-ty)\n",
    "#print(\"d2dot-expected_d2=\",d2dot-expected_d2)\n",
    "#assert fabs(d2dot-expected_d2)<eps, \"L2Ddot dist seem to be wrong\" \n",
    "#print(\"OK(part-2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cost function, $J$\n",
    "\n",
    "Now, most ML algorithm uses norms or metrics internally when doing minimizations. Details on this will come later, but for now we need to know that an algorithm typically tries to minimize a given performance metric, the loss function, for all the input data, and implicitly tries to minimize the sum of all norms for the 'distances' between some predicted output, $y\\st{pred}$ and the true output $y\\st{true}$, with the distance between these typically given by the $\\norm{2}$ norm\n",
    "\n",
    "$$   \n",
    "  \\mbox{individual loss:}~~L\\powni = \\mbox{d}(y\\st{pred}\\powni,y\\st{true}\\powni)\n",
    "$$ \n",
    "\n",
    "with $y\\st{pred}\\powni$, a scalar value, being the output from the hypothesis function, that maps the input vector $\\bx\\powni$ to a scalar\n",
    "\n",
    "$$ \n",
    "    y_{pred}\\powni = \\hat{y}\\powni = h(\\bx\\powni;\\btheta)\n",
    "$$\n",
    "\n",
    "and the total loss, $J$ will be the sum over all $i$'s\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni\\\\\n",
    "        &= \\frac{1}{n} \\sum_{i=1}^{n} \\mbox{d}( h(\\bx\\powni) , y\\powni\\st{true})\n",
    "    }\n",
    "$$\n",
    "\n",
    "\n",
    "## Cost function in vector/matrix notation using $\\norm{2}$\n",
    "\n",
    "Remember the data-flow model for supervised learning\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L02/Figs/ml_simple_vector.png\" alt=\"WARNING: could not get image from server.\" style=\"width:500px\">\n",
    "\n",
    "Let us now express $J$ in terms of vectors and matrices instead of summing over individual scalars, and let's use $\\norm{2}$ as the distance function\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J(\\bX,\\by;\\btheta) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni\\\\\n",
    "        &= \\frac{1}{n}\\sum_{i=1}^{n} (h(\\bx\\powni) - \\by\\powni\\st{true})^2\\\\\n",
    "        &= \\frac{1}{n} ||h(\\bX) - \\by\\st{true} ||_2^2\\\\\n",
    "        &= \\frac{1}{n} ||\\by\\st{pred} - \\by\\st{true} ||_2^2\\\\\n",
    "     }\n",
    "$$\n",
    "\n",
    "with the matrix-vector notation\n",
    "\n",
    "$$ \n",
    "    \\by_{pred} = \\hat{\\by} =  h(\\bX;\\btheta)\n",
    "$$\n",
    "\n",
    "## Loss or Objective Function using the Mean Squared Error\n",
    "\n",
    "This formulation is equal to the definition of the _mean-squared-error_, MSE (or indirectly also RMSE), here given in the general formulation for some random variable $Z$ \n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        \\mbox{MSE} &= \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{Z}_i-Z_i)^2 = \\frac{1}{n} SS\\\\\n",
    "        \\mbox{RMSE} &= \\sqrt{\\mbox{MSE}}\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "with sum-of-squares (SS) is given simply by\n",
    "\n",
    "$$\n",
    "    \\mbox{SS} = \\sum_{i=1}^{n} (\\hat{Z}_i-Z_i)^2\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "So, using the $\\norm{2}$ for the distance metric, is equal to saying that we want to minimize $J$ with respect to the MSE\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J &= \\mbox{MSE}(h(\\bX), \\by\\st{true}) \\\\\n",
    "          &= \\mbox{MSE}(\\by\\st{pred}~, \\by\\st{true}) \\\\\n",
    "          &= \\mbox{MSE}(\\hat{\\by}, \\by\\st{true})\n",
    "     }\n",
    "$$\n",
    "\n",
    "Note: when minimizing one can ignore the constant factor $1/n$ and it really does not matter if you minimize MSE or RMSE. Often $J$ is also multiplied by 1/2 to ease notation when trying to differentiate it.\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J(\\bX,\\by\\st{true};\\btheta) &\\propto \\half ||\\by\\st{pred} - \\by\\st{true} ||_2^2 \\\\\n",
    "          &\\propto \\mbox{MSE}\n",
    "     }\n",
    "$$\n",
    "\n",
    "## MSE\n",
    "\n",
    "Now, let us take a look on how you calculate the MSE.\n",
    "\n",
    "The MSE uses the $\\norm{2}$ norm internally, well, actually $||\\cdot||^2_2$ to be precise, and basically just sums, means and roots the individual (scalar) losses (distances), we just saw before. \n",
    "\n",
    "And the RMSE is just an MSE with a final square-root call.\n",
    "\n",
    "## Qc Construct the Root Mean Square Error (RMSE) function (Equation 2-1 [HOML]).\n",
    "\n",
    "Call the function RMSE, and evaluate it using the $\\bX$ matrix and $\\by$ from Qa.\n",
    "\n",
    "We implement a dummy hypothesis function, that just takes the first column of $\\bX$ as its 'prediction'\n",
    "\n",
    "$$\n",
    "    h\\st{dummy}(\\bX) = \\bX(:,0)\n",
    "$$\n",
    "\n",
    "Do not re-implement the $\\norm{2}$ for the RMSE function, but call the '''L2''' function you just implemented internally in RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solve Qc...implement your RMSE function here\n",
    "assert False, \"TODO: solve Qc, and remove me..\"\n",
    "\n",
    "\n",
    "# Dummy h function:\n",
    "def h(X):    \n",
    "    if X.ndim!=2:\n",
    "        raise ValueError(\"excpeted X to be of ndim=2, got ndim=\",X.ndim)\n",
    "    if X.shape[0]==0 or X.shape[1]==0:\n",
    "        raise ValueError(\"X got zero data along the 0/1 axis, cannot continue\")\n",
    "    return X[:,0]\n",
    "\n",
    "# Calls your RMSE() function:\n",
    "r=RMSE(h(X),y)\n",
    "\n",
    "# TEST vector:\n",
    "eps=1E-9\n",
    "expected=6.57647321898295\n",
    "print(f\"RMSE={r}, diff={r-expected}\")\n",
    "assert fabs(r-expected)<eps, \"your RMSE dist seems to be wrong\" \n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE\n",
    "\n",
    "## Qd Similar construct the Mean Absolute Error (MAE) function (Equation 2-2 [HOML]) and evaluate it.\n",
    "\n",
    "The MAE will algorithmic wise be similar to the MSE part from using the $\\norm{1}$ instead of the $\\norm{2}$ norm.\n",
    "\n",
    "Again, re-implementation of the$\\norm{1}$ is a no-go, call the '''L1''' instead internally i MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solve Qd\n",
    "assert False, \"TODO: solve Qd, and remove me..\"\n",
    "\n",
    "\n",
    "# Calls your MAE function:\n",
    "r=MAE(h(X), y)\n",
    "\n",
    "# TEST vector:\n",
    "expected=3.75\n",
    "print(f\"MAE={r}, diff={r-expected}\")\n",
    "assert fabs(r-expected)<eps, \"MAE dist seems to be wrong\" \n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonic Code\n",
    "\n",
    "## Robustness of Code\n",
    "\n",
    "Data validity checking is an essential part of robust code, and in Python the 'fail-fast' method is used extensively: instead of lingering on trying to get the 'best' out of an erroneous situation, the fail-fast pragma will be very loud about any data inconsistencies at the earliest possible moment.\n",
    "\n",
    "Hence robust code should include a lot of error checking, say as pre- and post-conditions (part of the design-by-contract programming) when calling a function: when entering the function you check that all parameters are ok (pre-condition), and when leaving you check the return parameter (post-conditions).  \n",
    "\n",
    "Normally assert-checking or exception-throwing will do the trick just fine, with the exception method being more _pythonic_.\n",
    "\n",
    "For the norm-function you could, for instance, test your input data to be 'vector' like, i.e. like\n",
    "\n",
    "```python\n",
    "    assert x.shape[0]>=0 and x.shape[1]==0\n",
    "    \n",
    "    if not x.ndim==1:\n",
    "        raise some error\n",
    "```\n",
    "or similar.\n",
    "\n",
    "## Qe Robust Code \n",
    "\n",
    "Add error checking code (asserts or exceptions), that checks for right $\\hat\\by$-$\\by$ sizes of the MSE and MAE functions.\n",
    "\n",
    "Also add error checking to all you previously tested L2() and L1() functions, and re-run all your tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solve Qe...you need to modify your python cells above\n",
    "assert False, \"TODO: solve Qe, and remove me..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qf Conclusion\n",
    "\n",
    "Now, conclude on all the exercise above. \n",
    "\n",
    "Write a short textual conclusion (max. 10- to 20-lines) that extract the _essence_ of the exercises: why did you think it was important to look at these particular ML concepts, and what was our overall learning outcome of the exercises (in broad terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qf concluding remarks in text.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    "---------||\n",
    "2018-12-18| CEF, initial.                  \n",
    "2019-01-31| CEF, spell checked and update. \n",
    "2019-02-04| CEF, changed d1/d2 in Qb to L1/L2. Fixe rev date error.\n",
    "2019-02-04| CEF, changed headline.\n",
    "2019-02-04| CEF, changed (.) in dist(x,y) to use pipes instead.\n",
    "2019-02-04| CEF, updated supervised learning fig, and changed , to ; for thetas, and change = to propto.\n",
    "2019-02-05| CEF, post lesson update, minor changes, added fabs around two test vectors.\n",
    "2019-02-07| CEF, updated def section. \n",
    "2019-09-01| CEF, updated for ITMAL v2.\n",
    "2019-09-04| CEF, updated for print-f and added conclusion Q.\n",
    "2019-09-05| CEF, fixed defect in print string and commented on fabs.\n",
    "2020-01-30| CEF, F20 ITMAL update.\n",
    "2020-02-03| CEF, minor text fixes.\n",
    "2020-02-24| CEF, elaborated on MAE and RMSE, emphasized not to use np functionality in L1 and L2.\n",
    "2020-09-03| CEF, E20 ITMAL update, updated figs paths.\n",
    "2020-09-06| CEF, added alt text.\n",
    "2020-09-07| CEF, updated HOML page refs.\n",
    "2021-01-12| CEF, F21 ITMAL update, moved revision table.\n",
    "2021-02-09| CEF, elaborated on test-vectors. Changed order of Design Matrix descriptions.\n",
    "2021-08-02| CEF, update to E21 ITMAL.\n",
    "2022-01-25| CEF, update to F22 SWMAL.\n",
    "2022-02-25| CEF, removed inner product equations.\n",
    "2022-08-30| CEF, updated to v1 changes.\n",
    "2023-02-07| CEF, minor update for d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "# Implementing a dummy binary-classifier with fit-predict interface\n",
    "\n",
    "We begin with the MNIST data-set and will reuse the data loader from Scikit-learn. Next we create a dummy classifier, and compare the results of the SGD and dummy classifiers using the MNIST data...\n",
    "\n",
    "## Qa  Load and display the MNIST data\n",
    "\n",
    "There is a `sklearn.datasets.fetch_openml` dataloader interface in Scikit-learn. You can load MNIST data like \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_openml\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784',??) # needs to return X, y, replace '??' with suitable parameters! \n",
    "# Convert to [0;1] via scaling (not always needed)\n",
    "#X = X / 255.\n",
    "```\n",
    "\n",
    "but you need to set parameters like `return_X_y` and `cache` if the default values are not suitable! \n",
    "\n",
    "Check out the documentation for the `fetch_openml` MNIST loader, try it out by loading a (X,y) MNIST data set, and plot a single digit via the `MNIST_PlotDigit` function here (input data is a 28x28 NMIST subimage)\n",
    "\n",
    "```python\n",
    "%matplotlib inline\n",
    "def MNIST_PlotDigit(data):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "```\n",
    "\n",
    "Finally, put the MNIST loader into a single function called `MNIST_GetDataSet()` so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: solve Qa, and remove me..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb  Add a Stochastic Gradient Decent [SGD] Classifier\n",
    "\n",
    "Create a train-test data-set for MNIST and then add the `SGDClassifier` as done in [HOML], p.103.\n",
    "\n",
    "Split your data and run the fit-predict for the classifier using the MNIST data.(We will be looking at cross-validation instead of the simple fit-predict in a later exercise.)\n",
    "\n",
    "Notice that you have to reshape the MNIST X-data to be able to use the classifier. It may be a 3D array, consisting of 70000 (28 x 28) images, or just a 2D array consisting of 70000 elements of size 784.\n",
    "\n",
    "A simple `reshape()` could fix this on-the-fly:\n",
    "```python\n",
    "X, y = MNIST_GetDataSet()\n",
    "\n",
    "print(f\"X.shape={X.shape}\") # print X.shape= (70000, 28, 28)\n",
    "if X.ndim==3:\n",
    "    print(\"reshaping X..\")\n",
    "    assert y.ndim==1\n",
    "    X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))\n",
    "assert X.ndim==2\n",
    "print(f\"X.shape={X.shape}\") # X.shape= (70000, 784)\n",
    "```\n",
    "\n",
    "Remember to use the category-5 y inputs\n",
    "\n",
    "```python\n",
    "y_train_5 = (y_train == '5')    \n",
    "y_test_5  = (y_test == '5')\n",
    "```\n",
    "instead of the `y`'s you are getting out of the dataloader. In effect, we have now created a binary-classifier, that enable us to classify a particular data sample, $\\mathbf{x}(i)$ (that is a 28x28 image), as being a-class-5 or not-a-class-5. \n",
    "\n",
    "Test your model on using the test data, and try to plot numbers that have been categorized correctly. Then also find and plots some misclassified numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: solve Qb, and remove me..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc Implement a dummy binary classifier\n",
    "\n",
    "Now we will try to create a Scikit-learn compatible estimator implemented via a python class. Follow the code found in [HOML], p.107 (for [HOML] 1st and 2nd editions: name you estimator `DummyClassifier` instead of `Never5Classifyer`).\n",
    "\n",
    "Here our Python class knowledge comes into play. The estimator class hierarchy looks like\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L02/Figs/class_base_estimator.png\" alt=\"WARNING: could not get image from server.\" style=\"width:500px\">\n",
    "\n",
    "All Scikit-learn classifiers inherit from `BaseEstimator` (and possibly also `ClassifierMixin`), and they must have a `fit-predict` function pair (strangely not in the base class!) and you can actually find the `sklearn.base.BaseEstimator` and `sklearn.base.ClassifierMixin` python source code somewhere in you anaconda install dir, if you should have the nerves to go to such interesting details.\n",
    "\n",
    "But surprisingly you may just want to implement a class that contains the `fit-predict` functions, ___without inheriting___ from the `BaseEstimator`, things still work due to the pythonic 'duck-typing': you just need to have the class implement the needed interfaces, obviously `fit()` and `predict()` but also the more obscure `get_params()` etc....then the class 'looks like' a `BaseEstimator`...and if it looks like an estimator, it _is_ an estimator (aka. duck typing).\n",
    "\n",
    "Templates in C++ also allow the language to use compile-time duck typing!\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Duck_typing\n",
    "\n",
    "Call the fit-predict on a newly instantiated `DummyClassifier` object, and find a way to extract the accuracy `score` from the test data. You may implement an accuracy function yourself or just use the `sklearn.metrics.accuracy_score` function. \n",
    "\n",
    "Finally, compare the accuracy score from your `DummyClassifier` with the scores found in [HOML] \"Measuring Accuracy Using Cross-Validation\", p.107. Are they comparable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: solve Qc, and remove me..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd Conclusion\n",
    "\n",
    "Now, conclude on all the exercise above. \n",
    "\n",
    "Write a short textual conclusion (max. 10- to 20-lines) that extract the _essence_ of the exercises: why did you think it was important to look at these particular ML concepts, and what was our overall learning outcome of the exercises (in broad terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd concluding remarks in text.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    "---------||\n",
    "2018-12-19| CEF, initial.                  \n",
    "2018-02-06| CEF, updated and spell checked. \n",
    "2018-02-08| CEF, minor text update.\n",
    "2018-03-05| CEF, updated with SHN comments.\n",
    "2019-09-02| CEF, updated for ITMAL v2.\n",
    "2019-09-04| CEF, updated and added conclusion Q.\n",
    "2020-01-25| CEF, F20 ITMAL update.\n",
    "2020-02-04| CEF, updated page numbers to HOMLv2.\n",
    "2020-09-03| CEF, E20 ITMAL update, udpated figs paths.\n",
    "2020-09-06| CEF, added alt text.\n",
    "2020-09-18| CEF, added binary-classifier text to Qb to emphasise 5/non-5 classification.\n",
    "2021-01-12| CEF, F21 ITMAL update, moved revision tabel.\n",
    "2021-08-02| CEF, update to E21 ITMAL.\n",
    "2022-01-25| CEF, update to F22 SWMAL.\n",
    "2023-02-07| CEF, update HOML page numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Performance Metrics\n",
    "\n",
    "There are a number of frequently uses metrics in ML, namely accuracy, precision, recall and the $F_1$ score. All are called _metrics_ (though they are not true norms, like ${\\cal L}_2$ or ${\\cal L}_1$ we saw last time).\n",
    "\n",
    "Maybe performance _score_ would be a better name than performance metric, at least for the accuracy, precision, recall we will be looking at---emphasising the conceptual distinction between the  _score-function_ and _cost(/loss/error/objective)-function_ (the later is typically a true distance/norm function).  \n",
    "\n",
    "You can find a lot of details on say precision and recall in Wikipedia\n",
    "\n",
    ">  https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "\n",
    "## Nomenclature\n",
    "\n",
    "| NAME | SYMBOL | ALIAS |\n",
    "| :---: | :---: | :---: |\n",
    "|true positives | $TP$ | |\n",
    "|true negatives | $TN$ | |\n",
    "|false positives| $FP$ | type I error| \n",
    "|false negatives| $FN$ | type II error |\n",
    "\n",
    "and $N = N_P + N_N$ being the total number of samples and the number of positive and negative samples\n",
    "respectively.\n",
    "\n",
    "## Precision\n",
    "\n",
    "$$\n",
    "\\def\\by{\\mathbf{y}}\n",
    "\\def\\ba{\\begin{array}{lll}}\n",
    "\\def\\ea{\\end{array}}\n",
    "\\newcommand{\\rem}[1]{}\n",
    "\\newcommand\\st[1]{_{\\scriptsize #1}}\n",
    "\\newcommand\\myfrac[2]{\\frac{#1\\rule{0pt}{8pt}}{#2\\rule{0pt}{8pt}}} \n",
    "\\ba\n",
    " p &= \\myfrac{TP}{TP + FP}\n",
    "\\ea\n",
    "$$\n",
    "\n",
    "## Recall or Sensitivity\n",
    "\n",
    "$$\n",
    "  \\ba\n",
    "    r &= \\myfrac{TP}{TP + FN}\\\\\n",
    "      &= \\myfrac{TP}{N_P}\n",
    "  \\ea\n",
    "$$\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "$$\n",
    "  \\ba\n",
    "      a &= \\myfrac{TP + TN}{TP + TN + FP + FN}\\\\\n",
    "        &= \\myfrac{TP + TN}{N}\\\\\n",
    "        &= \\myfrac{TP + TN}{N_P~~ + N_N} \n",
    "  \\ea\n",
    "$$\n",
    "\n",
    "## Accuracy Paradox\n",
    "\n",
    "A static constant model, say $p\\st{cancer}=0$ may have higher accuracy than a real model with predictive power. This is odd!\n",
    "\n",
    "Asymmetric weights could also be associated with the false positive and false negative predictions, yielding either FP of FN much more expensive than the other. Say, it is more expensive not to treat a person with cancer, than treating a person without cancer. \n",
    "\n",
    "## F-score\n",
    "\n",
    "General $\\beta$-harmonic mean of the precision and recall \n",
    "$$\n",
    "    F_\\beta = (1+\\beta^2) \\myfrac{pr}{\\beta^2 p+r}\\\\\n",
    "$$ \n",
    "that for say $\\beta=2$ or $\\beta=0.5$ shifts or skews the emphasis on the two variables in the equation. Normally only the $\\beta=1$ harmonic mean is used\n",
    "\n",
    "$$\n",
    "  \\ba\n",
    "    F_1 &= \\myfrac{2pr}{p+r}\\\\\n",
    "        &= \\myfrac{2}{1/p + 1/r}\n",
    "  \\ea\n",
    "$$\n",
    "with $F$ typically being synonymous with $F_1$. \n",
    "\n",
    "If needed, find more info on Wikipedia\n",
    "\n",
    "> https://en.wikipedia.org/wiki/F1_score\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "For statistical classification, the confusion matrix or error matrix (or\n",
    "matching matrix in unsupervised learning) is for a two-class problem given by\n",
    "the $2\\times2$ matrix with dimensions 'actual' and 'predicted'\n",
    "\n",
    "$$   \n",
    "{\\bf M}\\st{confusion} = \n",
    "\\begin{array}{l|ll}\n",
    "                           & \\mbox{actual true} & \\mbox{actual false} \\\\ \\hline\n",
    "    \\mbox{predicted true}  & TP & FP \\\\     \n",
    "    \\mbox{predicted false} & FN & TN \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The diagonal, in the square matrix, represent predicted values being the same\n",
    "as the actual values, off-diagonal elements represent erroneous prediction.\n",
    "\n",
    "Also notice, that the layout of this matrix is different of what is given in [HOML], \"Confusion Matrix\", p.110/fig 3-3. This is just a minor issue, since we can always flip/rotate/transpose the matrix (say by flipping the $\\by\\st{true}$ and $\\by\\st{pred}$ arguments). \n",
    "\n",
    "For N-class classification the matrix gives a matrix with $N$ actual\n",
    "classes and $N$ predicted classes\n",
    "\n",
    "$$\n",
    "{\\bf M}\\st{confusion}~~~ =\n",
    "  \\left[\n",
    "  \\begin{array}{llll}\n",
    "       c_{11} & c_{12} & \\cdots & c_{1n} \\\\ \n",
    "       c_{21} & c_{22} & \\cdots & c_{2n} \\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       c_{n1} & c_{n2} & \\cdots & c_{nn} \\\\ \n",
    "  \\end{array}\n",
    "  \\right]\n",
    "$$\n",
    "with say element $c_{21}$ being the number of actual classes '1' being predicted (erroneously) as class '2'.\n",
    "\n",
    "## Nomenclature for the Confusion Matrix\n",
    "\n",
    "The naming of the elements in the confusion matrix can be rather exotic, like _false omission rate_ (see the figure below), but we won't get to such detail here...let us stick with TP, TN, FP, FN and $F_1$!\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L02/Figs/performance_metrics.jpg\" alt=\"WARNING: could not get image from server\" style=\"width:900px\">\n",
    "\n",
    "If you need more info on the confusion matrix:\n",
    "\n",
    ">  https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n",
    "## Qa Implement the Accuracy function and test it on the MNIST data.\n",
    "\n",
    "We now follow the convention in Scikit-learn, that a score funtion takes the arguments `y_true` and then `y_pred`\n",
    "\n",
    "```\n",
    "    sklearn.metrics.accuracy_score(y_true, y_pred, ..)\n",
    "```\n",
    "\n",
    "Implement a general accuracy function `MyAccuracy(y_true, y_pred)`. Again, implement the function you self from scratch, i.e. do not use any helper functions from Scikit-learn (implementing via `sklearn.metrics.confusion_matrix` is also not allowed, othewise you will then learn nothing!)\n",
    "\n",
    "Reuse your MNIST data loader and test the `MyAccuracy` function  both on your dummy classifier and on the Stochastic Gradient Descent classifier (with setup parameters as in [HOML]).\n",
    "\n",
    "Compare your accuracy score with the acutal value from `sklearn.metrics.accuracy_score()`.\n",
    "\n",
    "(Implementation note: what do you do, if the denominator is zero?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa...\n",
    "\n",
    "def MyAccuracy(y_true, y_pred):\n",
    "    # TODO: you impl here\n",
    "    assert False, \"TODO: solve Qa, and remove me..\"\n",
    "\n",
    "    \n",
    "# TEST FUNCTION: example of a comperator, using Scikit-learn accuracy_score\n",
    "#def TestAccuracy(y_true, y_pred):\n",
    "#    a0=MyAccuracy(y_true, y_pred)\n",
    "#    a1=accuracy_score(y_true, y_pred)\n",
    "#\n",
    "#    print(f\"\\nmy a          ={a0}\")\n",
    "#    print(f\"scikit-learn a={a1}\")\n",
    "#\n",
    "#    # do some numerical comparison here, like\n",
    "#    #  if fabs(a0-a1)<eps then .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb Implement Precision, Recall and $F_1$-score and test it on the MNIST data for both the SGD and Dummy classifier models\n",
    "\n",
    "Now, implement the `MyPrecision`, `MyRecall` and `MyF1Score` functions, again taking MNIST as input, using the SGD and the Dummy classifiers and make some test vectors to compare to the functions found in Scikit-learn...\n",
    "\n",
    "(Implementation note: as before, what do you do, if the denominator is zero?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb..\n",
    "\n",
    "def MyPrecision(y_true, y_pred):\n",
    "    # TODO: you impl here\n",
    "    assert False, \"TODO: solve Qb, and remove me..\"\n",
    "\n",
    "def MyRecall(y_true, y_pred):\n",
    "    # TODO: you impl here\n",
    "    assert False, \"TODO: solve Qb, and remove me..\"\n",
    "    \n",
    "def MyF1Score(y_true, y_pred):\n",
    "    # TODO: you impl here\n",
    "    assert False, \"TODO: solve Qb, and remove me..\"\n",
    "\n",
    "# TODO: your test code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc The Confusion Matrix\n",
    "\n",
    "Revisit your solution to Qb in the `dummy_classifier.ipynb`. Generate the confusion matrix for both the Dummy and the SGD classifier using the `scklearn.metrics.confusion_matrix` function. \n",
    "\n",
    "I got the two confusion matrices\n",
    "\n",
    "```\n",
    "M_dummy=[[18166     0]\n",
    "        [ 1834     0]]\n",
    "   \n",
    "M_SDG=[[17618   548]\n",
    "      [  267  1567]]\n",
    "\n",
    "```\n",
    "your data may look similar (but not 100% equal).\n",
    "\n",
    "How are the Scikit-learn confusion matrix organized, where are the TP, FP, FN and TN located in the matrix indices, and what happens if you mess up the parameters calling\n",
    "\n",
    "```python\n",
    "confusion_matrix(y_test_5_pred, y_test5)\n",
    "```\n",
    "\n",
    "instead of \n",
    "```python\n",
    "confusion_matrix(y_test_5, y_test_5_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc\n",
    "assert False, \"TODO: solve Qc, and remove me.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd A Confusion Matrix Heat-map\n",
    "\n",
    "Generate a _heat map_ image for the confusion matrices, `M_dummy` and `M_SGD` respectively, getting inspiration from [HOML] \"Error Analysis\", pp.122-125.\n",
    "\n",
    "This heat map could be an important guide for you when analysing multiclass data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd\n",
    "assert False, \"TODO: solve Qd, and remove me.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qe Conclusion\n",
    "\n",
    "Now, conclude on all the exercise above. \n",
    "\n",
    "Write a short textual conclusion (max. 10- to 20-lines) that extract the _essence_ of the exercises: why did you think it was important to look at these particular ML concepts, and what was our overall learning outcome of the exercises (in broad terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qe concluding remarks in text.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2018-12-19| CEF, initial.\n",
    "2018-02-07| CEF, updated.\n",
    "2018-02-07| CEF, rewritten accuracy paradox section.\n",
    "2018-03-05| CEF, updated with SHN comments.\n",
    "2019-09-01| CEF, updated for ITMAL v2.\n",
    "2019-09-04| CEF, updated for print-f and added conclusion Q.\n",
    "2020-01-25| CEF, F20 ITMAL update.\n",
    "2020-02-03| CEF, minor text fixes.\n",
    "2020-02-04| CEF, updated page numbers to HOMLv2.\n",
    "2020-02-17| CEF, added implementation note on denominator=0.\n",
    "2020-09-03| CEF, E20 ITMAL update, udpated figs paths.\n",
    "2020-09-06| CEF, added alt text.\n",
    "2020-09-07| CEF, updated HOML page refs.\n",
    "2020-09-21| CEF, fixed factor 2 error in beta-harmonic.\n",
    "2021-01-12| CEF, F21 ITMAL update, moved revision tabel.\n",
    "2021-08-02| CEF, update to E21 ITMAL.\n",
    "2022-01-25| CEF, update to F22 STMAL.\n",
    "2023-02-07| CEF, update HOML page numbers.\n",
    "2023-02-09| CEF, chagned y_train to y_test in conf. matrix call.\n",
    "2023-08-30| CEF, minor table change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Supergruppe diskussion\n",
    "\n",
    "\n",
    "# § 2 \"End-to-End Machine Learning Project\" [HOML]\n",
    "\n",
    "Genlæs kapitel  § 2 og forbered mundtlig præsentation.\n",
    "\n",
    "# Forberedelse inden lektionen\n",
    "\n",
    "Een eller flere af gruppe medlemmer forbereder et mundtligt resume af § 2:\n",
    "\n",
    "* i skal kunne give et kort mundligt resume af hele § 2 til en anden gruppe (på nær, som nævnt, Create the Workspace og Download the Data),\n",
    "\n",
    "* resume holdes til koncept-plan, dvs. prøv at genfortælle, hvad de overordnede linier er i kapitlerne i [HOML].\n",
    "\n",
    "Lav et kort skriftlig resume af de enkelte underafsnit, ca. 5 til 20 liners tekst, se \"TODO\"-template herunder (MUST, til O2 aflevering).\n",
    "\n",
    "Kapitler (incl. underkapitler):\n",
    "\n",
    "* _Look at the Big Picture,_\n",
    "* _Get the Data,_\n",
    "* _Explore and Visualize the Data to Gain Insights,_ \n",
    "* _Prepare the Data for Machine Learning Algorithms,_\n",
    "* _Select and Train a Model,_\n",
    "* _Fine-Tune Your Model,_\n",
    "* _Launch, Monitor, and Maintain Your System,_\n",
    "* _Try It Out!._\n",
    "\n",
    "# På klassen\n",
    "\n",
    "Supergruppe [SG] resume af § 2 End-to-End, ca. 30 til 45 min.\n",
    "\n",
    "* en supergruppe [SG], sammensættes af to grupper [G], on-the-fly på klassen,\n",
    "\n",
    "* hver gruppe [G] forbereder og giver en anden gruppe [G] et mundtligt resume af § 2 til en anden gruppe,\n",
    "\n",
    "* tid: ca. 30 mim. sammenlagt, den ene grupper genfortæller første halvdel af § 2 i ca. 15 min., hvorefter den anden gruppe genfortæller resten i ca. 15 min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume: Look at the Big Picture\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Get the Data\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Explore and Visualize the Data to Gain Insights,\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Select and Train a Model\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Fine-Tune Your Model\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Launch, Monitor, and Maintain Your System\n",
    "\n",
    "TODO resume..\n",
    "\n",
    "## Resume: Try It Out!.\n",
    "\n",
    "TODO resume.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    "---------||\n",
    "2019-01-28| CEF, initial.\n",
    "2020-02-05| CEF, F20 ITMAL update.\n",
    "2021-08-17| CEF, E21 ITMAL update.\n",
    "2021-09-17| CEF, corrected some spell errors.\n",
    "2022-01-28| CEF, update to F22 SWMAL.\n",
    "2022-09-09| CEF, corrected 'MUST for O1' to 'MUST for O2' in text.\n",
    "2023-02-13| CEF, updated to HOML 3rd, removed exclude subsections in 'Get the Data' in this excercise, since the parts with python environments has been removed in HOML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SWMAL Opgave\n",
    "\n",
    "# Dataanalyse\n",
    "\n",
    "## Qa) Beskrivelse af datasæt til O4 projekt\n",
    "\n",
    "I kurset er slutprojektet et bærende element, som I forventes at arbejde på igennem hele kurset\n",
    "sideløbende med de forskellige undervisningsemner. \n",
    "\n",
    "I skal selv vælge et O4 projekt–det anbefales at I vælger en problemstilling, hvor der allerede er data til rådighed og en god beskrivelse af data, dataopsamlingsmetode og problemstilling.\n",
    "\n",
    "I denne opgave skal I:\n",
    "\n",
    "* a) Give en kort konceptmæssig projektbeskrivelse af Jeres ide til O4 projekt. \n",
    "\n",
    "* b) Beskrive jeres valgte datasæt med en kort forklaring af baggrund og hvor I har fået data fra.\n",
    "\n",
    "* c) Beskrive data–dvs. hvilke features, antal samples, target værdier, evt. fejl/usikkerheder, etc.\n",
    "\n",
    "* d) Forklare hvordan I ønsker at anvende datasættet – vil I fx. bruge det til at prædiktere noget\n",
    "bestemt, lave en regression eller klassifikation, el.lign. \n",
    "\n",
    "I vil nok komme til at anvende data også på andre måder i løbet af undervisningen – men det behøver I ikke nævne. Og det er også ok, hvis I ender med at bruge data på en anden måde end planlagt her.\n",
    "\n",
    "Omfang af beskrivelsen forventes at være 1-2 sider.\n",
    "\n",
    "\n",
    "## Qb) Dataanalyse af eget datasæt\n",
    "\n",
    "Lav data analyse på jeres egne data og projekt.\n",
    "\n",
    "Det indebærer de sædvanlige elementer såsom plotte histogrammer, middelværdi/median/spredning, analysere for outliers/korrupte data, forslag til skalering af data og lignende former for analyse af data.\n",
    "\n",
    "For nogle typer data (fx billed-data), hvor features ikke har en specifik betydning, er det mest\n",
    "histogrammer og lignende, som giver mening – det er helt o.k. \n",
    "\n",
    "\n",
    "## NOTE vdr. billeddatasæts\n",
    "\n",
    "For billeddata fer hver pixel en feature, og alm. analyse beskrevet ovenfor giver ikke indsigt. Prøv i stedet for billeder at beskrive billedformater (JPEG, PNG osv. / RGB, HSV, gråtone, multispektral, etc.), størrelser af billeder, hvordan de er repræsenteret på disk (dirs osv.)\n",
    "\n",
    "Giv også eksempler på billeder og evt. labels i billedesæt.\n",
    "\n",
    "Histogrammer kan udføres på enkelte billeder, men kun i forbindelse med labelede områder---og bedst på billesæt med ens baggrunde.\n",
    "\n",
    "Benytter i lyddata eller video gælder de samme begrænsinger som får billeder her.\n",
    "\n",
    "## NOTE vdr. valg af datasæt til O4\n",
    "\n",
    "I har frie hænder til at vælge O4 projekt og tilhørende datasæt og valg af datasæt og ide til O4 her er ikke endelig. \n",
    "\n",
    "Dvs. at i løbende kan modificere projektbeskrivelse og, evt. om nødvendigt, vælge et andet datasæt senere, hvis jeres nuværende valg viser sig umuligt (men er en dyr proces). \n",
    "\n",
    "Scope af O4 projekt bør også begrænses, så det passer til kurset og til den '_time-box_'ede aflevering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-|\n",
    "2021-08-17| CEF, moved from Word to Notebook.\n",
    "2021-11-08| CEF, elaborated on image based data.\n",
    "2022-01-25| CEF, update to F22 SWMAL.\n",
    "2023-02-19| CEF, updated to F23 SWMAL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "# Pipelines\n",
    "\n",
    "We now try building af ML pipeline. The data for this exercise is the same as in L01, meaning that the OECD data from the 'intro.ipynb' have been save into a Python 'pickle' file. \n",
    "\n",
    "The pickle library is a nifty data preservation method in Python, and from L01 the tuple `(X, y)` have been stored to the pickle file `itmal_l01_data.pkl', try reloading it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def LoadDataFromL01():\n",
    "    filename = \"Data/itmal_l01_data.pkl\"\n",
    "    with open(f\"{filename}\", \"rb\") as f:\n",
    "        (X, y) = pickle.load(f)\n",
    "        return X, y\n",
    "\n",
    "X, y = LoadDataFromL01()\n",
    "\n",
    "print(f\"X.shape={X.shape},  y.shape={y.shape}\")\n",
    "\n",
    "assert X.shape[0] == y.shape[0]\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1  # did a y.ravel() before saving to picke file\n",
    "assert X.shape[0] == 29\n",
    "\n",
    "# re-create plot data (not stored in the Pickel file)\n",
    "m = np.linspace(0, 60000, 1000)\n",
    "M = np.empty([m.shape[0], 1])\n",
    "M[:, 0] = m\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Revisiting the problem with the MLP\n",
    "\n",
    "Using the MLP for the QECD data in Qd) from `intro.ipynb` produced a negative $R^2$, meaning that it was unable to fit the data, and the MPL model was actually _worse_ than the naive $\\hat y$ (mean value of y).\n",
    "\n",
    "Let's just revisit this fact. When running the next cell you should now see an OK $~R^2_{lin.reg}~$ score and a negative $~R^2_{mlp}~$ score.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the MLP and lin. regression again..\n",
    "\n",
    "def isNumpyData(t: np.ndarray, expected_ndim: int):\n",
    "    assert isinstance(expected_ndim, int), f\"input parameter 'expected_ndim' is not an integer but a '{type(expected_ndim)}'\"\n",
    "    assert expected_ndim>=0, f\"expected input parameter 'expected_ndim' to be >=0, got {expected_ndim}\"\n",
    "    if t is None:\n",
    "        print(\"input parameter 't' is None\", file=sys.stderr)\n",
    "        return False\n",
    "    if not isinstance(t, np.ndarray):\n",
    "        print(\"excepted numpy.ndarray got type '{type(t)}'\", file=sys.stderr)\n",
    "        return False\n",
    "    if not t.ndim==expected_ndim:\n",
    "        print(\"expected ndim={expected_ndim} but found {t.ndim}\", file=sys.stderr)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def PlotModels(model1, model2, X: np.ndarray, y: np.ndarray, name_model1: str, name_model2: str):\n",
    "    \n",
    "    # NOTE: local function is such a nifty feature of Python!\n",
    "    def CalcPredAndScore(model, X: np.ndarray, y: np.ndarray,):\n",
    "        assert isNumpyData(X, 2) and isNumpyData(y, 1) and X.shape[0]==y.shape[0]\n",
    "        y_pred_model = model.predict(X)\n",
    "        score_model = r2_score(y, y_pred_model) # call r2\n",
    "        return y_pred_model, score_model    \n",
    "\n",
    "    assert isinstance(name_model1, str) and isinstance(name_model2, str)\n",
    "\n",
    "    y_pred_model1, score_model1 = CalcPredAndScore(model1, X, y)\n",
    "    y_pred_model2, score_model2 = CalcPredAndScore(model2, X, y)\n",
    "\n",
    "    plt.plot(X, y_pred_model1, \"r.-\")\n",
    "    plt.plot(X, y_pred_model2, \"kx-\")\n",
    "    plt.scatter(X, y)\n",
    "    plt.xlabel(\"GDP per capita\")\n",
    "    plt.ylabel(\"Life satisfaction\")\n",
    "    plt.legend([name_model1, name_model2, \"X OECD data\"])\n",
    "\n",
    "    l = max(len(name_model1), len(name_model2))\n",
    "    \n",
    "    print(f\"{(name_model1).rjust(l)}.score(X, y)={score_model1:0.2f}\")\n",
    "    print(f\"{(name_model2).rjust(l)}.score(X, y)={score_model2:0.2f}\")\n",
    "\n",
    "# lets make a linear and MLP regressor and redo the plots\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10, ),\n",
    "                   solver='adam',\n",
    "                   activation='relu',\n",
    "                   tol=1E-5,\n",
    "                   max_iter=100000,\n",
    "                   verbose=False)\n",
    "linreg = LinearRegression()\n",
    "\n",
    "mlp.fit(X, y)\n",
    "linreg.fit(X, y)\n",
    "\n",
    "print(\"The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\\n\")\n",
    "PlotModels(linreg, mlp, X, y, \"lin.reg\", \"MLP\")\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qa) Create a Min/max scaler for the MLP\n",
    "\n",
    "Now, the neurons in neural networks normally expect input data in the range `[0;1]` or sometimes in the range `[-1;1]`, meaning that for value outside this range then the neuron will saturate to its min or max value (also typical `0` or `1`). \n",
    "\n",
    "A concrete value of `X` is, say 22.000 USD, that is far away from what the MLP expects. Af fix to the problem in Qd), from `intro.ipynb`, is to preprocess data by scaling it down to something more sensible.\n",
    "\n",
    "Try to manually scale X to a range of `[0;1]`, re-train the MLP, re-plot and find the new score from the rescaled input. Any better?\n",
    "\n",
    "(If you already made exercise \"Qe) Neural Network with pre-scaling\" in L01, then reuse Your work here!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: rescale X and refit the model(s)..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb) Scikit-learn Pipelines\n",
    "\n",
    "Now, rescale again, but use the `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "When this works put both the MLP and the scaler into a composite construction via `sklearn.pipeline.Pipeline`. This composite is just a new Scikit-learn estimator, and can be used just like any other `fit-predict` models, try it, and document it for the journal.\n",
    "\n",
    "(You could reuse the `PlotModels()` function by also retraining the linear regressor on the scaled data, or just write your own plot code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: put everything into a pipeline..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc) Outliers and the Min-max Scaler vs. the Standard Scaler\n",
    "\n",
    "Explain the fundamental problem with a min-max scaler and outliers. \n",
    "\n",
    "Will a `sklearn.preprocessing.StandardScaler` do better here, in the case of abnormal feature values/outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: research the problem here..\n",
    "assert False, \"TODO: investigate outlier problems and try a StandardScaler..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd) Modify the MLP Hyperparameters\n",
    "\n",
    "Finally, try out some of the hyperparameters associated with the MLP.\n",
    "\n",
    "Specifically, test how few neurons the MLP can do with---still producing a sensible output, i.e. high $R^2$. \n",
    "\n",
    "Also try-out some other activation functions, ala sigmoid, and solvers, like `sgd`.\n",
    "\n",
    "Notice, that the Scikit-learn MLP does not have as many adjustable parameters, as a Keras MLP, for example, the Scikit-learn MLP misses neurons initialization parameters (p.333-334 [HOML,2nd], p.358-359 [HOML,3rd]) and the ELU activation function (p.336 [HOML,2nd], p.363 [HOML,3rd).\n",
    "\n",
    "[OPTIONAL 1]: use a Keras MLP regressor instead of the Scikit-learn MLP (You need to install the  Keras if its not installed as default).\n",
    "\n",
    "[OPTIONAL 2]: try out the `early_stopping` hyperparameter on the `MLPRegressor`. \n",
    "\n",
    "[OPTIONAL 3]: try putting all score-calculations into K-fold cross-validation  methods readily available in Scikit-learn using\n",
    "\n",
    "* `sklearn.model_selection.cross_val_predict`\n",
    "* `sklearn.model_selection.cross_val_score` \n",
    "\n",
    "or similar (this is, in theory, the correct method, but can be hard to use due to the  extremely small number of data points, `n=29`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: test out various hyperparameters for the MLP..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-|\n",
    "2020-10-15| CEF, initial. \n",
    "2020-10-21| CEF, added Standard Scaler Q.\n",
    "2020-11-17| CEF, removed orhpant text in Qa (moded to Qc).\n",
    "2021-02-10| CEF, updated for ITMAL F21.\n",
    "2021-11-08| CEF, updated print info.\n",
    "2021-02-10| CEF, updated for SWMAL F22.\n",
    "2023-02-19| CEF, updated for SWMAL F23, adjuste page numbers for 3rd.ed.\n",
    "2023-02-21| CEF, added types, rewrote CalcPredAndScore and added isNumpyData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Training a Linear Regressor I \n",
    "\n",
    "The goal of the linear regression is to find the argument $w$ that minimizes the sum-of-squares error over all inputs. \n",
    "\n",
    "Given the usual ML input data matrix $\\mathbf X$ of size $(n,d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "    \\renewcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\renewcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\renewcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\renewcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\renewcommand\\st[1]{_{\\textrm{\\scriptsize #1}}}\n",
    "    \\renewcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\renewcommand\\obs[2]{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\renewcommand\\diff[1]{\\mathrm{d}#1}\n",
    "    \\renewcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\renewcommand\\pfrac[2]{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\renewcommand\\dfrac[2]{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "            \\vdots      &             &        & \\vdots \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "        }\n",
    "$$\n",
    "\n",
    "and $\\by$ is the target output column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\by =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1} \\\\\n",
    "     y\\pown{2} \\\\\n",
    "     \\vdots \\\\\n",
    "     y\\pown{n} \\\\\n",
    "  }\n",
    "$$\n",
    "\n",
    "The linear regression model, via its hypothesis function and for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$ (with the additional element $w_0$ being the bias), can now be written as simple as\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  h(\\bx\\powni;\\bw) &= \\bw^\\top \\ac{c}{1\\\\\\bx\\powni} \\\\\n",
    "                   &= w_0 + w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni\n",
    "}\n",
    "$$\n",
    "\n",
    "using the model parameters or weights, $\\bw$, aka $\\btheta$. To ease notation $\\bx$ is assumed to have the 1 element prepended in the following so that $\\bx$ is a $d+1$ column vector\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} &\\mapsto \\bx\\powni, ~~~~\\textrm{by convention in the following...}\\\\\n",
    "  h(\\bx\\powni;\\bw) &= \\bw^\\top \\bx\\powni \n",
    "}\n",
    "$$\n",
    "\n",
    "This is actually the first fully white-box machine learning algorithm, that we see. All the glory details of the algorithm are clearly visible in the internal vector multiplication...quite simple, right? Now we just need to train the weights...\n",
    " \n",
    "## Loss or Objective Function - Formulation for Linear Regression\n",
    "\n",
    "The individual cost (or loss), $L\\powni$, for a single input-vector $\\bx\\powni$ is a measure of how the model is able to fit the data: the higher the $L\\powni$ value the worse it is able to fit. A loss of $L=0$ means a perfect fit.\n",
    "\n",
    "It can be given by, say, the square difference from the calculated output, $h$, to the desired output, $y$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || h(\\bx\\powni;\\bw)      - y\\powni ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - y\\powni ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - y\\powni \\right)^2 \n",
    "}\n",
    "$$\n",
    "when $L$ is based on the $\\norm{2}^2$ norm, and only when $y$ is in one dimension.\n",
    "\n",
    "To minimize all the $L\\powni$ losses (or indirectly also the MSE or RMSE) is to minimize the sum of all the\n",
    "individual costs, via the total cost function $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "\\textrm{MSE}(\\bX,\\by;\\bw) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                    &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni \\right)^2\\\\\n",
    "                    &= \\frac{1}{n} ||\\bX \\bw - \\by||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "here using the squared Euclidean norm, $\\norm{2}^2$, via the $||\\cdot||_2^2$ expressions.\n",
    "\n",
    "Now the factor $\\frac{1}{n}$ is just a constant and can be ignored, yielding the total cost function\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    J &= \\frac{1}{2} ||\\bX \\bw - \\by||_2^2\\\\\n",
    "     &\\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "adding yet another constant, 1/2, to ease later differentiation of $J$.\n",
    "\n",
    "## Training\n",
    "\n",
    "Training the linear regression model now amounts to computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total cost\n",
    "\n",
    "$$\n",
    " \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "where $\\textrm{argmin}_\\bw$ means find the argument of $\\bw$ that minimizes the $J$ function. This minimum (sometimes a maximum, via argmax) is denoted $\\bw^*$ in most ML literature. \n",
    "\n",
    "The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always form a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from server.\" style=\"height:240px\">\n",
    "\n",
    "## Training: The Closed-form Solution\n",
    "\n",
    "To solve for $\\bw^*$ in closed form (i.e. directly, without any numerical approximation), we find the gradient of $J$ with respect to $\\bw$. Taking the partial deriverty $\\partial/\\partial_\\bw$ of the $J$ via the gradient (nabla) operator\n",
    "\n",
    "$$\n",
    "\\rem{\n",
    " \\frac{\\partial}{\\partial \\bw} = \n",
    "   \\ac{c}{\n",
    "     \\frac{\\partial}{\\partial w_1} \\\\\n",
    "     \\frac{\\partial}{\\partial w_2} \\\\\n",
    "     \\vdots\\\\\n",
    "     \\frac{\\partial}{\\partial w_d}\n",
    "   }\n",
    "}    \n",
    " \\nabla_\\bw~J = \n",
    "   \\left[ \\frac{\\partial J}{\\partial w_1}, \\frac{\\partial J}{\\partial w_2}, \\ldots ,  \\frac{\\partial J}{\\partial w_m}   \\right]^\\top\n",
    "$$\n",
    "     \n",
    "and setting it to zero yields the optimal solution for $\\bw$, and ignoring all constant factors of 1/2\n",
    "and $1/n$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J(\\bw) &= \\bX^\\top \\left( \\bX \\bw - \\by \\right) ~=~ 0\\\\\n",
    "                  0 &= \\bX^\\top\\bX \\bw - \\bX^\\top\\by\n",
    "}\n",
    "$$\n",
    "\n",
    "giving the closed-form solution, with $\\by = [y\\pown{1}, y\\pown{2}, \\cdots,\n",
    "y\\pown{n}]^\\top$\n",
    "\n",
    "$$\n",
    "\\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1} \\bX^\\top \\by\n",
    "$$\n",
    "\n",
    "You already know this method from math, finding the extrema for a function, say \n",
    "\n",
    "$$f(w)=w^2-2w-2$$ \n",
    "\n",
    "so is given by finding the place where the gradient $\\mathrm{d}~f(w)/\\mathrm{d}w = 0$\n",
    "\n",
    "$$\n",
    "   \\dfrac{f(w)}{w} = 2w -2 = 0\n",
    "$$\n",
    "\n",
    "so we see that there is an extremum at $w=1$. Checking the second deriverty tells if we are seeing a minimum, maximum or a saddlepoint at that point. In matrix terms, this corresponds to finding the _Hessian_ matrix and gets notational tricky due to the multiple feature dimensions involved.\n",
    "\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Ordinary_least_squares\n",
    "\n",
    "> https://en.wikipedia.org/wiki/Hessian_matrix\n",
    "\n",
    "## Qa Write a Python function that uses the closed-form to find $\\bw^*$\n",
    "\n",
    "Use the test data, `X` and `y` in the code below to find `w` via the closed-form. Use the test vectors for `w` to test your implementation, and remember to add the bias term (concat an all-one vector to `X` before solving). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa...\n",
    "\n",
    "# TEST DATA: \n",
    "import numpy as np\n",
    "from libitmal import utils as itmalutils\n",
    "\n",
    "def GenerateData():\n",
    "    X = np.array([[8.34044009e-01],[1.44064899e+00],[2.28749635e-04],[6.04665145e-01]])\n",
    "    y = np.array([5.97396028, 7.24897834, 4.86609388, 3.51245674])\n",
    "    return X, y\n",
    "\n",
    "X, y = GenerateData()\n",
    "assert False, \"find the least-square solution for X and y, your implementation here, say from [HOML, p.114 2nd./p.134 3rd.]\"\n",
    "\n",
    "# w = ...\n",
    "\n",
    "\n",
    "# TEST VECTOR:\n",
    "w_expected = np.array([4.046879011698, 1.880121487278])\n",
    "itmalutils.PrintMatrix(w, label=\"w=\", precision=12)\n",
    "itmalutils.AssertInRange(w, w_expected, eps=1E-9)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb Find the limits of the least-square method\n",
    "\n",
    "Again find the least-square optimal value for `w` now using then new `X` and `y` as inputs.\n",
    "\n",
    "Describe the problem with the matrix inverse, and for what `M` and `N` combinations do you see, that calculation of the matrix inverse takes up long time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb...\n",
    "def GenerateData(M, N):\n",
    "    # TEST DATA: Matrix, taken from [HOML]\n",
    "    print(f'GenerateData(M={N}, N={N})...')\n",
    "    \n",
    "    assert M>0\n",
    "    assert N>0\n",
    "    assert isinstance(M, int)\n",
    "    assert isinstance(N, int)\n",
    "\n",
    "    # NOTE: not always possible to invert a random matrix; \n",
    "    #       it becomes sigular, hence a more elaborate choice \n",
    "    #       of values below (but still a hack): \n",
    "    X=2 * np.ones([M, N])\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i,0]=i*4\n",
    "    for j in range(X.shape[1]):\n",
    "        X[0,j]=-j*4\n",
    "\n",
    "    y=4 + 3*X + np.random.randn(M,1)\n",
    "    y=y[:,0] # well, could do better here!\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = GenerateData(M=10000, N=20)\n",
    "assert False, \"find the least-square solution for X and y, again\"\n",
    "\n",
    "# w = \n",
    "\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2018-12-18| CEF, initial.\n",
    "2018-02-14| CEF, major update.\n",
    "2018-02-18| CEF, fixed error in nabla expression.\n",
    "2018-02-18| CEF, added minimization plot.\n",
    "2018-02-18| CEF, added note on argmin/max.\n",
    "2018-02-18| CEF, changed concave to convex.\n",
    "2021-09-26| CEF, update for ITMAL E21.\n",
    "2011-10-02| CEF, corrected page numbers for HOML v2 (109=>114).\n",
    "2022-03-09| CEF, elaboreted on code and introduced GetData().\n",
    "2023-02-22| CEF, updated page no to HOML 3rd. ed., updated to SWMAL F23.\n",
    "2023-09-19| CEF, changed LaTeX mbox and newcommand (VSCode error) to textrm/mathrm and renewcommand.\n",
    "2023-09-28| CEF, elaborated on L-expressions from (..)^2 to \\|\\| .. \\|\\|_2^2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Gradient Descent Methods and Training\n",
    "\n",
    "Finding the optimal solution in one-step, via \n",
    "\n",
    "$$\n",
    "    \\renewcommand\\rem[1]{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no newlines in defs}\n",
    "    \\renewcommand\\eq[2]{#1 &=& #2\\\\}\n",
    "    \\renewcommand\\ar[2]{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\renewcommand\\ac[2]{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\renewcommand\\st[1]{_{\\textrm{\\scriptsize #1}}}\n",
    "    \\renewcommand\\norm[1]{{\\cal L}_{#1}}\n",
    "    \\renewcommand\\obs[2]{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\renewcommand\\diff[1]{\\mathrm{d}#1}\n",
    "    \\renewcommand\\pown[1]{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\def\\bX{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bw{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\renewcommand\\pfrac[2]{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\renewcommand\\dfrac[2]{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "\\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1} \\bX^\\top \\by\n",
    "$$\n",
    "\n",
    "has its downsides: the scaling problem of the matrix inverse. Now, let us look at a numerical solution to the problem of finding the value of $\\bw$  (aka $\\btheta$) that minimizes the objective function $J$.\n",
    "\n",
    "Again, ideally we just want to find places, where the (multi-dimensionally) gradient of $J$ is zero (here using a constant factor $\\frac{2}{m}$)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J(\\bw) &= \\frac{2}{m} \\bX^\\top \\left( \\bX \\bw - \\by \\right)\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "and numerically we calculate $\\nabla_{\\bw} J$ for a point in $\\bw$-space, and then move along in the opposite direction of this gradient, taking a step of size $\\eta$\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)} = \\bw^{(step~N)} - \\eta \\nabla_{\\bw} J(\\bw)\n",
    "$$\n",
    "\n",
    "That's it, pretty simple, right (apart from numerical stability, problem with convergence and regularization, that we will discuss later).\n",
    "\n",
    "So, we begin with some initial $\\bw$, and iterate via the equation above, towards places, where $J$ is smaller, and this can be illustrated as\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from server.\" style=\"height:240px\">\n",
    "\n",
    "\n",
    "If we hit the/a global minimum or just a local minimum (or in extremely rare cases a local saddle point) is another question when not using a simple linear regression model: for non-linear models we will in general not see a nice convex $J$-$\\bw$ surface, as in the figure above.\n",
    "\n",
    "## Qa The Gradient Descent Method (GD)\n",
    "\n",
    "Explain the gradient descent algorithm using the equations in the section _'§ Bach Gradient Descent'_ [HOML, p.121-122 2nd, p.143 3rd], and relate it to the code snippet \n",
    "\n",
    "```python\n",
    "X_b, y = GenerateData()\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "``` \n",
    "in the python code below. \n",
    "\n",
    "As usual, avoid going top much into details of the code that does the plotting.\n",
    "\n",
    "What role does `eta` play, and what happens if you increase/decrease it (explain the three plots)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa...examine the method (without the plotting)\n",
    "\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def GenerateData():\n",
    "    X = 2 * np.random.rand(100, 1)\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "    X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "    return X, X_b, y\n",
    "\n",
    "X, X_b, y = GenerateData()\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "print(f'stochastic gradient descent theta={theta.ravel()}')\n",
    "\n",
    "#############################\n",
    "# rest of the code is just for plotting, needs no review\n",
    "\n",
    "def plot_gradient_descent(theta, eta, theta_path=None):\n",
    "    m = len(X_b)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    n_iterations = 1000\n",
    "    for iteration in range(n_iterations):\n",
    "        if iteration < 10:\n",
    "            y_predict = X_new_b.dot(theta)\n",
    "            style = \"b-\" if iteration > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        \n",
    "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "        if theta_path is not None:\n",
    "            theta_path.append(theta)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.axis([0, 2, 0, 15])\n",
    "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta_path_bgd = []\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
    "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb The Stochastic Gradient Descent Method (SGD)\n",
    "\n",
    "Now, introducing the _stochastic_ variant of gradient descent, explain the stochastic nature of the SGD, and comment on the difference to the _normal_ gradient descent method (GD) we just saw.\n",
    "\n",
    "Also explain the role of the calls to `np.random.randint()` in the code, \n",
    "\n",
    "HINT: In detail, the important differences are, that the main loop for SGC is \n",
    "```python\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = ...\n",
    "        theta = ...\n",
    "```\n",
    "where it for the GD method was just\n",
    "```python\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = ..\n",
    "```\n",
    "\n",
    "NOTE: the call `np.random.seed(42)` resets the random generator so that it produces the same random-sequence when re-running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb...run this code\n",
    "\n",
    "# NOTE: code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "theta_path_sgd = []\n",
    "m = len(X_b)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        if epoch == 0 and i < 20:\n",
    "            y_predict = X_new_b.dot(theta) \n",
    "            style = \"b-\" if i > 0 else \"r--\"\n",
    "            plt.plot(X_new, y_predict, style)\n",
    "        \n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)        \n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_sgd.append(theta)                 \n",
    "\n",
    "        plt.plot(X, y, \"b.\")      \n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "print(f'stochastic gradient descent theta={theta.ravel()}')\n",
    "print(f'Scikit-learn SGDRegressor \"thetas\": sgd_reg.intercept_={sgd_reg.intercept_}, sgd_reg.coef_={sgd_reg.coef_}')\n",
    "\n",
    "#############################\n",
    "# rest of the code is just for plotting, needs no review \n",
    "plt.xlabel(\"$x_1$\", fontsize=18)                     \n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)           \n",
    "plt.axis([0, 2, 0, 15])                              \n",
    "\n",
    "plt.show()        \n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc Adaptive learning rate for $\\eta$  \n",
    "\n",
    "There is also an adaptive learning rate method in the demo code for the SGD. \n",
    "\n",
    "Explain the effects of the `learning_schedule()` functions.\n",
    "\n",
    "You can set the learning rate parameter (also known as a hyperparameter) in may ML algorithms, say for SGD regression, to a method of your choice \n",
    "\n",
    "```python\n",
    "SGDRegressor(max_iter=1,\n",
    "             eta0=0.0005,\n",
    "             learning_rate=\"constant\", # or 'adaptive' etc.\n",
    "             random_state=42)\n",
    "```\n",
    "\n",
    "but as usual, there is a bewildering array of possibilities...we will tackle this problem later when searching for the optimal hyperparameters.\n",
    "\n",
    "NOTE: the `learning_schedule()` method could also have been used in the normal SG algorithm; is not directly part of the stochastic method, but a concept in itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qc...in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd Mini-batch Gradient Descent Method \n",
    "\n",
    "Finally explain what a __mini-batch__ SG method is, and how it differs from the two others.\n",
    "\n",
    "Again, take a peek into the demo code below, to extract the algorithm details...and explain the __main differences__, compared with the GD and SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd...run this code\n",
    "\n",
    "# NOTE: code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "theta_path_mgd = []\n",
    "\n",
    "n_iterations = 50\n",
    "minibatch_size = 20\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "t0, t1 = 200, 1000\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "t = 0\n",
    "for epoch in range(n_iterations):\n",
    "    shuffled_indices = np.random.permutation(m)\n",
    "    X_b_shuffled = X_b[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(0, m, minibatch_size):\n",
    "        t += 1\n",
    "        xi = X_b_shuffled[i:i+minibatch_size]\n",
    "        yi = y_shuffled[i:i+minibatch_size]\n",
    "        \n",
    "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(t)\n",
    "        theta = theta - eta * gradients\n",
    "        theta_path_mgd.append(theta)\n",
    "\n",
    "print(f'mini-batch theta={theta.ravel()}')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qe Choosing a Gradient Descent Method\n",
    "\n",
    "Explain the $θ_0−θ_1$ plot below, and make a comment on when to use GD/SGD/mini-batch gradient descent (pros and cons for the different methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qd...run this code\n",
    "\n",
    "# NOTE: code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "theta_path_bgd = np.array(theta_path_bgd)\n",
    "theta_path_sgd = np.array(theta_path_sgd)\n",
    "theta_path_mgd = np.array(theta_path_mgd)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
    "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
    "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
    "plt.legend(loc=\"upper left\", fontsize=16)\n",
    "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
    "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
    "plt.show()\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL]  Qf Extend the MyRegressor \n",
    "\n",
    "NOTE: this excercise only possible if `linear_regression_2.ipynb` has been solved.\n",
    "\n",
    "Can you extend the `MyRegressor` class from the previous `linear_regression_2.ipynb` notebook, adding a numerical train method? Choose one of the gradient descent methods above...perhaps starting with a plain SG method.\n",
    "\n",
    "You could add a parameter for the class, indicating it what mode it should be operating: analytical closed-form or numerical, like\n",
    "\n",
    "```python  \n",
    "class MyRegressor(BaseEstimator):\n",
    "    def __init__(self, numerical = False):\n",
    "        self.__w = None\n",
    "        self.__numerical_mode = numerical\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qf...[OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2018-02-14| CEF, initial.\n",
    "2018-02-14| CEF, added optional exe.\n",
    "2018-02-20| CEF, major update.\n",
    "2018-02-20| CEF, fixed revision table malformatting.\n",
    "2018-02-25| CEF, removed =0 in expression.\n",
    "2018-02-25| CEF, minor text updates and made Qc optional.\n",
    "2018-02-25| CEF, minor source code cleanups.\n",
    "2021-09-18| CEF, update to ITMAL E21.\n",
    "2021-10-02| CEF, corrected link to extra material and page numbers for HOML v2 (114/115=>121/122).\n",
    "2022-01-25| CEF, update to SWMAL F22.\n",
    "2023-02-22| CEF, updated page no to HOML 3rd. ed., updated to SWMAL F23\n",
    "2023-09-19| CEF, changed LaTeX mbox and newcommand (VSCode error) to textrm/mathrm and renewcommand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "#  Artificial Neural Networks as Universal Approximators\n",
    "\n",
    "An ANN can in principle approximate any n-dimensional function: given enough neurons (and layers) a ANN is an _universal approximator_.\n",
    "\n",
    "Let us test this by using a very simple ANN consisting of only two neurons in a hidden layer(and an input- and output-layer both with the identity activation function, _I_ ).\n",
    "\n",
    "Given a `tanh` activation function in a neuron, it can only approximate something similar to this monotonic function, but applying two neurons in a pair, they should be able to approximate an up-hill-then-downhill non-monotonic function, which is a simple function with a single maximum. \n",
    "\n",
    "We use Scikit-learns `MLPRegressor` for this part of the exercise. Use the synthetic data, generated by the `GenerateSimpleData()` functions, in the next cells and train the MLP to make it fit the curve. \n",
    "\n",
    "Notice the lack of a train-test split in the exercise; since we only want to look at the approximation capabilities of the MLP, the train-test split is omitted, (and you are welcome to do the split yourself, and also to add noise in the data generators.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One data generator just to test out the MLP..\n",
    "#   An MLP with just two neurons should be able to approximate this simple\n",
    "#   down-up graph using its two non-linear sigmoid or tanh neurons...\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def GenerateSimpleData():\n",
    "    X = np.linspace(-10, 10, 100)\n",
    "    y = 2*np.tanh(2*X - 12) - 3*np.tanh(2*X - 4)  \n",
    "    y = 2*np.tanh(2*X + 2)  - 3*np.tanh(2*X - 4)   \n",
    "    X = X.reshape(-1, 1) # Scikit-algorithms needs matrix in (:,1)-format\n",
    "    return X,y\n",
    "\n",
    "X, y_true = GenerateSimpleData()\n",
    "plt.plot(X, y_true, \"r-.\")\n",
    "plt.legend([\"y_true\"])\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, Groundtruth data simple\")\n",
    "           \n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qa)\n",
    "\n",
    "Fit the model using the data generator and the MLP in the next cell. \n",
    "\n",
    "Then plot `y_true` and `y_pred` in a graph, and extract the network weights and bias coefficients (remember the `coefs_` and `intercepts_` attributes you found on a linear regressor in an earlier exercise, the MLP is similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP and fit model, just run..\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp = MLPRegressor(activation = 'tanh',      # activation function \n",
    "                   hidden_layer_sizes = [2], # layes and neurons in layers: one hidden layer with two neurons\n",
    "                   alpha = 1e-5,             # regularization parameter\n",
    "                   solver = 'lbfgs',         # quasi-Newton solver\n",
    "                   max_iter=10000,\n",
    "                   verbose = True)\n",
    "\n",
    "mlp.fit(X, y_true)\n",
    "y_pred = mlp.predict(X)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the fit.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb)\n",
    "\n",
    "Draw the ANN with its input-, hidden- and output-layer. Remember the bias input to the input- and hidden-layer (a handmade drawing is fine).\n",
    "\n",
    "Now, add the seven weights extracted from the MLP attributes to the drawing: four w coefficients and three bias coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: extract and print all coefficients.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc)\n",
    "\n",
    "Create a mathematical formula for the network ala\n",
    "\n",
    "    y_math = 0.3* tanh(2 * x + 0.1) - 0.3 * tanh(5 * x + 3) + 0.9\n",
    "\n",
    "with the seven weights found before, two or three decimals should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create formula.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd)\n",
    "\n",
    "Plot the `y_math` function using `np.tanh` and `X` as input similar to  \n",
    "\n",
    "    y_math = 0.3*np.tanh(2 * X + ..\n",
    "   \n",
    "and compare `y_math` with `y_pred` and `y_true` in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the formula.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qe)\n",
    "\n",
    "Plot the first half of the function ala\n",
    "\n",
    "    y_math_first_part = 0.3* tanh(2 * X + 0.1)\n",
    "   \n",
    "and then plot the second part. The sum of these two parts gives the total value of y_math if you also add them with the last bias part.\n",
    "\n",
    "Are the first and second parts similar to a monotonic tanh activation function, and explain the ability of the two-neuron network to be a general approximator for the input function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the first and second half of the formula.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qf)\n",
    "\n",
    "Now we change the data generator to a `sinc`-like function, which is a function that needs a NN with a higher capacity than the previous simple data.\n",
    "\n",
    "Extend the MLP with more neurons and more layers, and plot the result. Can you create a good approximation for the `sinc` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateSincData():\n",
    "    # A Sinc curve, approximation needs more neurons to capture the 'ringing'...\n",
    "    X = np.linspace(-3, 3, 1000) \n",
    "    y = np.sinc(X)\n",
    "    X = X.reshape(-1,1)\n",
    "    return X, y\n",
    "\n",
    "X, y_true = GenerateSincData()\n",
    "plt.plot(X, y_true, \"r-\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"ANN, Groundtruth data for Sinc\")\n",
    "\n",
    "# TODO:\n",
    "assert False, \"TODO: instantiate and train an MLP on the sinc data..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [OPTIONAL] Qg) \n",
    "\n",
    "Change the hyperparameters in the MLP, say the `alpha` to `1e5` and `1e-1`, and explain the results (hint: regularization).\n",
    "\n",
    "Also, try out different `activation` functions `learning_rate`s and `solver`s, or other interesting hyperparameters found on the MLP regressor in the documentation.\n",
    "\n",
    "Finally, implement the MLP regressor in `Keras` instead.\n",
    "\n",
    "(Solvers aka. optimizers and regularization will be discussed in a later lecture.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: do some experiments.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2021-10-04| CEF, initial, converted from old word format.\n",
    "2021-10-04| CEF, inserted ANN_example.py into Notebook.\n",
    "2023-03-06| CEF, minor table update.\n",
    "2023-03-09| CEF, major update, translated to English, elaborated on NNs as Universal Approximator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "\n",
    "Excercise 9 from [HOML], p.496 2nd./p.535 3rd. (slighty modified):\n",
    "\n",
    "__\"9. Build an CNN via the Keras API and try to achieve the highest possible accuracy on MNIST.\"__\n",
    "\n",
    "For the journal: \n",
    "\n",
    "* write an introduction to CNNs (what are CNNs, what is a convolution layer, etc..), \n",
    "* document your experiments towards the end-goal of reaching 'a high accuracy' (what did you try, what work/did not work), \n",
    "* document how you use '_generalization_' in your setup (us of simple hold-out/train-test split or k-fold, or etc..),\n",
    "* produce some sort of '_learning-curve_' that illustrates the drop in cost- or increase in score-function with respect to, say training iteration (for inspiration see fig 4.20, 10-12 or 10.17 in [HOML]),\n",
    "* document the final CNN setup (layers etc., perhaps as a graph/drawing), \n",
    "* discus on your iterations towards the end-goal and other findings you had,\n",
    "* and, as always, write a conclusion.\n",
    "\n",
    "If you use a code template from slides, HOML or the internet, then remember to add a reference to the original work in you journal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CNN implemetation via Keras.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2021-10-20| CEF, initial version, clone from [HOML].\n",
    "2021-10-26| CEF, added learning curve item.\n",
    "2022-01-25| CEF, update to SWMAL F22.\n",
    "2023-03-08| CEF, updated page no to HOML 3rd. ed., updated to SWMAL F23.\n",
    "2023-03-15| CEF, removed wording \"from scratch\", replaced with \"via the Keras API\" and added comment about references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "# Generalization Error\n",
    "\n",
    "In this exercise, we need to explain all important overall concepts in training. Let's begin with Figure 5.3 from Deep Learning (Ian Goodfellow, et. al. [DL]), that pretty much sums it all up\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L08/Figs/dl_generalization_error.png\" alt=\"WARNING: could not get image from server.\" style=\"height:500px\">\n",
    "\n",
    "\n",
    "## Qa) On Generalization Error\n",
    "\n",
    "Write a detailed description of figure 5.3 (above) for your hand-in.\n",
    " \n",
    "All concepts in the figure must be explained \n",
    "\n",
    "* training/generalization error, \n",
    "* underfit/overfit zone, \n",
    "* optimal capacity, \n",
    "* generalization gab, \n",
    "* and the two axes: x/capacity, y/error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ...in text\n",
    "assert False, \"TODO: write some text..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb A MSE-Epoch/Error Plot\n",
    "\n",
    "Next, we look at a SGD model for fitting polynomial, that is _polynomial regression_ similar to what Géron describes in [HOML] (\"Polynomial Regression\" + \"Learning Curves\"). \n",
    "\n",
    "Review the code below for plotting the RMSE vs. the iteration number or epoch below (three cells, part I/II/III). \n",
    "\n",
    "Write a short description of the code, and comment on the important points in the generation of the (R)MSE array.\n",
    "\n",
    "The training phase output lots of lines like \n",
    "\n",
    "> `epoch= 104, mse_train=1.50, mse_val=2.37` <br>\n",
    "> `epoch= 105, mse_train=1.49, mse_val=2.35`\n",
    "\n",
    "What is an ___epoch___ and what is `mse_train` and `mse_val`?\n",
    "\n",
    "NOTE$_1$: the generalization plot figure 5.3 in [DL] (above) and the plots below have different x-axis, and are not to be compared directly!\n",
    "\n",
    "NOTE$_2$: notice that a 90 degree polynomial is used for the polynomial regression. This is just to produce a model with an extremly high capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code: Qb(part I)\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def GenerateData():\n",
    "    m = 100\n",
    "    X = 6 * np.random.rand(m, 1) - 3\n",
    "    y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = GenerateData()\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split( \\\n",
    "        X[:50], y[:50].ravel(), \\\n",
    "        test_size=0.5, \\\n",
    "        random_state=10)\n",
    "\n",
    "print(\"X_train.shape=\",X_train.shape)\n",
    "print(\"X_val  .shape=\",X_val.shape)\n",
    "print(\"y_train.shape=\",y_train.shape)\n",
    "print(\"y_val  .shape=\",y_val.shape)\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled   = poly_scaler.transform(X_val)\n",
    "\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "plt.plot(X, y, \"b.\", label=\"All X-y Data\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18, )\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()\n",
    "\n",
    "print('OK')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code: Qb(part II)\n",
    "\n",
    "def Train(X_train, y_train, X_val, y_val, n_epochs, verbose=False):\n",
    "    print(\"Training...n_epochs=\",n_epochs)\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    sgd_reg = SGDRegressor(max_iter=1,\n",
    "                           penalty=None,\n",
    "                           eta0=0.0005,\n",
    "                           warm_start=True,\n",
    "                           early_stopping=False,\n",
    "                           learning_rate=\"constant\",\n",
    "                           tol=-float(\"inf\"),\n",
    "                           random_state=42)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        sgd_reg.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_predict = sgd_reg.predict(X_train)\n",
    "        y_val_predict   = sgd_reg.predict(X_val)\n",
    "\n",
    "        mse_train=mean_squared_error(y_train, y_train_predict)\n",
    "        mse_val  =mean_squared_error(y_val  , y_val_predict)\n",
    "\n",
    "        train_errors.append(mse_train)\n",
    "        val_errors  .append(mse_val)\n",
    "        if verbose:\n",
    "            print(f\"  epoch={epoch:4d}, mse_train={mse_train:4.2f}, mse_val={mse_val:4.2f}\")\n",
    "\n",
    "    return train_errors, val_errors\n",
    "\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = Train(X_train_poly_scaled, y_train, X_val_poly_scaled, y_val, n_epochs, True)\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code: Qb(part III)\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(np.sqrt(train_errors), \"b--\", linewidth=2, label=\"Training set\")\n",
    "plt.plot(np.sqrt(val_errors), \"g-\", linewidth=3, label=\"Validation set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code review..\n",
    "assert False, \"TODO: code review in text form\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc)  Early Stopping\n",
    "\n",
    "How would you implement ___early stopping___, in the code above? \n",
    "\n",
    "Write an explanation of the early stopping concept...that is, just write some pseudo code that 'implements' the early stopping. \n",
    "\n",
    "OPTIONAL: also implement your early stopping pseudo code in Python, and get it to work with the code above (and not just flipping the hyperparameter to `early_stopping=True` on the `SGDRegressor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: early stopping..\n",
    "assert False, \"TODO: explain early stopping\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd) Explain the Polynomial RMSE-Capacity plot\n",
    "\n",
    "Now we revisit the concepts from `capacity_under_overfitting.ipynb` notebook and the polynomial fitting with a given capacity (polynomial degree).\n",
    "\n",
    "Peek into the cell below (code similar to what we saw in `capacity_under_overfitting.ipynb`), and explain the generated RMSE-Capacity plot. Why does the _training error_ keep dropping, while the _CV-error_ drops until around capacity 3, and then begin to rise again?\n",
    "\n",
    "What does the x-axis _Capacity_ and y-axis _RMSE_ represent?\n",
    "\n",
    "Try increasing the model capacity. What happens when you do plots for `degrees` larger than around 10? Relate this with what you found via Qa+b in `capacity_under_overfitting.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and review this code\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "def GenerateData():\n",
    "    n_samples = 30\n",
    "    #degrees = [1, 4, 15]\n",
    "    degrees = range(1,8)\n",
    "\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "    return X, y, degrees\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y, degrees  = GenerateData()\n",
    "\n",
    "print(\"Iterating...degrees=\",degrees)\n",
    "capacities, rmses_training, rmses_validation= [], [], []\n",
    "for i in range(len(degrees)):\n",
    "    d=degrees[i]\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression)\n",
    "        ])\n",
    "    \n",
    "    Z = X[:, np.newaxis]\n",
    "    pipeline.fit(Z, y)\n",
    "    \n",
    "    p = pipeline.predict(Z)\n",
    "    train_rms = mean_squared_error(y,p)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, Z, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    score_mean = -scores.mean()\n",
    "    \n",
    "    rmse_training=sqrt(train_rms)\n",
    "    rmse_validation=sqrt(score_mean)\n",
    "    \n",
    "    print(f\"  degree={d:4d}, rmse_training={rmse_training:4.2f}, rmse_cv={rmse_validation:4.2f}\")\n",
    "    \n",
    "    capacities      .append(d)\n",
    "    rmses_training  .append(rmse_training)\n",
    "    rmses_validation.append(rmse_validation)\n",
    "    \n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(capacities, rmses_training,  \"b--\", linewidth=2, label=\"training RMSE\")\n",
    "plt.plot(capacities, rmses_validation,\"g-\",  linewidth=2, label=\"validation RMSE\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Capacity\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: investigate..\n",
    "assert False, \"TODO: ...answer in text form\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-\n",
    "2018-12-19| CEF, initial.                  \n",
    "2018-02-14| CEF, major update and put in sync with under/overfitting exe.         \n",
    "2018-02-20| CEF, fixed revision table malformatting.\n",
    "2018-02-25| CEF, minor text updates, and made Qc optional.\n",
    "2018-02-25| CEF, updated code, made more functions.\n",
    "2018-03-11| CEF, corrected RSME to RMSE.\n",
    "2019-10-08| CEF, updated to ITMAL E19.\n",
    "2020-03-14| CEF, updated to ITMAL F20.\n",
    "2020-10-15| CEF, updated to ITMAL E20.\n",
    "2020-11-17| CEF, added comment on 90 degree polynomial, made early stopping a pseudo code exe.\n",
    "2021-03-22| CEF, changed crossref from \"capacity_under_overfitting.ipynb Qc\" to Qa+b in QdExplain the Polynomial RMSE-Capacity Plot. \n",
    "2021-03-23| CEF, changed 'cv RMSE' legend to 'validation RMSE'.\n",
    "2021-10-31| CEF, updated to ITMAL E21.\n",
    "2022-03-25| CEF, updated to SWMAL F22.\n",
    "2023-03-16| CEF, minor update to SWMAL F23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Model capacity and under/overfitting\n",
    "\n",
    "NOTE: text and code to the exercise taken from\n",
    "\n",
    "* https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    " \n",
    "This example demonstrates the problems of underfitting and overfitting and\n",
    "how we can use linear regression with polynomial features to approximate\n",
    "nonlinear functions. \n",
    "\n",
    "The plot below shows the function that we want to approximate,\n",
    "which is a part of the cosine function. In addition, the samples from the\n",
    "real function and the approximations of different models are displayed. The\n",
    "models have polynomial features of different degrees. \n",
    "\n",
    "We can see that a linear function (polynomial with degree 1) is not sufficient to fit the\n",
    "training samples.  This is called **underfitting**. \n",
    "\n",
    "A polynomial of degree four approximates the true function almost perfectly. However, for higher degrees the model will **overfit** the training data, i.e. it learns the noise of the\n",
    "training data.\n",
    "\n",
    "We evaluate quantitatively **overfitting**/**underfitting** by using\n",
    "cross-validation. We calculate the mean squared error (MSE) on the validation\n",
    "set, the higher, the less likely the model generalizes correctly from the\n",
    "training data.\n",
    "\n",
    "## Qa) Explain the polynomial fitting via code review\n",
    "\n",
    "Review the code below, write a __short__ code review summary, and explain how the polynomial fitting is implemented?\n",
    "\n",
    "NOTE: Do not dig into the plotting details (its unimportant compared to the rest of the code), but just explain the outcome of the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code review \n",
    "#assert False, \"TODO: remove me, and review this code\"\n",
    "\n",
    "# NOTE: code from https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "def GenerateData(n_samples = 30):\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "    return X, y\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = GenerateData()\n",
    "degrees = [1, 4, 15]\n",
    "    \n",
    "print(\"Iterating...degrees=\",degrees)\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression)\n",
    "        ])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    \n",
    "    score_mean = scores.mean()\n",
    "    print(f\"  degree={degrees[i]:4d}, score_mean={score_mean:4.2f},  {polynomial_features}\")   \n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    y_pred = pipeline.predict(X_test[:, np.newaxis])\n",
    "    \n",
    "    # Plotting details\n",
    "    plt.plot(X_test, y_pred          , label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nScore(-MSE) = {:.2e}(+/- {:.2e})\".format(degrees[i], scores.mean(), scores.std()))\n",
    "    \n",
    "    # CEF: loop added, prints each score per CV-fold. \n",
    "    #      NOTICE the sub-means when degree=15!\n",
    "    print(f\"    CV sub-scores:  mean = {scores.mean():.2},  std = {scores.std():.2}\")\n",
    "    for i in range(len(scores)):\n",
    "        print(f\"      CV fold {i}  =>  score = {scores[i]:.2}\")\n",
    "        \n",
    "plt.show()\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code review..\n",
    "assert False, \"TODO: review in text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb) Explain the capacity and under/overfitting concept\n",
    "\n",
    "Write a textual description of the capacity and under/overfitting concept using the plots in the code above.\n",
    "\n",
    "What happens when the polynomial degree is low/medium/high with respect to under/overfitting  concepts? Explain in details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot explainations..\n",
    "assert False, \"TODO: answer...in text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc) Score method\n",
    "\n",
    "Why is the scoring method called `neg_mean_squared_error` in the code? \n",
    "\n",
    "Explain why we see a well known $J$-function, the $MSE$, is conceptually moving from being a cost-function to now be a score function, how can that be?\n",
    "\n",
    "What happens if you try to set it to `mean_squared_error`, i.e. does it work or does it raise an exception, ala\n",
    "\n",
    "```python\n",
    "scores = cross_val_score(pipeline, X[:, np.newaxis], y, scoring=\"mean_squared_error\", cv=10)\n",
    "```\n",
    "\n",
    "Remember to document the outcome for Your journal.\n",
    "\n",
    "What is the theoretical minimum and maximum score values (remember that the score range was $[-\\infty;1]$ for the $r^2$ score). Why does the Degree 15 model have a `Score(-MSE) = -1.8E8`? And, why is this by no means the best model?\n",
    "\n",
    "More on Score funs at\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: examine the score method..\n",
    "assert False, \"TODO: explain and test the neg_mean_squared_error in the code above\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-\n",
    "2018-12-18| CEF, initial.\n",
    "2018-02-14| CEF, major update.\n",
    "2018-02-20| CEF, added code reference.\n",
    "2018-02-20| CEF, fixed revision table malformatting.\n",
    "2018-02-25| CEF, minor text updates, and made Qc optional.\n",
    "2019-10-08| CEF, updated to ITMAL E19.\n",
    "2020-03-14| CEF, updated to ITMAL F20.\n",
    "2020-10-15| CEF, updated to ITMAL E20.\n",
    "2021-10-29| CEF, changed sign of score(-MSE) for score=neg_mean_squared_error.\n",
    "2021-10-04| CEF, update to ITMAL E21.\n",
    "2022-03-25| CEF, updated to SWMAL F22.\n",
    "2023-03-15| CEF, minor update to SWMAL F23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Hyperparameters and Gridsearch \n",
    "\n",
    "When instantiating a Scikit-learn model in python most or all constructor parameters have _default_ values. These values are not part of the internal model and are hence called ___hyperparameters___---in contrast to _normal_ model parameters, for example the neuron weights, $\\mathbf w$, for an `MLP` model.\n",
    "\n",
    "## Manual Tuning Hyperparameters\n",
    "\n",
    "Below is an example of the python constructor for the support-vector classifier `sklearn.svm.SVC`, with say the `kernel` hyperparameter having the default value `'rbf'`. If you should choose, what would you set it to other than `'rbf'`? \n",
    "\n",
    "```python\n",
    "class sklearn.svm.SVC(\n",
    "    C=1.0, \n",
    "    kernel=’rbf’, \n",
    "    degree=3,\n",
    "    gamma=’auto_deprecated’, \n",
    "    coef0=0.0, \n",
    "    shrinking=True, \n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200, \n",
    "    class_weight=None, \n",
    "    verbose=False, \n",
    "    max_iter=-1, \n",
    "    decision_function_shape=’ovr’, \n",
    "    random_state=None\n",
    "  )\n",
    "```  \n",
    "\n",
    "The default values might be a sensible general starting point, but for your data, you might want to optimize the hyperparameters to yield a better result. \n",
    "\n",
    "To be able to set `kernel` to a sensible value you need to go into the documentation for the `SVC` and understand what the kernel parameter represents, and what values it can be set to, and you need to understand the consequences of setting `kernel` to something different than the default...and the story repeats for every other hyperparameter!\n",
    "\n",
    "## Brute Force  Search\n",
    "\n",
    "An alternative to this structured, but time-consuming approach, is just to __brute-force__ a search of interesting hyperparameters, and  choose the 'best' parameters according to a fit-predict and some score, say 'f1'. \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/gridsearch.png\"  alt=\"WARNING: could not get image from server.\"  style=\"width:350px\">\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of grid search for two distinct hyperparameters. </center> \n",
    "    <center> Notice that you would normally search hyperparameters like `alpha` with an exponential range, say [0.01, 0.1, 1, 10] or similar.</center>\n",
    "</em></small>\n",
    "\n",
    "Now, you just pick out some hyperparameters, that you figure are important, set them to a suitable range, say\n",
    "\n",
    "```python\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 10]\n",
    "```\n",
    "and fire up a full (grid) search on this hyperparameter set, that will try out all your specified combination of `kernel` and `C` for the model, and then prints the hyperparameter set with the highest score...\n",
    "\n",
    "The demo code below sets up some of our well known 'hello-world' data and then run a _grid search_ on a particular model, here a _support-vector classifier_ (SVC)\n",
    "\n",
    "Other models and datasets  ('mnist', 'iris', 'moon') can also be examined.\n",
    "\n",
    "## Qa Explain GridSearchCV\n",
    "\n",
    "There are two code cells below: 1) function setup, 2) the actual grid-search.\n",
    "\n",
    "Review the code cells and write a __short__ summary. Mainly focus on __cell 2__, but dig into cell 1 if you find it interesting (notice the use of local-function, a nifty feature in python).\n",
    "  \n",
    "In detail, examine the lines:  \n",
    "  \n",
    "```python\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, ..\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "..\n",
    "FullReport(grid_tuned , X_test, y_test, time_gridsearch)\n",
    "```\n",
    "and write a short description of how the `GridSeachCV` works: explain how the search parameter set is created and the overall search mechanism is functioning (without going into too much detail).\n",
    "\n",
    "What role does the parameter `scoring='f1_micro'` play in the `GridSearchCV`, and what does `n_jobs=-1` mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa, code review..cell 1) function setup\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            ret_str=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(ret_str)>0:\n",
    "                    ret_str += ','\n",
    "                ret_str += f'{key}={temp_str}{value}{temp_str}'  \n",
    "            return ret_str          \n",
    "        try:\n",
    "            param_str = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + param_str + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=0)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def TryKerasImport(verbose=True):\n",
    "    \n",
    "    kerasok = True\n",
    "    try:\n",
    "        import keras as keras_try\n",
    "    except:\n",
    "        kerasok = False\n",
    "\n",
    "    tensorflowkerasok = True\n",
    "    try:\n",
    "        import tensorflow.keras as tensorflowkeras_try\n",
    "    except:\n",
    "        tensorflowkerasok = False\n",
    "        \n",
    "    ok = kerasok or tensorflowkerasok\n",
    "    \n",
    "    if not ok and verbose:\n",
    "        if not kerasok:\n",
    "            print(\"WARNING: importing 'keras' failed\", file=sys.stderr)\n",
    "        if not tensorflowkerasok:\n",
    "            print(\"WARNING: importing 'tensorflow.keras' failed\", file=sys.stderr)\n",
    "\n",
    "    return ok\n",
    "    \n",
    "print(f\"OK(function setup\" + (\"\" if TryKerasImport() else \", hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!\") + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qa, code review..cell 2) the actual grid-search\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = svm.SVC(\n",
    "    gamma=0.001\n",
    ")  # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks,\n",
    "# FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel': ('linear', 'rbf'), \n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qb Hyperparameter Grid Search using an SDG classifier\n",
    "\n",
    "Now, replace the `svm.SVC` model with an `SGDClassifier` and a suitable set of the hyperparameters for that model.\n",
    "\n",
    "You need at least four or five different hyperparameters from the `SGDClassifier` in the search-space before it begins to take considerable compute time doing the full grid search.\n",
    "\n",
    "So, repeat the search with the `SGDClassifier`, and be sure to add enough hyperparameters to the grid-search, such that the search takes a considerable time to run, that is a couple of minutes or up to some hours.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: grid search\n",
    "assert False, \"TODO: make a grid search on the SDG classifier..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qc Hyperparameter Random  Search using an SDG classifier\n",
    "\n",
    "Now, add code to run a `RandomizedSearchCV` instead.\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/randomsearch.png\" alt=\"WARNING: could not get image from server.\"  style=\"width:350px\" >\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of randomized search for two distinct hyperparameters. </center> \n",
    "</em></small>\n",
    "\n",
    "Use these default parameters for the random search, similar to the default parameters for the grid search\n",
    "\n",
    "```python\n",
    "random_tuned = RandomizedSearchCV(\n",
    "    model, \n",
    "    tuning_parameters, \n",
    "    n_iter=20, \n",
    "    random_state=42, \n",
    "    cv=CV, \n",
    "    scoring='f1_micro', \n",
    "    verbose=VERBOSE, \n",
    "    n_jobs=-1\n",
    ")\n",
    "```\n",
    "\n",
    "but with the two new parameters, `n_iter` and `random_state` added. Since the search-type is now random, the `random_state` gives sense, but essential to random search is the new `n_tier` parameter.\n",
    "\n",
    "So: investigate the `n_iter` parameter...in code and write a conceptual explanation  in text.\n",
    "\n",
    "Comparison of time (seconds) to complete `GridSearch` versus `RandomizedSearchCV`, does not necessarily give any sense, if your grid search completes in a few seconds (as for the iris tiny-data). You need a search that runs for minutes, hours, or days.\n",
    "\n",
    "But you could compare the best-tuned parameter set and best scoring for the two methods. Is the random search best model close to the grid search?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "assert False, \"implement a random search for the SGD classifier..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qd MNIST Search Quest II\n",
    "\n",
    "Finally, a search-quest competition: __who can find the best model+hyperparameters for the MNIST dataset?__\n",
    "\n",
    "You change to the MNIST data by calling `LoadAndSetupData('mnist')`, and this is a completely other ball-game that the iris _tiny-data_: it's much larger (but still far from _big-data_)!\n",
    "\n",
    "* You might opt for the exhaustive grid search, or use the faster but-less optimal random search...your choice. \n",
    "\n",
    "* You are free to pick any classifier in Scikit-learn, even algorithms we have not discussed yet---__except Neural Networks and KNeighborsClassifier!__. \n",
    "\n",
    "* Keep the score function at `f1_micro`, otherwise, we will be comparing 'æbler og pærer'. \n",
    "\n",
    "* And, you may also want to scale your input data for some models to perform better.\n",
    "\n",
    "* __REMEMBER__, DO NOT USE any Neural Network models. This also means not to use any `Keras` or `Tensorflow` models...since they outperform most other models, and there are also too many examples on the internet to cut-and-paste from!\n",
    "\n",
    "Check your result by printing the first _return_ value from `FullReport()` \n",
    "```python \n",
    "b1, m1 = FullReport(random_tuned , X_test, y_test, time_randomsearch)\n",
    "print(b1)\n",
    "```\n",
    "that will display a result like\n",
    "```\n",
    "best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "```\n",
    "and paste your currently best model into the message box, for ITMAL group 09 like\n",
    "```\n",
    "Grp09: best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "\n",
    "Grp09: CTOR for best model: SGDClassifier(alpha=1.0, average=False, class_weight=None, early_stopping=False,\n",
    "              epsilon=0.1, eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n",
    "              learning_rate='invscaling', loss='hinge', max_iter=1000,\n",
    "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "              random_state=None, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "```\n",
    "              \n",
    "on Brightspace: \"L09: Regularisering, optimering og søgning\" | \"Qd MNIST Search Quest\"\n",
    "\n",
    "> https://itundervisning.ase.au.dk/itmal_quest/index.php\n",
    "\n",
    "and, check if your score (for MNIST) is better than the currently best score. Republish if you get a better score than your own previously best. Deadline for submission of scores is the same as the deadline for the O3 journal handin.\n",
    "\n",
    "Remember to provide an ITMAL group name manually, so we can identify a winner: the 1. st price is  cake! \n",
    "\n",
    "For the journal hand-in, report your progress in scoring choosing different models, hyperparameters to search and how you might need to preprocess your data...and note, that the journal will not be accepted unless it contains information about Your results published on the Brightspace 'Search Quest II' page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:(in code and text..)\n",
    "assert False, \"participate in the Search Quest---remember to publish your result(s) on Brightspace.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-\n",
    "2018-03-01| CEF, initial.\n",
    "2018-03-05| CEF, updated.\n",
    "2018-03-06| CEF, updated and spell checked.\n",
    "2018-03-06| CEF, major overhaul of functions.\n",
    "2018-03-06| CEF, fixed problem with MNIST load and Keras.\n",
    "2018-03-07| CEF, modified report functions and changed Qc+d.\n",
    "2018-03-11| CEF, updated Qd.\n",
    "2018-03-12| CEF, added grid and random search figs and added bullets to Qd.\n",
    "2018-03-13| CEF, fixed SVC and gamma issue, and changed dataload to be in fetchmode (non-keras).\n",
    "2019-10-15| CEF, updated for ITMAL E19\n",
    "2019-10-19| CEF, minor text update.\n",
    "2019-10-23| CEF, changed demo model i Qd) from MLPClassifier to SVC.\n",
    "2020-03-14| CEF, updated to ITMAL F20.\n",
    "2020-10-20| CEF, updated to ITMAL E20.\n",
    "2020-10-27| CEF, type fixes and minor update.\n",
    "2020-10-28| CEF, added extra journal hand-in specs for Search Quest II, Qd.\n",
    "2020-10-30| CEF, added non-use of KNeighborsClassifier to Search Quest II, Qd.\n",
    "2020-11-19| CEF, changed load_mode=2 (Keras) to load_mode=0 (auto) for MNIST loader.\n",
    "2021-03-17| CEF, updated to ITMAL F21.\n",
    "2021-10-31| CEF, updated to ITMAL E21.\n",
    "2021-11-05| CEF, removed iid=True paramter from GridSearchCV(), not present in current version of Scikit-learn (0.24.1).\n",
    "2022-03-31| CEF, updated to SWMAL F22.\n",
    "2022-08-30| CEF, updating to v1 changes.\n",
    "2022-11-04| CEF, updated link to Brightspace, Search Quest II.\n",
    "2022-11-04| CEF, fixed error \"TypeError: classification_report() takes 2 position..\".\n",
    "2022-11-11| CEF, elaborated on Search Quest II deadline.\n",
    "2023-03-24| CEF, updated link and updated to SWMAL F23."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "# Regulizers\n",
    "\n",
    "## Resume of The Linear Regressor\n",
    "\n",
    "For our  data set $\\mathbf{X}$ and target $\\mathbf{y}$ \n",
    "\n",
    "$$\n",
    "    \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands, remember: no  newlines in defs}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\def\\st#1{_{\\scriptsize\\textrm{#1}}}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{obs}}^{\\left(#2\\right)}}\n",
    "    \\def\\diff#1{\\mathrm{d}#1}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\powtest{\\pown{\\scriptsize\\textrm{test}}}\n",
    "    \\def\\powtrain{\\pown{\\scriptsize\\textrm{train}}}\n",
    "    \\def\\pred{\\st{pred}}\n",
    "    \\def\\bM{\\mathbf{M}}\n",
    "    \\def\\bX{\\mathbf{X}}\n",
    "    \\def\\bZ{\\mathbf{Z}}\n",
    "    \\def\\bm{\\mathbf{m}}\n",
    "    \\def\\bx{\\mathbf{x}}\n",
    "    \\def\\by{\\mathbf{y}}\n",
    "    \\def\\bz{\\mathbf{z}}\n",
    "    \\def\\bw{\\mathbf{w}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "\\bX =\n",
    "    \\ac{cccc}{\n",
    "        x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "        x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2}\\\\\n",
    "        \\vdots      &             &        & \\vdots \\\\\n",
    "        x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn\\\\\n",
    "    }\n",
    ", ~~~~~~~~\n",
    "\\by =\n",
    "    \\ac{c}{\n",
    "         y\\pown{1} \\\\\n",
    "         y\\pown{2} \\\\\n",
    "         \\vdots \\\\\n",
    "         y\\pown{n} \\\\\n",
    "    }\n",
    "%, ~~~~~~~~\n",
    "%\\bx\\powni = \n",
    "%    \\ac{c}{\n",
    "%        1\\\\\n",
    "%        x_1\\powni \\\\\n",
    "%        x_2\\powni \\\\ \n",
    "%        \\vdots \\\\\n",
    "%        x_d\\powni\n",
    "%     }  \n",
    "$$\n",
    "\n",
    "a __linear regressor__ model, with the $d$-dimensional (expressed here without the bias term, $w_0$) weight column vector,\n",
    "\n",
    "$$\n",
    "\\bw =\n",
    "    \\ac{c}{\n",
    "         w_1 \\\\\n",
    "         w_2 \\\\\n",
    "         \\vdots \\\\\n",
    "         w_d \\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "\n",
    "was previously found to be of the form\n",
    "\n",
    "$$\n",
    "    y\\powni\\pred =  \\bw^\\top \\bx\\powni\n",
    "$$\n",
    "\n",
    "for a single data instance, or for the full data set in a compact matrix notation \n",
    "\n",
    "$$\n",
    "    \\by\\pred = \\bX \\bw\n",
    "$$\n",
    "\n",
    "(and rememering to add the bias term $w_0$ on $\\bw$ and correspondingly adding fixed '1'-column in the $\\bX$ matrix, later.) \n",
    "\n",
    "An accociated cost function could be the MSE \n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\textrm{MSE}(\\bX,\\by;\\bw) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                            &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni\\pred \\right)^2\\\\\n",
    "                            &\\propto ||\\bX \\bw - \\by\\pred||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "here using the squared Euclidean norm, $\\norm{2}^2$, via the $||\\cdot||_2^2$ expressions. We used the MSE to express the total cost function, $J$, as\n",
    "\n",
    "$$\n",
    "   \\textrm{MSE} \\propto J = ||\\bX \\bw - \\by\\pred||_2^2\n",
    "$$\n",
    "\n",
    "give or take a few constants, like $1/2$ or $1/n$.\n",
    "\n",
    "## Adding Regularization to the Linear Regressor\n",
    "\n",
    "Now the weights, $\\bw$ (previously also known as $\\btheta$), in this model are free to take on any value they like, and this can  lead to both numerical problems and overfitting, if the algorithm decides to drive the weights to insane, humongous values, say $10^{200}$ or similar.\n",
    "\n",
    "Also for some models, neural networks in particular, having weights outside the range -1 to 1 (or 0 to 1) may cause complete saturation of some of the internal non-linear components (the activation function). \n",
    "\n",
    "Now, enters the ___regularization___ of the model: keep the weights at a sane level while doing the numerical gradient descent (GD) in the search space. This can quite simply be done by adding a ___penalty___ part, $\\Omega$, to the $J$ function as\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        \\tilde{J} &= J + \\alpha \\Omega(\\bw)\\\\\n",
    "                  &= \\frac{1}{n} ||\\bX \\bw - \\by||_2^2 + \\alpha ||\\bw||^2_2\n",
    "     }\n",
    "$$\n",
    "\n",
    "So, the algorithm now has to find an optimal value (minimum of $J$) for both the usual MSE part and for the added penalty scaled with the $\\alpha$ constant.\n",
    "\n",
    "## Regularization and Optimization for Neural Networks (NNs)\n",
    "\n",
    "The regularization method mentioned here is strictly for a linear regression model, but such a model constitutes a major part of the neurons (or perceptrons), used in neural networks. \n",
    "\n",
    "## Qa The Penalty Factor\n",
    "\n",
    "Now, lets examine  what $||\\bw||^2_2$ effectively mean? It is composed of our well-known $\\norm{2}^2$ norm and can also be expressed as simple as\n",
    "\n",
    "$$\n",
    "  ||\\bw||^2_2 = \\bw^\\top\\bw\n",
    "$$\n",
    "\n",
    "Construct a penaltiy function that implements $\\bw^\\top\\bw$, re-using any functions from `numpy` (implementation could be a tiny _one-liner_).\n",
    "\n",
    "Take $w_0$ into account, this weight factor should NOT be included in the norm. Also checkup on `numpy`s `dot` implementation, if you have not done so already: it is a typical pythonic _combo_ function, doing both dot op's (inner product) and matrix multiplication (outer product) dependent on the shape of the input parameters.\n",
    "\n",
    "Then run it on the three test vectors below, and explain when the penalty factor is low and when it is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qa..first define some numeric helper functions for the test-vectors..\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def isFloat(x):\n",
    "    # is there a python single/double float??\n",
    "    return isinstance(x, float) or isinstance(x, np.float32) or isinstance(x, np.float64)\n",
    "    # NOT defined on Windows?:   or isinstance(x, np.float128)      \n",
    "\n",
    "# Checks that a 'float' is 'sane' (original from libitmal)\n",
    "def CheckFloat(x, checkrange=False, xmin=1E-200, xmax=1E200, verbose=0):\n",
    "    if verbose>1:\n",
    "        print(f\"CheckFloat({x}, type={type(x)}\")\n",
    "    if isinstance(x, collections.Iterable):\n",
    "        for i in x:\n",
    "            CheckFloat(i, checkrange=checkrange, xmin=xmin, xmax=xmax, verbose=verbose)\n",
    "    else:\n",
    "        #if (isinstance(x,int)):\n",
    "        #    print(\"you gave me an integer, that was ignored\")\n",
    "        #    return\n",
    "        assert isFloat(x), f\"x={x} is not a float/float64/numpy.float32/64/128, but a {type(x)}\"\n",
    "        assert np.isnan(x)==False , \"x is NAN\"\n",
    "        assert np.isinf(x)==False , \"x is inf\"\n",
    "        assert np.isinf(-x)==False, \"x is -inf\"\n",
    "        # NOTE: missing test for denormalized float\n",
    "        if checkrange:\n",
    "            z=fabs(x)\n",
    "            assert z>=xmin, f\"abs(x)={z} is smaller that expected min value={xmin}\"\n",
    "            assert z<=xmax, f\"abs(x)={z} is larger that expected max value={xmax}\"\n",
    "        if verbose>0:\n",
    "             print(f\"CheckFloat({x}, type={x} => OK\")\n",
    "\n",
    "# Checks that two 'floats' are 'close' (original from libitmal)\n",
    "def CheckInRange(x, expected, eps=1E-9, autoconverttofloat=True, verbose=0):\n",
    "    assert eps>=0, \"eps is less than zero\"\n",
    "    if autoconverttofloat and (not isFloat(x) or not isFloat(expected) or not isFloat(eps)):\n",
    "        if verbose>1:\n",
    "            print(f\"notice: autoconverting x={x} to float..\")\n",
    "        return CheckInRange(1.0*x, 1.0*expected, 1.0*eps, False, verbose)\n",
    "    CheckFloat(x)\n",
    "    CheckFloat(expected)\n",
    "    CheckFloat(eps)\n",
    "    x0 = expected - eps\n",
    "    x1 = expected + eps\n",
    "    ok = x>=x0 and x<=x1\n",
    "    absdiff = np.fabs(x-expected)\n",
    "    if verbose > 0:\n",
    "        print(f\"CheckInRange(x={x}, expected={expected}, eps={eps}: x in [{x0}; {x1}] => {ok}\")\n",
    "    assert ok, f\"x={x} is not within the range [{x0}; {x1}] for eps={eps}, got eps={absdiff}\"\n",
    "\n",
    "print(\"OK(setup..)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code\n",
    "    \n",
    "def Omega(w):\n",
    "    assert False, \"TODO: implement Omega() here and remove this assert..\"\n",
    " \n",
    "# weight vector format: [w_0 w_1 .. w_d], ie. elem. 0 is the 'bias'    \n",
    "w_a = np.array([1., 2., -3.])  \n",
    "w_b = np.array([1E10, -3E10])\n",
    "w_c = np.array([0.1, 0.2, -0.3, 0])\n",
    "\n",
    "p_a = Omega(w_a)\n",
    "p_b = Omega(w_b)\n",
    "p_c = Omega(w_c)\n",
    "\n",
    "print(f\"P(w0)={p_a}\")\n",
    "print(f\"P(w1)={p_b}\")\n",
    "print(f\"P(w2)={p_c}\")\n",
    "\n",
    "# TEST VECTORS\n",
    "e0 = 2*2+(-3)*(-3)\n",
    "e1 = 9e+20\n",
    "e2 = 0.13\n",
    "\n",
    "CheckInRange(p_a, e0)\n",
    "CheckInRange(p_b, e1)\n",
    "CheckInRange(p_c, e2)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Regularization for Linear Regression Models\n",
    "\n",
    "Adding the penalty $\\alpha ||\\bw||^2_2$ actually corresponds to the Scikit-learn model `sklearn.linear_model.Ridge` and there are, as usual, a bewildering array of regulized models to choose from in Scikit-learn with exotic names like `Lasso` and `Lars`\n",
    "\n",
    "> https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n",
    "\n",
    "Let us just examine `Ridge`, `Lasso` and `ElasticNet` here.\n",
    "\n",
    "### Qb Explain the Ridge Plot\n",
    "\n",
    "First take a peek into the plots (and code) below, that fits the `Ridge`, `Lasso` and `ElasticNet` to a polynomial model. The plots show three fits with different $\\alpha$ values (0, 10$^{-5}$, and 1).\n",
    "\n",
    "First, explain what the different $\\alpha$ does to the actual fitting for the `Ridge` model in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qb, just run the code..\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, ElasticNet, Lasso\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def FitAndPlotModel(name, model_class, X, X_new, y, **model_kargs):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    alphas=(0, 10**-5, 1) \n",
    "    random_state=42\n",
    "    \n",
    "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
    "        #print(model_kargs)\n",
    "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
    "        model_pipe = Pipeline([\n",
    "                (\"poly_features\", PolynomialFeatures(degree=12, include_bias=False)),\n",
    "                (\"std_scaler\", StandardScaler()),\n",
    "                (\"regul_reg\", model),\n",
    "            ])\n",
    "            \n",
    "        model_pipe.fit(X, y)\n",
    "        y_new_regul = model_pipe.predict(X_new)\n",
    "        \n",
    "        lw = 2 if alpha > 0 else 1\n",
    "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
    "    \n",
    "    plt.plot(X, y, \"b.\", linewidth=3)\n",
    "    plt.legend(loc=\"upper left\", fontsize=15)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    plt.title(name)\n",
    "    plt.axis([0, 3, 0, 4])\n",
    "\n",
    "def GenerateData():\n",
    "    np.random.seed(42)\n",
    "    m = 20\n",
    "    X = 3 * np.random.rand(m, 1)\n",
    "    y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
    "    X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
    "    return X, X_new, y\n",
    "    \n",
    "X, X_new, y = GenerateData()\n",
    "\n",
    "FitAndPlotModel('ridge',      Ridge,        X, X_new, y)\n",
    "FitAndPlotModel('lasso',      Lasso,        X, X_new, y)\n",
    "FitAndPlotModel('elasticnet', ElasticNet,   X, X_new, y, l1_ratio=0.1)\n",
    "\n",
    "print(\"OK(plot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Explain the Ridge, Lasso and ElasticNet Regularization Methods\n",
    "\n",
    "Then explain the different regularization methods used for the `Ridge`, `Lasso` and `ElasticNet` models, by looking at the math formulas for the methods in the Scikit-learn documentation and/or using [HOML]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:(in text..)\n",
    "assert False, \"Explain the math of Ridge, Lasso and ElasticNet..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd Regularization and Overfitting\n",
    "\n",
    "Finally, comment on how regularization may be used to reduce a potential tendency to overfit the data\n",
    "\n",
    "Describe the situation with the ___tug-of-war___ between the MSE ($J$) and regulizer ($\\Omega$) terms in $\\tilde{J}$ \n",
    "\n",
    "$$\n",
    "  \\tilde{J} = J + \\alpha \\Omega(\\bw)\\\\\n",
    "$$\n",
    "and the potential problem of $\\bw^*$ being far, far away from the origin, and say for a fixed $\\alpha=1$ in regulizer term (normally for real data $\\alpha \\ll 1$).\n",
    "\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/weights_regularization_l2.png\" alt=\"WARNING: could not get image from server.\" style=\"width:240px\">\n",
    "\n",
    "[OPTIONAL]: Would data preprocessing in the form of scaling, standardization or normalization be of any help to that particular situation? If so, describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: (in text..)\n",
    "Assert False, \"Explain the tug-of-war..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qe Regularization Methods for Neural Networks\n",
    "\n",
    "What kinds of regularizations are used for neural network models?\n",
    "\n",
    "Explain the NN-regularizations by giving a shorter textual (or perhaps also a mathematical) explanation of the regularization method.\n",
    "\n",
    "Setup a NN model with one or more of the NN-regularization method you found, showing that the generalization error drops if a particular regularization method is applied.\n",
    "\n",
    "Use some data of your own choice for the demonstration, and also design and setup our own particular NN instantiation (you choose layout and all the other NN-hyperparameters) using either the `sklearn` or `Keras` framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: (in text and code..)\n",
    "Assert False, \"Regularization Methods for Neural Networks..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-\n",
    "2018-03-01| CEF, initial.\n",
    "2018-03-06| CEF, updated.\n",
    "2018-03-07| CEF, split Qb into Qb+c+d and added NN comment.\n",
    "2018-03-11| CEF, updated Qa and $w_0$ issues.\n",
    "2018-03-11| CEF, updated Qd with plot and Q.\n",
    "2018-03-11| CEF, clarified $w_0$ issue and update $\\tilde{J}$'s.\n",
    "2019-10-15| CEF, updated for ITMAL E19.\n",
    "2019-10-19| CEF, updated text, added float-check functions.\n",
    "2020-03-23| CEF, updated to ITMAL F20.\n",
    "2020-10-20| CEF, updated to ITMAL E20.\n",
    "2020-10-27| CEF, minor updates.\n",
    "2020-10-28| CEF, made preprocessing optional part of Qq (tug-of-war).\n",
    "2020-03-17| CEF, updated to ITMAL F21.\n",
    "2021-10-31| CEF, updated to ITMAL E21.\n",
    "2022-03-31| CEF, updated to SWMAL F22.\n",
    "2023-03-24| CEF, updated to SWMAL F23, added Qe.\n",
    "2023-09-19| CEF, changed LaTeX mbox and newcommand (VSCode error) to textrm/mathrm and renewcommand.\n",
    "2023-10-02| CEF, changed LaTeX commands to defs to get both KaTeX and MathJax to work."
   ]
  },
  {
   "attachments": {
    "roboflow-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABuQAAAJRCAMAAACKpCETAAAYbXpUWHRSYXcgcHJvZmlsZSB0eXBlIGV4aWYAAHjarZppkmQpdoX/swotgXlYDnDBTDvQ8vUdPCIrh2p1tZkyKsK9fHjAHc4Az53/+e/r/ot/pYfucmm9jlo9//LII06edP/5N97f4PP7+/6d7OvXq7+87krw8T2LvJR4TJ832vw8hsnr5a8vfI8R1q+vu/71TuxfF/p64/uCSSNrKPt5krweP6+H/HWhcT5P6ujt56muzzz9/vrgm8rXb92f5YWvi+v/3c8v5EaUrDBQivGkkPz72z8zSPpNafKY+at3mXYaPE+pOh5i+p4JAflled+P3v8coF+C/P3M/R79Ur/e/i34cX59Iv0Wy6/PO5787Ruh/H3wX4h/Gjj9mFH89Y2eQ/ljOV+/91q/93xWN3MlovWrorz7jo6+wwcXIU/va5Wfxm/heXs/g5/up9+k3Pz2i58dRojE/bqQg4UZbjjvcYfNFHM8sfEY447pvdZTiyNushTIGT/hxkbGLHWSteNxpC6n+GMu4Y073ng7dEa2wEdj4GKBr/zLH/d/vfmf/Lh7t0IUFMxSX6yYV1RdMw1lTn/5FAkJ9ytv5QX4++cr/f6nwlKpZj6mMHcWOP36XGKV8FdtpZfnxOcKj58WCq7Z1wUIEWMXJhMSGfA1pBJq8C3GFgJx7CRoMnMaIS4yEEqJxiRjpkOia7FHjc13WnifjSXWqJfBJhJRUk2N3NBTJCvnQv203KmhWVLJpZRaWumujDJrqrmWWmurArnZUsuttNpa62202VPPvfTaW+999DniSGBgGXW00ccYc0Y3GWhyrcnnJ6+suNLKq6y62uprrLkpn5132XW33ffY06IlAyasWrNuw+YJ7oAUJ59y6mmnn3HmpdZuuvmWW2+7/Y47f2TtK6t//PwHWQtfWYsvU/pc+5E1XnWtfV8iCE6KckbGIr1LxpQBCjoqZ76HnKMyp5z5IZQrkUkW5cZZUMZIYT4hlht+5O6vzP2jvLnS/1He4r/LnFPq/j8y50jdn3n7m6yZeG6/jH26UDH1ie677czY3eynDJ6E1LZFYG+m24fBcrDuYhWxz1lAmTBru+ZztHbPZbTbaZAfr7tf3ih3pXFrTYf4GWse7dZdx7hXrUgV3LvuXPek/Ns77ue3PlcJe+61ChcKq9caSz2ZaOdBgAKIuD0TuKflQwQXcQQCq7nTWJtfliE+InkK/3Pmmm3Xs/ttVs88k9rLW4REjrgk4/S4d4s7tOz7tlNdI2osrLcIvo9rmWtlJtbWJL7NVtZQba9w1kj3hGz3kKbE8x5DpfTbLe26XfvfvsH6719LTWeb9MmLyN032YnljNxszNtoJXOfYO17Tm7xX1507kzJe2M5p+ZAdqlBFg+v5cJaBi1S6EwV/vXF0Cp+taFSPH6E0M86K1tVRNNt9fZ+Cyn1VBgTOEsxH68G3O/FATf+XB6/vZMIHsvcivQdRHrWjHC5aTn6RCDeyMlggUkdW7JNRt6r1EAHCS3qqfRzOR60GjcVRlk15X3v9L0wDHhkl0K80jSVRBeWfwfwoEEX66v95AGe0LC29oiZ9YZYJ6DfShj1mtF8B/BvMS3qhJZtPTLtSuO2VyknjA6snXzJxmpEnYHodyqm0ZK0JVMcdqgkhBYR64AKXcmwmlm644WSjIFzUNKt2UdFBKS6Z9TWS6S0DwXZxg6f2nN5B4CgLSvdqIg05x1oACbnmQJFej1TvXUUFA1BzG8wjcEVZqnpghTDN3ftIlOoe1pmnp4FQRTiLuckaW5C4A8/LXEZWoZu6eBNlT5prJK6oh46MbpoljCBJlRnAMFDOhb9hKyYY0w3hWE5Mrk+DqjYKTIqD2R4VU3DprtyMscqNkmq882F96cH4nynUekvmoH22oW/tlTpSajOKpgR+TfxFDRTXvpBSn2JhEX68zbEMkulLXJmjeDyYUhQ9AJPZP3r+8Dl5wr6OmO7st6SQlAhX0ULMCIiFBF6LGQqhwjEYiyT2aZd9b/A6C/zgSB1wTyjAT704ACExumw/R9LGZmLLYSgX+KRmwGAcd+wa5zkbA+oI4pfOkAGdHGBmcAi3EkqOw+ad3JxOANQgRjGOtbquSUsgzopGpgygEcH5oMoMBKT6j20lr5CjYAOZAk+7GKcEMC1vam3S+sxZGZEaPkE0KCYs94KLCPhh+Y6yKTMXDvlfCMMuc+mBaFhOnXZPgBMXSB+Ol/Vf84A4idZIxs2EkkBSq0tCrdGe1Uc+ewCGinhvlTCtA+rV8TqDIcEUeg53rnpH8fCFvBAy1zKDliwlibL3eqFdOL3uMugphXgB8BshD+GcD/GEIWtB1dUVhUxd3LUUqW7NSwgCu6H3lpA6u3U4DQ4vIFcENR0pSktR3J/oClBrTsPUyT7XM6TPQTpBPzMMqqFihwp94VMyVIcXX2GkOmuz8PlT7ISQIX9OO8iC3dcLRrVspnFRoIQxLapvUMbB9VJUxcvLcvumu7Wcs8uCvKASPjcenT8HfCkJhiAGN1GA/E1aid/Agww98Vbrdh1FZDKu9vOuW1yl+bNi/G82BjU6cX4qu1UoW0PCq8tQumsocELYTKgJuJ+nglw6xdIi3xAs1d/drI+oEaCR+XNufGtp9g4rI7VQ/5YIsJJVbr7vVzq9mqxLU7YLF803H1LebHfvWXWDy4RTL26Tv8r+6TefYJFuTIpyn8BVRBfBJoAgB1hxleIZEYfGFYWNBoN/l2aCy/VkSJLQwCmbWDDwu5dVUVBiRpZorX79zAUBeUP0rzqVtleJFktrK/QoRENuYHMTlohGBA1ItrMID4uVACkPhDOe9BHHWKwV8PTwITaxIpDygB9xdodwpNmBspG6LGCW0hxgHJKNo7y2xxoV4+wA8uRQYtU74i6v6BMdV2IsQtksKrIx5hiRT3rAuO29glsBHoIX6R8JB5es/0I+BvP3ddsigQDUpuqWsCrL9iBCCQYajS0wKKtK6hklaXuYYiaFvjlHXA34GnLhQ0HMLQ3oF/DoObsA+4TQeONts0I4/4d4iampd1opzAK+Fjgmu1KHBB2WpUW9sdkze+i02q9Uj8jCbFTRrWfV7qMgY5u5JFaqKYaest1GkRFfmgIPlhV49gPcomFYI0bsQIhKWKwx03NPuEfDHPHRP57vIVNhwZbBfdzgqqhnMT3c/oe7qeMQcdI0zgAp6ANoju6oY9CneBGoNdIkH/OiMycDZn3Ht4Mjh0cFBeKEBKBCbISJ+GBfltmKykHF9A8oeaye9jwGZWfpr3oDMEpJTMbfO95j0pHFCHscC1l25LS+BFJ90LZa0ayh4UpgXaNYisfWqaAlzo6vZpuFmRdyKOXtOh+Lq+dk1OnOZwadDVJEjDb0B5oIAzaXrgqwpYnmIPRQ45G3BZqD+eoKFKFYcCmnSl04ZEBgpMyga/RY5NcMa01qvZYrjJAtYuqCoJmNsiwkBoMxw6IQGTOLPF0BftIDhmurROsBFD7nrQGZCSCrl9qEkVBnVHXYDzVSz+V2fOOCalKqxMZGsAdEgzCeqCEcJWRCUOB+6HVKGUPSwdoHqpHu5Qu4o9hCngqYhYHRO/UkoZjEFhXhUHuRwJQVGLFj4oswZsehOzRNhZKUDo8tl4jK211RXlmFBt9APdnGhkyxIQmnlGee6IhQcCC8FYnKHeUN24dHUFeM3kFC4GAs+CyFFHfHrsO5a24IciFkM0oYATWc+DATCjklmqiKNKZu74+z1wcX8/ECqyHVcVqU5sOlI4wMKVPTC8YWCY2m2q3A2JMkID4TJqZYk00/XxI5GV8VGDbvkDIgUxHPTpZOPJkQ3Dg68Ye4RRgYaS/IdH2zbQFCqTAKe9SXWCOaaxFMof0UzLgKjzRSws4OP5xOY+NHZQCSolSQofKRjSjMDG7QDmEGSJGfE/+Q5dER7bQfmBUm2PKDuKWLrCFzsZWkTr04YIucFYYthVoCQiBviS5DbZFP0Z0dnZdOg/LsczO8ghEVMSAomlECi71AiBAgkesuiciEqSPnlYui6Q1Vl3B+mPOy2XKH3VV3zoBZk/kdoI4iB+TCbEx4HPEnzRcqtJSaWmLmrrFdMnaGQRJt9aGKi40Gkw1pLEQprsGO+iWCF8h16SWhoIBHEJgKQ9JaKE/juXe7pDLMpuNOnmek2ySrEbzowzCWgY3HWkmbboMIHJRsm9j+G1ut0v/UiHeQU4Im2dVNv7+IE3fslC2qQGdJRLxloE3qH5S86DIvCFVug5Nkw4Gx2psmBqMN+5qEigyjeIYWxvzJAiti7bvkBrMi3knvbGKkLFO+IdcBL4D10GctosS6USNjq7aNBrIo/rqH102MAiQM0sBAeDiQQ0BUlprBUFCRpnjdALpdQHgTsK+gkFKn/OOXkHQgoT0eDawEFrQtleRADQYj45kTfTbw4eiFhgYv4oEQ4IECBTlCPLR4dBlGAdSY7SL56B4UIofRU18m7aI+mtgoLVteRHYsaq3IlWG8YqrZrzyGSOilcjOjvN2vDGvZQrERLxZTb1y8yjORkdEBLnTHt4bMKhtc8Lej7yRoQi2wELVtFCYelbIh5E5ILOH4ZrYf0XLFZhawEhGNQ2sJaTTWW/HhuOsN1KoHMUCPY9+n0UdIBAFueGENI42kruURmGZDs1QIGJma/S7WsemwSgTsOsEZalleZIgm+DLPuSDgieFcVjcie7HpdXuuOCjm5Np4qbipZUWsmeYODgCx9oZCBQ6Dr7Q/TMgk9DqtD+axjpoiitx4IwtGUWfWS9SeoQF5XtUJTOnXRBcLPRW3DrqxlbudIUVek+KvCFQCuL2OEKDFOxoyIi2voai90Z+xmMLKMDygeuJ9G1eg2ENGQFe4GoI4EeL1LMr8DTQkpJIFnGAAIKTYG4gQDBFbW3tC9PVUC6mYIEbOKdIHkeVPqFequ/YrKZhDiQAF2BjRajI9FXIJARPo4m//GXNphiDy0HkePHwLJrAAcl7O4HNpoSQOWCo5+IYKzAf34GSnVjijTQEXbAd8DxW7BCvyisDJw8sgbYl9ONYISNP7YGAM5W6CR5A5qJcidTq1Ajs50rQM5Cfwsy9VXQbILNE5XiLmZvDT9Ofiq1KEyDjQ8wSUIBJGIlGDgDhAQN8xTFR8lJQwXA8INQlZdo0WvQaEi/lCoYspAUv+Y9RuFmbXCLdDv7zJQAMeQbFRHDtyKNp70Ibe4TfsFlJOhbyAytg2gMP3wgMxV5ChtohWDk2XR1hcD8CdRZUCkmuIAbUgpQg2Om5OWgEarzICeqtUEdYANrHs+qdQFroJBvBFBlcAr0r4qd1VocepXERWlJXFwmC5jwgKfgPOVGGvkAMgWSrPSKqVHvdgBqw4Jf2P1hQo2gKNqUjRvGPSAhIiCQTK0PK8WXGDSOr2KY223z5nCeOA+PRK+g4RENWKFAFYMR0gQQRDlJVQIc6IyynjbzLJ5l8l/HQNtDasJwNgPrMjK4jC1R9bdrbCcFQbHA8JUNRhIqqpAvCcykRxEPue8BP588m10GRjpvzANPQRQoxSk6SNozgAItKW5BsvDPtCfsjHCCQCntPoZxED/AOloibwXh0Ha3SULNoH7oPL5GiozXoCwCfx0Zt96Otv42ODZQg18ZYcCEuLz+/Mz4XZO1yyQSzI7QiPtw355O8+SFD2iRkDtKEiM0+0+BbcW+oCsS8EMtE2m35AtCcKgKsUr06loFZXfaCl8410bhXmTyJ7mY5Ag5EDMoTrBh3gU2oLjwqhehPVr+1h7kVWukOgV9p8o4h6IfOspQwO9pBbMJ63CZaYhhSsR0ryF16Bc5MxWuvlrz7qXGLSwsaq4IxmhbJlOnyNsOUdKcBtrYp8xwQ0124q5KHUB4WAUl85rMIQMJxXUYrQD3aUFpVJQU8kux+eBfMIrQHwyBfhg6HgMrQltFt0p07CRQhDXrRobbPfPc/XJVqkWWC3/mTEYJTnNF8HVrSoWqpH5ZKZaCqS6HitdeHHIraq9WjDi6oahwcHiy2vfFZuDK0YIPhDWY4AWBdakRtnJGzKaBHx1wcjm6uQMBAu1A9xRbpqdGjUOTAHxANw1wJYEM/D7kfgEQWKsvcXPwbVEPeQ3c0TShB1WI6h4bjgCIvEY979mipoiRkmBAMGHmsqX2PjV869PQc1D11+TY0ywX7C2IUGu0bwsd9KT9teqxipybpK6I9IKcz0ccAHkSESBLH6yhJ50FuwreaFCIchKds6NqjKU9VLMiC6WmKlE4a1epUP2Qalk5skORd5yU+VTwtBoge1dYzhiY3gZoXsix05pVeYiEIBZkrowOJkq0J3++u7UEv8MaAuq+jAKy9tteLeg1oo2lY89a2DipOmwZHp+pJqE3lGzqqVrq178kK9q7ZVf+xTTPSRgk0zJ6EoRvf92RDpxBqai+/9j5VJOgoen+njXIfLEj7yq6ES5gzOHSgmAQ+vnJGkTS7ach/VTALAEB5qme69gd6YEh8IkKbagYvsRAgfAJQ6AncgCoYMEHVtSENUwxzeioSU7tBMQUaBcC6XhPtk+/isby0JZVNYuInMVnoC2JmNK02UgFN5WlOKp6ZKix0rm5kAGpEXASlQIig6HawKeRxVJWGSKuvQBESInEkt47BpC/gTkAc8bdgIp33XKERywkD66RtH9qHHgBpwOQp81WAMI/YB3mrTgEsvfO4DgvZpHl029TCWcJY2qFmfon+ug4eaAgz+HBI9pIIbDlwSeWYjkQy7BPJ5NQRt8/5QXxe2DhGY/Qc0eZXMcL6IN8Kl5nN9koIJIwDqgpDhhLhbbxoyzy5XX4PH3KEakm7DqkwBo4vFKcTTVQ5RCwrrX3IiQ0kR1BrLrghzFvEa8HxkawD8Ihj3fR1L2NnbW14YDq7vJKocUUGTzlvKANVpvOLpzvohSXRRZcE3VSlRgJm+dDbXwpFe+qIs+My8MH3r3ndB+KRSe2RL5YKQ9TrxlyhIG6FxMfAT6SQmXdb2iJsVBCj43q9i+/OLxoDO1dAkzZ0XoDcCKgPD9PUt0+rYxsouviNqUWi0msJtousDIRnmq7puBR1AlpQSUsbH7YlkJtJug+d09ScdLob2xDJCCO5+NMgKHPQFMOt28/wgipO3WxwTX2Z25hVUrshk4cUaSfMG2I4mhh172nk3N6dBKweHN6DyoZ4Yoh0rBcIVXg3CSuiTpuxTQg/gzPAaJD2bZNQa8AE0EGjCMcxmcdE2bxfg8oUhl0NKRXQLSpJ3cqQ7lAvU0BSjsiOcOk8AsqlwZO3vXppR2AENBf6yfnQa0X2fyNzYP25EftdW9MgoYTswGWywqHtACIzhdOACYDKhcB7L1lOrZCyoOzDVdpUpyCmdsyH7kAY9DIEvgOaJMowTRBfcBsHRD+La9r4mEedydc6sh0CQFeR56kKRghKsFiTW4F+WKj2RSjPUHVSEaduWwibrJmgB/zfpAKZ1W3kiVxEGqeWdMopeoC4kaibD7+oQLcUXIRKO0BfPemHv8D5odtFvEwJ+BV0qD10qoFgwvcxjomHTbc6XtTWsJnBpomTRmiDRidGN3AeSEMdDUvnFcR6BgiQTtTi2tDRkbkRsgNEj+nRQzK7+O6gAimCV3MgK/EXIRc4FsxG7/uAqKAKdfMGZaBugHs8KT9bO7P4d90Esj5HCNa05ehwmUDIXoJGxPRATR7d/wKPQfbIbCKMNAUPdSJPZwHuh1Eo1FduB8YHcaLb4Z0Ty2VTDpgLvHdFlwWhKenVqRXpxg/jhWnYqvNfFAYcojvNBIkolQyw6e4RCJ6pvtsKkbLT6whf2/AEpnZwkB5GEwEPQemrVHkHaQY24+gGHcTIcrACoEYVbhqUeoM8UfOsrgvK4Unaw6AnCMN39FlEalMCuisKwbi0ycs1Jzobr4BTy1Cf4ZtA36vD2Y0zwWx6EIy6BlZxtpNexBTQO0hiGGAnjyjXAScBddoj4B2U89URJFqk04o6rJb6Bi8XFhL9tqb6bReou/AOpY15AsekbOnD7WDRiFCTiaP7UTxxnaIjT0aAjAbyBMjTHTwM9NLNtGgCCdegeyOWDjZSchDxPjrltQ8gmO7t2GgErR9o0v0wGAhVdcIQdR3QozkkqNTCWzt0jyA7hplpA4dU30aNZQDRQ/mVpXTdFnVMCkvSCBpHgsKlNGqFbngi2r6AenXqaB0DkgRthAT+ouUvXN39g3LSzPuRmkZv69QEG0IToiCQTVXqBS0azD3cRf7qbKBVPOKdr1t1h4+JYJCe5R88un/zARAHIQeaofvhj1aOsmgdpEhgu4nRM0mYrlEiumts6k7PFcSpeDzYh1bvuI0gnCodhRhb075Ibg+R2n6H0tPeeHm5fzj1piNJ979Q7VuVXWj+dgAAAYRpQ0NQSUNDIHByb2ZpbGUAAHicfZE9SMNAHMVf00qlVETsIOKQoTpZEC3iqFUoQoVQK7TqYHLpFzRpSFJcHAXXgoMfi1UHF2ddHVwFQfADxNXFSdFFSvxfWmgR48FxP97de9y9A4RGhWlWYALQdNtMJxNiNrcqBl8RQAhBDCAuM8uYk6QUPMfXPXx8vYvxLO9zf44+NW8xwCcSzzLDtIk3iKc3bYPzPnGElWSV+Jx43KQLEj9yXWnxG+eiywLPjJiZ9DxxhFgsdrHSxaxkasRx4qiq6ZQvZFusct7irFVqrH1P/sJwXl9Z5jrNESSxiCVIEKGghjIqsBGjVSfFQpr2Ex7+YdcvkUshVxmMHAuoQoPs+sH/4He3VmFqspUUTgA9L47zMQoEd4Fm3XG+jx2neQL4n4ErveOvNoCZT9LrHS16BPRvAxfXHU3ZAy53gKEnQzZlV/LTFAoF4P2MvikHDN4CobVWb+19nD4AGeoqdQMcHAJjRcpe93h3b3dv/55p9/cDUkBymudeneAAABAwaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8P3hwYWNrZXQgYmVnaW49Iu+7vyIgaWQ9Ilc1TTBNcENlaGlIenJlU3pOVGN6a2M5ZCI/Pgo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA0LjQuMC1FeGl2MiI+CiA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICB4bWxuczppcHRjRXh0PSJodHRwOi8vaXB0Yy5vcmcvc3RkL0lwdGM0eG1wRXh0LzIwMDgtMDItMjkvIgogICAgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iCiAgICB4bWxuczpzdEV2dD0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL3NUeXBlL1Jlc291cmNlRXZlbnQjIgogICAgeG1sbnM6cGx1cz0iaHR0cDovL25zLnVzZXBsdXMub3JnL2xkZi94bXAvMS4wLyIKICAgIHhtbG5zOkdJTVA9Imh0dHA6Ly93d3cuZ2ltcC5vcmcveG1wLyIKICAgIHhtbG5zOmRjPSJodHRwOi8vcHVybC5vcmcvZGMvZWxlbWVudHMvMS4xLyIKICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIgogICAgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIgogICB4bXBNTTpEb2N1bWVudElEPSJnaW1wOmRvY2lkOmdpbXA6NmNiNjNjYjQtNDljMy00MTI5LThjMjQtMjY3ZTc3MWM2NjUwIgogICB4bXBNTTpJbnN0YW5jZUlEPSJ4bXAuaWlkOmZkMzJiYWU4LTIyMGUtNDJkOS05ZTZkLTYyYjgyZmM0YzdkNCIKICAgeG1wTU06T3JpZ2luYWxEb2N1bWVudElEPSJ4bXAuZGlkOmY4MWY4YTRkLTQ4NWMtNDBiNS05MWE1LTFmZGU0OTA1ZTdhOCIKICAgR0lNUDpBUEk9IjIuMCIKICAgR0lNUDpQbGF0Zm9ybT0iTGludXgiCiAgIEdJTVA6VGltZVN0YW1wPSIxNjgxMTEyMTg0MjU2MzI5IgogICBHSU1QOlZlcnNpb249IjIuMTAuMTgiCiAgIGRjOkZvcm1hdD0iaW1hZ2UvcG5nIgogICBleGlmOlBpeGVsWERpbWVuc2lvbj0iMTc2NCIKICAgZXhpZjpQaXhlbFlEaW1lbnNpb249IjgwOCIKICAgeG1wOkNyZWF0b3JUb29sPSJHSU1QIDIuMTAiPgogICA8aXB0Y0V4dDpMb2NhdGlvbkNyZWF0ZWQ+CiAgICA8cmRmOkJhZy8+CiAgIDwvaXB0Y0V4dDpMb2NhdGlvbkNyZWF0ZWQ+CiAgIDxpcHRjRXh0OkxvY2F0aW9uU2hvd24+CiAgICA8cmRmOkJhZy8+CiAgIDwvaXB0Y0V4dDpMb2NhdGlvblNob3duPgogICA8aXB0Y0V4dDpBcnR3b3JrT3JPYmplY3Q+CiAgICA8cmRmOkJhZy8+CiAgIDwvaXB0Y0V4dDpBcnR3b3JrT3JPYmplY3Q+CiAgIDxpcHRjRXh0OlJlZ2lzdHJ5SWQ+CiAgICA8cmRmOkJhZy8+CiAgIDwvaXB0Y0V4dDpSZWdpc3RyeUlkPgogICA8eG1wTU06SGlzdG9yeT4KICAgIDxyZGY6U2VxPgogICAgIDxyZGY6bGkKICAgICAgc3RFdnQ6YWN0aW9uPSJzYXZlZCIKICAgICAgc3RFdnQ6Y2hhbmdlZD0iLyIKICAgICAgc3RFdnQ6aW5zdGFuY2VJRD0ieG1wLmlpZDplNGRlY2YyZS1jMmRhLTQ2NjItYmZkMi02ZjQ3N2U4MTQyMzIiCiAgICAgIHN0RXZ0OnNvZnR3YXJlQWdlbnQ9IkdpbXAgMi4xMCAoTGludXgpIgogICAgICBzdEV2dDp3aGVuPSIrMDI6MDAiLz4KICAgIDwvcmRmOlNlcT4KICAgPC94bXBNTTpIaXN0b3J5PgogICA8cGx1czpJbWFnZVN1cHBsaWVyPgogICAgPHJkZjpTZXEvPgogICA8L3BsdXM6SW1hZ2VTdXBwbGllcj4KICAgPHBsdXM6SW1hZ2VDcmVhdG9yPgogICAgPHJkZjpTZXEvPgogICA8L3BsdXM6SW1hZ2VDcmVhdG9yPgogICA8cGx1czpDb3B5cmlnaHRPd25lcj4KICAgIDxyZGY6U2VxLz4KICAgPC9wbHVzOkNvcHlyaWdodE93bmVyPgogICA8cGx1czpMaWNlbnNvcj4KICAgIDxyZGY6U2VxLz4KICAgPC9wbHVzOkxpY2Vuc29yPgogICA8ZXhpZjpVc2VyQ29tbWVudD4KICAgIDxyZGY6QWx0PgogICAgIDxyZGY6bGkgeG1sOmxhbmc9IngtZGVmYXVsdCI+U2NyZWVuc2hvdDwvcmRmOmxpPgogICAgPC9yZGY6QWx0PgogICA8L2V4aWY6VXNlckNvbW1lbnQ+CiAgPC9yZGY6RGVzY3JpcHRpb24+CiA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgCjw/eHBhY2tldCBlbmQ9InciPz6VYU3oAAAAYFBMVEUGCQkeISVbA7RnCdGDFvo6PT5uJr6PPOiDQtuaQfthZGN8VN+mWvpwfeqAg4Nrl++4f/Wni+uhpKKsqvDJovObtvRrx/rNue3JzMrDzvCY2/2/4PDd5vrn6eby9PP9//xbjMxvAAAAAWJLR0QAiAUdSAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAAAd0SU1FB+cECgckGLpjqEsAACAASURBVHja7F0Jm6O4rg09Qy1ULi+h6YHqAvP//+XDKzJLYnYTzvnu7amqEECWrGPJsn2pAAAAAOBFcUETAAAAACA5AAAAAADJAQAAAABIDgAAAABAcgAAAAAAkgMAAAAAkBwAAAAAkgMAAAAAkBwAAAAAgOSAY4Al5esKl+dDn5RxFJfQ/gltom0ICXO1GQAkB4xDFMQO/iaIVujY5KZxEG7lOi1R4mCqM+l8c7CNWDD0kDIMgoBtqGoPPWc+aH/CJuxWfSxB7mLKfiJsvfqwzQAgOWAs0wSBCzM4cVAUslGPbvxX0vVPSbhKN7dFGSS5p6L0kNxQG4VhOXSPqNwwWplLcnk4mUTCwUFSMshM4hO7VR9LMHwr8+ozRFhMYidrGrQZACQHjA1IQgfX50hyYTCmZ5aPw8M4SHYkuaeijCA5X0Kr2SQ3PVIabpvk8U2XIjn10XbB3mapCQAkBzzhpbyHa3R8wSxXMxjbsEFmKPsuUz8PkBzrITnWc09WDd27/3LWJ0pDVWyY5Fj/67W+2W2jclC27kPsZ0wexTM23N4PKWI4bm1iTcoQrHooaPuGz0iOPSe5PgnKZ7diD0jOFoE9sNTub+xp01GJmbtun+QPGDwWSA4YOziPalfLrL8keSjoJ4+CIJR1EbWrYeI3Jn0zdRhJGARRLNw+ByEu/kkQleL7kfy+egb/OaEkJ+7FHy0+EX/gCPXVgXly/Ut9RxbzpyaKKqr6tzDOm5u3Ly/Fu8VdURRVke8oBjOikFawftHfFC9S6huHjYxC+Ny0V329aKmchCX6IUSauqXKyGSQzQBETt0lERW6E5kIKUTrtZ8mX5JQhB5B5JKZrec3HN7YgvhEoNTX62yypWX+7vVDRTvlhMw5xP2JCIaZymjg6SRd2ZHAauGeW6lniebQr05EaPTGH1b/0rwT+ahjZ+KGpb6vbXp62MPfuCVx2KNbSwXqu83TlH5Jc3UNCADJAc/BO5edF0yCKAgjQTn1f+v/M+nAoyCKlN+3HGzMKU7Mmyfix4Q68SiWN6i7dxjWDCBdsrhz/ZX2nFz9T+0wQjHnnnOeiWN5dcTJg4kn89cohRPht1P+IBb3k2FR3+WMX67e3hZFOhf6Hfk6RhTaCtYv8pulFIX/zG8sfo6kQEYUTXKxeGwTvDUPIdLUd6nf2jSMqkuRZBcHAzrQjFg/MpK3sZ8WiS+GhCJywybd5zc0Y2zB+PtYMLxqzqRXy9yli2Zqxk5xzL+Zt0Ro7E38Le4+vSG5rgRWC/fcij5Lv3ojQkm+HAv7MbchKu3amfglDCKjWWJ6lOR6JbZ1a6lAfreWMI7Dxmas5uoaEACSA56DjytLK5eUqC6d654bqaFwWelSTMvB6g/Kdroyl2QQ8dsx2VFz4fhK6SCSoEtyykGHZKRb30a+h3xyqLw2/63U/kDdLxy6XNGpjAEtUVTIQL5jZxKtVrB+Ed9kMgJIZB2g+Hupn5KbBJoiOfGFqOchVBoWtKo/FZMIpy+YpEcHelCRmNjMeprSoUUI6tEiirefb9GMNf7RD1PXC0FbWo4lIUnmTDrJO0sEY2/q7ZJhkuuRwGrhnltxyqvMa7TSlSxUg5umrcx9iUrbdsZfXj1EvpFlepTkiMTNW7R0S1Wg7VC8QKj1azVXnwEBIDngCbQTLumfQiubEinXmysP0HKwiiEZ7bbW7INKOgUqf5g3finqklyo3Bd5Ae06hC9QRTKalnPlaeLmTfsu13M3cVeUuPMd2wNZrdD6hYgSR8zIKLmm0mleEslZCcjmIZY0LGgljyM9FtGvVAbBULqyIk4zpg2kCTtvsaf4vPV8i2bCqktyOs2spsA6Ws6l/qwyEHUjSwTDTOXQ0+OmBbsS0BbuuZXWZqnGPhbJJUHSPMca3FgqbdtZ3pBrqd6rx/QskqNv0dItUYFlTc2UgNVcfQYEgOSAJ4g6o2Hzi/Ki0h8w7ZVEZ7QcbN3PyxYzUJSx7bySxhkkPelK9VZlwy+h/WRmuXblafLmTfsuV5ybR11RqDxWwaQZZlfdeydGCvoNZsbupjXLpCEkzZCUN+RDLGnaNZpCBOXuo6ppnj6Sk1KGlI+tpWa0jrZsgo/e51scbpNcru5N5lQtLau/W95YD1+oCJVtBVyiAZLrlYC2cO+tyC9tkovV8xNqj10jaNsZMwOOPtPrIznWJ11XBWb8lTQ1XO3m6jMgACQHPAYflXLQpXLasem+ZE3/W85eugBeIaKLPlqRXBKFqrSiIYCkoZ7cieQCg+bJxhEpf0/u13e52FlE/80WRdzJ+o5NclYrWL90WJSZYb+kAik8Ibm8Ij6SPMSSpk1yohUii4vijg60T47lI3P7aeYaq2xDNF0QDD3/AcnFprnCXi0Pk5wtgm1vA08Xf+mXgLRwz63yRqtlh+Qi81ncJjmq0radVV2SI6bXR3LWW7R126jAUB2f4DSjK7u5+gwIAMkBz7KVGnnbU7DGo0cPSa4eTEe6ctEiuVJUXSb5PJLj8xgKuQPJ9V4uC0biJO4nOfs7Fv9YrWA3yWOSkwWcST6b5ErzYOPykgGSi4WUeeRGcryx82ZechTJhaq54l4tO5BcMp/krBbuJzmt1S7JheazZHWSa96irdtGBVatrprRbDcXSA4kB4xH3ddjjrBn75GH6UrWysslemq+pOPUuEOSyfR0JX0p4yPVzEVPutKSQdVAJE/SlXbTTE1X5lIUWYPuQnKWNJ0l5fU18oKyla5s60DV81Qdkit7k338QTpb3fP8aJjkElpR0qPlYZIrXdOVkU1yfRJYLdxzK3sVZiddmfcnKO3f2nZWuaQrS0py1lu0dduogK66bFLcZStdCZIDyQEjYcoq6Yx4QufSq1bhiezokS75E6VnxNlaJGfccJvkHhSe9JCcfg/KObr39xSe9F3euM6kLUrc+Y5NclYrWL9YoU4SM5vkjMNzIDlLmg7J1U0WkTI8XTFj6cDy42Gb5PQ6hNLeD7GWwnbEsjVV8UYwTHL6elb2a3mY5GwRTDaBFIh0ns7UnzsSWC3cdyv9jLKH5DR7laxDclSlbTvrIbmYkrJaPGIXnjRv0dGtUYFeytLcTNEqaS6QHEgOGI2mrCzqpo9UbbjssswsDovk30rhUmLTs+UNWpM+iUxatklOuao8eExyiY4RQskEslzSCs1CvcorNy88cLks8FAkR0ShSwhKWvymRLFawfpFfNOIEnYiOT3385zkLGk6jrD+g1lQFTVaozqwhixJZ05OfzG0Sa5+6SDptmYkFsVHfSRXGset6l6TXi33kZxy15YIhpmiB0+nSwgsCawW7rtVrDknIq9e6leUlZJhOxy3Vdq2sx6Ss0xPGpN8hC1x0juA4SqIG5KLdIK+NDZDmgskB5IDJmQrS9PbojbJCZcRq5VmYhkrT2sKL883dRaLuWXZeGQ+SOhuFpzEkjjokpz4JI56F4MTkivVRiqJvA/1M9w7iXXPeoJF3E86gb7L+RrzOh7SLEJEic2SPf0d4jCTVitYvzTLyOPYqmhRKyv4HesnuZAclaa7A2Zk3kutFY6qlg7M3eqPo6Abyakvhq1Nscy6Yqs1eakEv76H5PgTIqauVy3Yo+U+ktPLrakIJsfI/6b3l2k/3SwGb0tgtXDfrcR3ksis6havrv+bK7ONOpEcVWnbznpIjpoenyWsbUwv82wkVm/R1a3ewUAPtoRIJGykzQWSA8kBY5GTPtfso9A4tqTZNkltSKQ3O8rF9kIy9VOKufKIWWNp7S34ZV2Sk3eOy8ckJ54iPhO7VeU0+SN2VlKbJfExc0w2eOq7XBQEJmpOjoqiPAf5TjssSOjmUeQXuiFYmFft6kpxik7olK60pOk6QlJaIXbTUruPUR0o5pevEnVJTn5Strx5M8igrSm3aSt7Sa5UkUsZNTrvarmP5CrNFFQEbRJ52Oz41n66umePBLSFe28lnqW1al5dB1+R2eitvdV2TozBtrMekqOmx+QGXiY53khsW0iPCuL2S+kSq7C9Ax1IDiQHLAiWD/1mbQRMj4uxNrotc+Z05yGYm/Xdp6RLiuwrei7P8yeP735HP8C62OmbSvhRRzKw4bFI3N/UjHVbdfCRPZ9Y27nR5z/YBdhsL0xeZFjLtoQ9IvSK5SxBbwtbt2fk1fSrGxEe6IdKVLJHuZDWxb37VrPcfV/lzkuV2MULJAcg6/rS4q11Hg/b8rxWmB4AkgMAeJqecDBeZxunPMcmiDA9ACQHwNPsC77Ae5V8lX0UAADTA0BygK9IXjgiyZN8nZxikuBcMpgeAJIDAAAAAJAcAAAAAIDkAAAAAJAcAAAAAIDkAAAAAAAkBwAAAAAgOQAAAAAAyQEAAAAASA4AlkfusJibJdjdFwBAcgBwPLDg8b7N6nzyEC0FACA5ADgewsd7WloH9AEAAJIDgFcC+A0AQHIAcDywx39hAyTHHH4GAAAkBwC70VsQsSgIwlgQkzjlJQ7yPAyCSE3Psbj+JcwNyYmJOfO1zjX19/mXEzQtAIDkAMADkov4/+RBcIrk4iCMQl2Ewimr/jyp6Jxc/bVQXBO3r6mimvpqnsOhOwAAkgOA/UkuCEpBTXFDcuLnnH5Qil8oyQlWrK9hrWtyUXzJQpRgAgBIDgA8IDkRczHBb4rkJEEJNivVWd8Nv2mSU/nNvO8aTMsBAEgOAPwgOflDzOlKpyvFXwR35TIHKX+hJBeqbyWta+obJmA4AADJAYAfJKfyiglnKkVyqmqEfxQHGmEfyYlvWdeI36IYu6IAAEgOAPwmuUDkLiOJ+AHJkWvqwI6XVwYorwQAkBwA7E9yj9OVCWWrAZJLOozGEn1bAABAcgCwJ8nplQKVITlSR6KLSlg5THLWNZW8XYQ1BAAAkgMAD0guZIbZ9BKCRARygqdCGabFMmTrJTnrmkj8zOT6AwAAQHIAsCvJhWHAF28LqtORXBDGem03C+Xibv75EMnRa3K+3Un9c4S2BQCQHADsT3JlxOshReClt/VKeKWkmmgr+cYmQcSqYZKj11Q5vx12cgYAkBwAeEFy9T/W/JkoQSlptrF0SD3Sa3KkKgEAJAcAvpCcjRg1IwAAkgMAkBwAACA5AADJAQAAkgOAPRB3tibJsScXAIDkAAAAAAAkBwAAAAAgOQAAAAAAyQEAAAAgOQAAAAAAyQEAAAAASA4AAAAAQHIAAAAAAJIDAAAAAJAcAAAAAJIDAAAAAJAcAAAAAIDkAAAAAAAkBwAAAAAgOQAAAAAAyQEAAAAASA4AAAAAyQEAAAAASA4AAAAAQHIAAAAAAJIDAAAAAJAcAAAAAIDkAAAAAJAcAAAAAIDkAAAAAAAkBwAAAAAgOQAAAAAAyQEAAAAASA4AAAAAyQEAAAAASA4AAAAAQHIAAAAAAJIDAAAAAJAcAAAAAIDkAAAAAJAcAAAAAIDkAAAAAAAkBwAAAAAgOQAAAAAAyQEAAACA9yTHFIqzgTm1zknagnnRFswPjOk2Lw80xri2qA4hBjsJyQle+85qpKdE5qDo4iRtkxUO9uLSFtkL4NvBMJjoON+vjp/vwsWT/JwELtRQHEOUwp21j0pyNb8pcrvfToq7A8mx9CSNkTr4smJSY9wPgd8ULqOf79/z8cdj/NVwGf38/D0HXEjuKI3RcN1Lkhzj8Vua3k6O1EG72f0shO8S1L4YvQ2ST+nQGC/Haja1GbhEtcVJOM6F8KujEX7Nc7sQ3WVVgiuy0/PburHLaxI+G034RyU5p8ztSUjOieO+EcgdluOUZNvz3GU1hstAcBourgzJyjmEf1CS+14vWXk8kkOy8jWTlb0R3aZEd1mL4sBw45KVp5mdXCWoPSjJucQuv09Cci75udMkK10o4NiE//OzHc+tQXIlGM5ywA7KLM9Sk5OtMzt5TJJziV2+T0Jy3wyBXMMApyD8n5IdlOQYGK7lf5GsHJesnGJAhyS5b4fu9P37JCSHZOXIZOVrSFocj+QYKim7ft2l2ZCsnFeBc0iSc+jg5e+TkFyByspRnv9VCN9xQaA/JAeKQ2Xlk2RlsUqy8pgk971isvJoJOeyDvw8lZXnmp1cn+aWI7kCFLdl7IJk5cFJzmESagbHHYzkkKw8S2XlQLXlIUgO5ZSTCy1OlKxkaxH+8UjOJagtfp+E5IqXXRUGjnNeVOA7ydUUdwehTYxdvlMQ/tx9X45Hci4cl52E5LAMfGSy8gUJf80lBUuQHCtAcUhWPid8tl5jHI7kVk5WHovkkKw8eSC38tzcZQGKQ6YSyUoHHnIptJhqSUcjubWTlYciOVRWnrOyskd45ifJgeL2KLR4zaB2OuEfjeTWTlYeieS+T7MqbJlk5SsHtesEc5fZmUpw2SDHOfj1EslK4tfvJyG51ZOVRyI5JCvHOfni5+wtsC3JFRkm42YmK3HAzhKzk8ciufWTlQciOexZidnJsQnbLUkOmcq5ycrzVJ1UqxL+sUhuvQN2jkdyOEQOlZVrB3MzSA41lY8BjtsqWXkwkvve4jTwo5AcYhckK9cO5i4I4/ZLVlan4bi1l1IcieS2SFYehuQKxC4NcBDDOqsJppIcDhtYIll5mgm5tZdSHIfk7pskK49Cci5BbYnTwBHU7kFySFU+87ouW8xjz8pF9n25HwHbJisNy/32muS+kazEMvD1U5aTSA4LB5aprDwLx32vnKw8EjNmTqeBPzxP/GWiPMcJuf8aNE6w/TuSlQjmliS5WanK6ylwd+G4E7TDR41r6rIvs7i0D19f9f92xQNr/t9Y3J0m5Ebd8v8Emp82xgyScwjkWPHfOeC2DPxwUnnBcpftOE66vc8t8P7+PuU7i+HqUmhxez8Hbg6NkX59vL9/bIX9+PJ/vx361+//rYv/8wR/XFL6f89Cci4HMRxWuH1Z7jKB4+6T+G0s5yxNS+3vrEhyV5dVYelJOO7LifA35LgdY0KnQO77LBznsu/LXwRyr9IY+7HcZTzHjSe4ScHYgUnOYYkcy67n4LgPh6oTdv94PwPJ/c9lk+rifhKSc6k6+TkJx/33ksnK+US3zPk7Y0luZIFAzXDvEzOOxyU5l2RldppkZeGYrDwDyf12mJ1cO1n5P2+SlQ6N8QfJSrJc8DVE3b7IciTJjTsLZc4E3HFJziVZWZwlWfnx7ZisPAPJ/S73T1Z6Q3IuSymQrKR7m72OuAtv57ksyY3huJk1JscluXvp0I4nSVa+uVTM3z7OQXJeJCt9IblvhmTl2ZKV03juZ1uSG5GrnF1FeViSuyFZ2cRxNwdXlkmOe3mS+1/mQ7LSE5JzqaxkZ+G4v+xsHDeK5n62JDlnjltipcBRSc5tVdhJkpUus5OlTFa+PMn9z2U/r+x/5yA5VFaeN1k5muZ+frYjOVeOW2Yx3FFJzqnQ4jSVlQ6u7P7xfgqSc0pW/j4Lybns+3KaZOVpCX8jlrssz3ELrec+Jsm57Od1nmSlw+qBhvBfm+R8SVZ6QXIuyUpUVr5eZeV0mtuG5Bw5brEtTY5Jck7Jytv1JBz3PFnJMp2sfHWS++2ydvJ+DpL78/orn0e4+eLEHOdMc8UWJFdkm1LcUUnOJVl5mmXgd5dy3Y9zkNz/fElWekFySFaOSla+eJnp6nufOJKc215eS25MCZI7Ccm9g+S22rQSJAeSOyPLXRw5Lt00jHvtdOX9JOnKLwfHnt0+zlF44rQQ/CzpShfG/0Zt5UvXVo6luRks50ZyLnHcwucLHLTw5IrCEzIp57AsPrueg+RcCk8YCk9w/sDpJuXWZzknknMoOln8DJ3DLiHIkLActYQgPc0SAk/KK71YQlAiYdkkLE+338k0mpvMcpeFOG75I+GOuuOJg2OvTrNzpcu+Fl9YDH66xeD/55CwxM6VZ0pYurBcsSLJ7cFxxyU5p/2ZT3MGgcu2Xl8n2dbrN/OC5Q6zrVd1llDu7KsIXFlu6pEEDiSXbZ6qPPYGzU7Lw5CwJIOos2zQ7HJ87O9zkBwqLEdWWLIz7Fa9UsLyOckV9z047tBH7RQOGeCTsJxLIc5ZjtpxORecrV9hiaN2DpmwPMXq+HVY7inJPT1dZxWKe/lDU8+TsPx2TVji0FQZ5OPQVGx7ctKE5Uosd3na5/bhuCOTHBKWVsLSZQPLk5CcF5s0+0JyThtYImF5roTlfzuQHPu+78Nxhya56x3H7YxPWL4+yTkmLM9Bcm7H7WBJ+NlWDq7Acpd5qwdW47hDkxwSlqNXDn59nIHkvv73e//dvf7PG5ZzCeWwJPxsjbH8gQSXWcnK9Tju2CTnsoclEpbWYOocJOfDRs3/dyyWO806Agdv/fMfWO4vW5jk9uO4g5Pc+7ij1F48Yel0/tDHKSI5l4RlVZyF5JCwJHDZOOEvWG5CKHeZkaxck+M8wmozUeVp9rB02+rsFDT35cnGJ56wHHJ0SFiuznKX6cnKK7jsEcu5+LKPg7OX4/t/3J0qLL2AFzs1l5scR7A31f35gwrLkRWWJ0lYPma5n3JBknscyF3fp9LJ9Xq73V8emcsmTvdzIHUZsXcbIx3CqGcvht/LwMUwvifd+Y/n+Lbx9+/3t0uO7ufvOeBCcsdqjLVYbjmSe7IMfCK51c4iOwdc+m92FrikYl5F1u/HyAqXxvh+Qfz0wMWxFz8ngUNjlAcTaSrXLVl78oDkHh+UOjZZKQkuOxPO5NifOv6XaoyZzt5lr4/ilZkNLNcPlw2I2fHEmkR0Cy6Wu0wM5MZx3Pvtlp6M4XgoB5YbF9d+n4TkXBifnYXkCvaSjn0aSheXcUjJRvPcw1CuXIjkHp4Gfh0XxN3Px3Av59g3iGvZWUjuTAnLJXJ0FRKWL8D4I2luuVDuMqnq5DpqIu6cDOfIciUSlseLa2d7fpfhT3ESkiuQsDxLXDuO5hY7P/WybiD3fmaKc3LsDAlLanUnIbkTJSyXmYkqEcqRdQQ/56C5pUK5y5QZuat7ovLMFOc2LXeehGX5MgnL7y1CuRdJWP4glEPCchrNLZWwvEwI5Nw57uQUV3vD8nVydLORv0zC8vsbCcvlSA4sd4IKS0pzW4dyl/GBnOtJ4NdbBiBhOa4xyu+TkJxTwrI4B8m5OfYCodyrFOL8XYDlitkk9z0/WXlLQXGosHzNCsslnP9ZKiydSK54fceOhOUEllsmlBsguXQux13BcS/m2DeaojxEwnIR73+SabnligpRYflCKwf/bhjKXQb2x59Jctc72E2jwrTcq8W1y7j/c1RYLjcThVDulRrj73ah3GWVQO6KMG6cYwfLHYrxF/L/pwjlHH3e6+71gXUEK7FcMYvkynRe1QlSlaNZDrUnR2K5hfz/KRKWS+boUGH5SludzWU551DuMvIcuavLRpXguNHTcucJ5V6h3HQpAjhDheUPWA4JyxlL5tYiuXRWshIch1DuxROWSxHAGSosnUmuAMmdqsJyAZabQXLFrEAOHAeWm5mw9D6UW4wBXDY3Lc5Bck6h3Gl29zrJVmcuLDc/lLuMylZeUXNy4pkoFOIsTXLZCSosFy2dRyhnDX9egOXmhHIzSK6Yk60Ex02eicIeloeJaxdjgBOcEr5w+AKWe61j9v5ukK+8jMpWguPOPBOFuHZxkjvBHpZLr4I+C8uVYLnnodxUkpuTrcQacCQsl2F8vxtjOQrIXp7lUDqPCsuZLDccyrGJJPcgW/l88QDYDAnLExTiLEkCr56wXLzeAgnLF2P8v2uHcpcRezNfkaxEheUyOEtce7aE5eyiQoZQrsFZ4trNSW5GthIc99Df4Wi5l0lYLkoMxQosV/Th6QVPvlWsDxy6MzZheYppueF8ZTmJ5IazldelJuSKkp0RLupAW5yvMdwmz0ecEQkA+4OxsiyKtVnuZxLJfU8mOadV4EXh3K0BAACAw/IcH6cW65KcC5lcFstWOiQrvwsQHAAAwJlCuhVZbhLJFdl6gVwBhgMAADgZzZWza09mLSLokFy6ViAHigMAADgjzc0N5WZNyrVJLptKcrenmUqoGgAA4Iwsx1ZiuUkkt04g9w2KAwAAQDC3bCjnwCwtkmNTSe6OVCUAAADQjyeHJvzdjuQmZisfB3IFNAwAAHBqFH6Q3GDdyfU6fUYOcRwAAABYbvr5qTMm5Vokl60QyCGOAwAAAIrlQzmHlXItkpuarUQcBwAAADxCuUu+chmSu6OuEgAAAHgINpnlFiM5NnFKLkWyEgAAAJjMchNDuecHEdgkV6ZLZyvBcQAAAIDE8iT3NJSzSa5Il85WIlkJAAAAzAzlplee2CSX3SeRXIpADgAAAHhKcsXeJDep7uTBAgIEcgAAAMDzUM5nkrshkAMAAAAcMHE9+PTySpvklp6Sg0IBAACA9UK5cSTH0mWzlQjkAAAAgBVJ7ukaAovkyoVJDjNyAAAAgBPL/V1nDYFFcpNWELwPkxzUCQAAAKxIck+3aJ5Pcp+3FNlKAAAA4FVJDlNyAAAAwEyWA8kBAAAA5wzlJi+UW5PkUHcCAAAAHJzkroPL5EpoEwAAALA4rgTJAQAAACA530nuGyQHAAAA+ENyGUgOAAAAAMmB5AAAAICDkRzSlQAAAABIDiQHAAAAnIPkbiA5AAAAwJHlsBgcUKaAJgAA4OUc2wuRHLb1AgAAAEByAKI5AABO4tR+jrdB89BRO98MPvpMNAdlowkBKHs6yW1x1A7z/GRwuIATdVkGsWE8UPYrKrtcluSKUelK30nuEFbBTusCoGwAyoZmto7kyg1I7vO+4ZQcDO1E7wdl4/1g468o9bIk9+xpFslVgyR3HU9y7IxdjJ3TsZxTbCgbyoayp6FckOSevqNNctltscqTUw4kWXVGsc8pNZQNZUPZi7LcNI7bhuR6JuVWXT3A2Pk6gsd9Yd33grIh9v5SMyh7A5qbWFw5luSmTcqlW+91wk7Y/z319+u/FJQNZUPZLyh2uQ/JTVso11kOzqoT9gX2Cj7Gy1eCsqHsF1d2dUplyDp0ugAAIABJREFUl7uQ3MTyyutW03H+GsU2Lsk7x8cqKBtiw8ah7AVY7jHHTS+udCW5MfnKl3I0vr0NO2H/h7JPJTaUfR6xywVI7vmb2iRXTSS52w4bVvrUFzZ8l5NKDbHPMqCBsk9F7RsUV7ZJblp5JamvLKoTGsWmb+KN2OeUGsqGsiH2Giw3MZCbQHL3SST3ft/j4AFfjGLj9/BE7HNKfVplMygbYq/KchNJ7mc0yRXT9jwx68GrExrF1m/BvBCbbS/2GZVdQdlQ9otTu+K4iSTnEFe1SG5a5cm7zldWJzSK7d/BB5Zje4gNZUPZr/0OpxS7XDlb2SG5KZNy7+8qlDujUezxBvs7PraP2FD2icSGss8idvmc45YkuSmVJ+81RCjHqvP1hX2ev3dfYHuJDWVD2S8tNjuljZfTSe5nAsl9j89XvguWu21bdOKJUez1+N39XgVlQ9mvLDY7q43v8tifn/+mkpzD3dskV4wmuXdJctesOp9R7PfwXbvCjlJD2VD2qyv7jGJPzla6hFZtkhs9KfeucDufOZ700WjxBW+apR30pESg7BONK06p7GLFKbkRJNfPcprj3rPzGQWGmmfyAKs8ue/Yj3taQNknFvucUv+sNyXXIbmx+UrNcekJuwLbOY3EIPXBH9zf2e4ZlL3vkOa0Nr6j1H8nBXIuU3JzSc4DjqtYyXZCuXNXOKHU1W7KXsMDDGwwlDJvlL179cdG0vgk9nbv4pPUxWpTcl2SG5WvNMnKPXtC9Z3thGJXB7CX1BnzQtnfVO3fAvTn5vcOvidhDWUP9LUektMvsTnKPZUtlgmPel19+c9M7OrQfvbCmJ5dfBfPL2dF4XzPnymBHJtEclUxgeTS6pTu3vkF6163goc8I7X7r+wlSK7ocSn7YF9lV98/P997ePtdqb17ZPZWGNMN7199dtqy2qy3jmp0wnJetrKP5FJnltu9snJfd184msPtWqM1PM9uH+/vHzfbdaa3j4+Pr5T57e53VnbxQtTuTnLVTiS3s7LZz/f36Thut1COjeC4j1+/fr3dH/eKIr0PWPPIhOXiJOeer3z3I5Dbi+XcdFfcrwq3xmOw2z8Kt+Yu2bv6m2t7FicM5HYb0lQ7kxw7YyBX2/guLFftzu2eh6/F7RfHW+oy5ezcd/6O5bi/bCLJPchX3nxMVu7m7tk4jquhtc0+/vmnw3Jp87f3u7/ufne3t08EW+xNctUZAznO7TvkK8vdxfY8WVl9vwmS+/XY9aucoDM/FOsEcr0k55iv9KPqZDd37+T2WHqlKDoc988/91YcNyKWY68S0byGssfw9E14iY8RJMfOGMiJUO50gdw+LDeG2rNfI0jOfTbrZ2QgN53kHuQrb+9rJCsXKF1lnnr7wuK4q5xuyyjH/aOW0VvE989X4am798Dt7aHsYtn3V+PgUSRXHZ7jiozP0aRpMaa7szMGcrvUnlT7k1y1Rm1lL8k9yldelw/kWHF/+xpn+D64e0cWsklOhnL/2LizdiBnmM8/d+/FqY7H5jg1nzGa5NixSY4kiNIxZrR9JLfOyIyxwsyqFMXTcb3f1L4Wyf2sEcj1ktzwyalkVm6hQI5TnJzCnEdzzMtArpWtlCTHWiQnZuVu7T/66e59COR2UPaSYhcfv6aRXHXoGbnCWvc+guU2D+XKxemNs1vacqrPSuv9ztGuRXLVqECumEFyVeYQyi0SyGmKW4Dm/CytvDmQ3Bd3Jl9tkmNeuns/SK46sNSMcNxIkmMHC+Rq517fRMYs7ezQCJY7diDHiuz29uG25N/6ntfUvhrJ/YwJ5NgcknsUyl2XC+TYd0Nxs2mO+Zi2cyO5bDrJVafkuK2VvaTYt1+TSa46VCBX9+6vunu/ffHmKzr7l7mzHDtuICcYjiv6q0/LmU/cXvlBctUK2coBksvcSW5GJ0gtiptLcz66Pfd05VSSY6ckuY25fcEX/zam/vb2McBxgyTHjhPIsaLp3TXNpc4y9g0VDxrI1R7uS2v7y2kf7v24vfSF5P66c1wxi+R6Bl5tlpsbyPVRnOgQk2mu8NHbOxee3Nt/9NLd+8Jx23L7ktSuvd7HnYcy2TiSq44SyNURzBft1X3uxNNQbrFArshonuptNMlVPlP7eiT348px7oHcAMlVz/KVM7OVQxQ3h+aYj96+RXIyPmsVmaSyHMH620fmI8l5E8htK/aS5Pymg5tHk9+DJMeOEciVNsX9+jVKyH1DucVGYfZUTE9m+km6clNuL70huVYo94Dkirkk9zCUe5+Zrawp7uPXA0ykOR+9fe9i8OK9k61sM9+d+cjt1SlJbklqT9W4XjWlzyTHFvPv/SR3e2GSq5ug69VGs/w5Se7HNZBjc0nuUShXs9ycQI5ljylO0tyEPrahu3d/Oav0RNdTpT0xG7v907PXl1fu3uGlREHdxMhh3BrhQ2crdbcfS3LVAbKVfSPYXindx0zsWNnKdkHdAMulHi2Vq/whucqN40YEcoMklz1KWF6nT8m5UNxUmvMypCEsZ6ya3d+7ecniZjZovmVeuvunZlXUVvM1Gbc0Kw6u7Kd4swI590NTtw/lJlJ70c5ULkFyG4ZyS0zGDXi4t7Ekx/yl9jVJ7u/SgdwgyTG3UG4lihP1SKNpzs+xPa8s49NxdKa9bt0P3nzWWTss/eJ//PhKi8pLknv6Ire3X7PwNqIa4ZgTkYrkCmIb4yoS/A7kahPoVexckmPHCeSKgSboTss9l/+cJPezdCA3SHJDg0xdfDIlkGPZ/WOMyxtLc8zTsb2o+2wl42q6r9GSj58xmI72qp54+yKdSXG/HE6o2kHZbEWSq8RujrcRe2H4HMj1TMadjuS+7w96wdeYupMtSa7yieQqF44bdYj5pZoSyt0UyY1Lq92/xrq8kTR3wgKMDd39Y7/H7gtwHK82Z8dUNuNDGT5G6YxnHpEcExs+WXhMq8xfkntQTtYbr455RHGMbGV/sraf5RzyNcwbametBVrpKJIrbDzr4X8dSG7U0PMy7EQcEparUpyiOf/maYpzktzjl0iX4Th3lvNJ2ew7u9++3t7EIu+v233wW22Sk9+28IxKPK2tLNIvxxBm/BKCo5Bc8Wyg9zZmSm67UO4pyWVDwxc3khup+p+FA7kHJPc4lBMsdxtDcW9T52nce0NxSpKrfCC576U4rqnK8IPbXUpKs7Rl3TwF4U5y4+IlL6fkns3Hjl8KvUtMU84bbD6fk/4YJ78n1D4cn04juScu/ecpx40L5B6Q3KPDUxXLpatT3DiaY35mK/k5Wmn7eJG++TfGc1fZ6FG0B96e3X8thi92LGUXaZ9196cgjkJyxUj3/iyE+ZoXyAl3v8FpBHPmX5lTTd3HKPmZFyT3oHNPJLkn87F/n3HcyIj78nBk8oTlnNw938FtdtVd6tMhoqMHeB9io9p7q7zyrYYlGB8L1LAudEG5P8kVb8uR3K+0OpCyB8fvb309eT7JMf9I7mGmcjCUG2fk3mcrC8c56a8R2cqNSO5Z/Mpuy5Mcm0NyYwcjl2pqKFeznJvyv5aounMzivttA4wjOWujWi1DYRzj281khpqR4Fc6SovFFlLfnKqtlklYurnYLZT9NKnEHjj4r2wFkqv+T+G3xv89QOcS863ffx6hWNq9txeJffx6G2XkP/854e/fxyHAQ/z3d0aA7bp85m1UtnaOOK5S/7daJJdN9KE/ywZyD0mOPVxGcLumCw5wnhuHy9NSvX7vczm8tzFuMiEj8je7m/QUzdPKjbdxazPe18bH+8ftsatfkuR+uc31Zh8rQq9QT59x3Nsom12A5H7/bxXY5DjGwt16uL0PvyjRGRPLFf9tgOmRXOY+kn9zD+S6e/Kvgudu9W3a/Hk2sfKkeMJxbEGSexbKuXjibLmCBAfRsgeOuk1ZT8lsCOOs30rUf0nlprYnZN2E39coJr19fKxIcBLp+ITGPw/vO7v25Hsmfznhe1ZNaYflfCC5/3uOP5Mt/NF0lKG5jzfaG9zgM8mxbEzl+MeIspsfL0iuHsf04xkDZOm9D0+XCD6OPUf3nks1neVcFLVgHsuFv30juXaII3wmsz2j3Kjy1loUPWa0ku5Ocr2d/P36AA9Zzq32ZAuSmzmEa58msQDJFRuQ3IjiyjElR2914398vJlGGxPK/fGV5FhRpB+jHNmXe9nNFiT314XF+9gqfW4mxbSK5Z8lk5XPSO5x7YmLooqvTXNYG5DcqPX7bfEFdxV9cUurn4wa5WYrktzHZJJ7yHE1y83NTfMAdl+SK547t1Yo+HokV9w2WPi/USQ3QRm8Rnr0bnZuSbCtSG7BY2KXws+SycpnJPd4sdzykfwjw3ByDLf1SW7UdFl7+Zhwb1nP7G0ruhuXr9yC5LLRJPfP52SS+/VR+EFyj8+udfDvrZC8s+NJNn7CYgOSG0HC3zP69630iuT+TmK423iWf3M/ZQQkN/k8cGeSe5iwdIzm0wVo7ua4Ats3ksv6CgfvPSTXrsF/GzUp97Eey2lqqCakK6fOyTnG7en6JPd7direVmSL5FrHDeqjdR/b+gYkt9iU3NvDIGdMuqLwjeSY3nd0QtXB3fkpf09JcmzJZOVzknuQsHT19vNp7st5k5F0fZLLNiG5jxcgubX3Pfnel+SY20zMjQ2TXHbrDXLTVStPFia5t+EgNvv+/s4e7APiGcmN8p9ma+0JHDciXf3XxyTtjiQ3ab3+pZrMciMOPRw7NdumOPcDWDwjuXa6Uni8vnTlrDm5NcsrFTPcNiY5l9qTDUjue35NlUXXLZJL+zO5j0O59UluVHHl19C0quq1rCzuL0dyhTk9Ykr1+AgX8nNKkquWTFY6kNzwtNyoIwgm09wYituE5MYNSe495s36gravB+N/N5L72K3uZBGSe3v7eBtZe7IvyX1NyE0tQXLZEUjui1ZOsoFwz7M1BD8jOO42g+PG1Nv8eDcTueuk3LTU6sXBjO9L7PzBJp03No7i/CO51nBfercWJUibt/3AyNXgfHbq2CRXD/xr0AMnHTzg+iT3yNG5mjMV5Bgk9z2b5NrK62+so5JcNovjPlbY6eUkJDdxc1EHkhva+GTsRsXjae5r/ImVvpGcnapR8hR9dZS36YHc8UlOa5qM+d/ufpPc8CYv98yqxqADlpOQXJe9ihciuYbjJmWnRp3wAJKbz3EuJDc0LedooUVzCuQ4miMUVzgf0Lw6yY095pauIzJ5iqxnyobuGvI18qiTo5NcIy/LRtTe7Epyg5XjfAebYiBf6UhyD9VfeE9yfVVDmf8kN5rjJln9uNgAJDd3Qs6R5AbWERRuPv6ff/75aHZqcz5c84NQ3O29vkl6TJJrdu+kO9KaoT5ZD2d2RxmxjOY1SM5Kzn65h7O7ktzbwxkX2iD35lBkRXLf6tcXJbmb4xTmIUmOOMNJC+Arz0ju5zAkN3kLhUs1meWcDkS6/yMwlube2hRXIzsmycmDGNqnBYkDeN5aR06w9IuftPM9+gnpamsInNaCt0juHxcMLvTP3CcmbzuSHHt7WFXgNpR7f0mS66+QL16D5GZy3BcDyU0kuemn/V0cTTmdRnLf79qpjaG5Poqr73BUklsf2cda5ZXjSe7Jbl4G/wzNQLo7wdVJ7u5ad/KmaxBUr3BcXvCaJPflOio4IMmRqZspRSfjN3QDyc3mOFeS6ys+ceoCZOxOae7R8RyU4u7v5AYuD7yB5NbgOHeSc+U4uq1X39ZXLrUne5KctQKSzymKDLRy29+/Dk1yxTySG+gi91cgOT3aT9Mpgdzb+CQNSG42x7mSXF/xicvXvq0U1Uezc8kgzTU5KmZTnFskB5I7PMm5L6TYk+SsJR939RfltYtfZya5Ia1l80juxweS48nKNOVujE3Z6SSrQHLTSG4OxzmTXA/LjZmTc6W5YYpzrDzxMl3JOFz+2HuhI8l9+EFyv1xvPTQn9+2+Kbcv6UoVcWaKnJxX0L1munIoXCnmkdwfH0guu6cqz3SfwnEMJDeR5OZwnDvJdVnOrbry659BmmufGk4O4WPfbYr7x60syUOSY2KuOrX33yyy29fHl73vNMvkheMVmnpEchMKT6wR7s194Ltn4YnFZFZfKNxXT729JMllL0tyzVKmCYHcJI5D4cnsOG4MyXVYzs1CWdqmObKLCaW5t6bEvktx765Hz3i4hEDXidNSyuzLNAZrmrfnwiOS3KxzlMhcl9dLCGwvRxKu3RMUyVHK2rsrfLwkya2TrvSB5Jo0S/G2CcdhndysBXLjSY5HHxMWg3Oae39Ec2/y0GBCcelHi+Lcd1fxcDE42V2edetxzHrAjHi5sf1hfZJbeceTe6F1/3aUxeBv/cfG9RwiuvBicO93POnvIuz2AunK/ilZt7Wgk6IRkNx8jhtFci2Wc3/0E5r7eHtGce724d22XpYfU267sPO3HY4zF56G5PiBgYwVReZeWrk3yd17fVjfQdlnI7mjLiH4WZHkpsVxIDnBcWxLkrMzltkoX/+I5pppqJkU5+EGzbb7kr36Zk82MjHOtd0cOxnJ/Xr7ut/vH+M84K4kl/WN1Iueaio6R3WKUwj6J+W+f70SyX2PI7mPiRwHkvv78zOX40aSXB3L3Sdu0NxDc2mboudSnH8k1964SR610y7F6ZLhyFDutjrJbX6enPdH7XTOck/rULSvYvhWnIzkeidT2ddLkdy4wpOvbLK3PyfJLRnHjSY5ynKOp8EUhXrNLs3dLJpj2SDF8X3+3EIazw5NLdrui9kzcmZtxH3U4dAnILmb94em3jonYae9u2AsfZ6c/4emHnSD5lF1hiPWgr/dJnPc6Q9NXYLjRpMc2fskdbucVMU/orlHFMcfmWZHJLl2gCbylfcekru1/dwopXz4T3JvLSywbjZdn+R+P+JY10zVMMllt16Se9yz1ie5/5tJcj01Q73pveOeDD5mocjbfUbhxF/PxN4GrFk7sADHjSc5tZrLccsTJr38U5p7THEOw9tekvschd7vdf1+Opvkbi4kx8aS3PvHmntXvs0kuY/Ohjlvs09P3oDk7vMH89YtWiTH0j6We+IU1+e4MVueDBya2pplcDta9WHY/p9nibvUdTouneOn/zs1yf2wRW53mfAG36kzyZkC+mGae7+l39m9Q3Fq035NcY7Bze19ddxmk1zqQHKjTuTIjANfKJrroYbvWST31nOKxUPP4OL+buuRmxPJOa2VsqdjWiQnRox3G2n6RPYjkFzt2gs6x/H2ay7J/XhGcsX9fa2Dn0Fyqu7kp9yN5NQWHk5rCMic1AOa+3KguBoOr3ZdG7evUfTTnpMTo7rvHpJL58zJZbevG3fJXCnX+n9X/t9ZUqqbSE8vlL0pyb05id9/YP2iuM8dzLckeZu4Fz01qf+bht+/+f87+POHTsYpfM8mOT4R9c2kt8iGovYxJDc9b/fXFWNmp+rR+/s/LqnK71numbm//XT4SnLL5CqnkhyTSwmKkU7+QdLSgeKcSO62AUZ5JIfqSlHdUIwpPeiS3PrI5pHcV+eGX/OqTjjJbYDHHeH23MmxpUnu+7cb/mgSm4AxJPf2YFFIWuM+nJgeRXIbwN2nFnd7i/EhAVO2hLdfF75GcuVSHDeV5EoezDnUBjCbpgjN3d5HUpxLcFP4RnKtfKXiLrvyRHaEtBvxuU8PbCB1OtPbt7OFH7N3ay8aKrqtRnKP3XDxjNzbAfB2JPdnDsaQ3MfjcqOHHDCiHf7zieSY6K2f789GONki3v58JCdTlXuTnGA5l5xSu37MVEn209wwxTkFN1uENOOCLEbJSw8LGN3OU29df5scyG0Svz4mOddZ+F/jazWGlb0/yQ0m64bWRy1Acn82ILkRawi+Z6weGVFdtUneztndq6HrY5b7yOY76S1I7q+PJCc4bm+Sq1kuc/JFWepOc48oLnUqrtzd3T9KWDYL27OG5Uw8XDQijxwBbhK/PlmgvOg68MIfkvtdzWC5bq5jAZL7vQXJFSOMb7qiR+QrCp9IzmxP9GBa7u0rXcTbexHANhX1liN87qayLO0ie2ZdPz+S4/YkOclyrPh26gpskOZ4meZ7D8WxzvIhN4rbJqQZSXJiYaGVqhXjX1VO+kEmpgvVTq7Cbhu/Pia54m1BknNs4NsWJHd/3vhvI4oO5pPc9yYk556vZPfpg5kRY7nv/zyKabK+k3/bS2buxXzvzDYhuafcXgxsWvBsVoUNBB33JzFuWUqO84Dk6ldxdfODNNdEc+/6bLXpFLdRSDP+sB0+nlG1ZqRN0vu9dXYcvzDLRneObH9un+Hqujk+5hPJFc89wMfzIvrXJblnGVu3A5aeJ2n/86jy5H59wnIf48fC+2Urn5NcOliGnT3JtQz5kuIJyTF/SM59ovZRNPf1lOKYV97+VizU9ksp8eZBADvipNCFxvfZbYvqSofxeB2qv/UVHfRpdz7J/dmE5EZMyrFsYhQ/ZkNH9scPd9+dPu9huY+vpbzENiT3jNvZfZDk0omzR49rtT0iOeYaySmauw7RXGbStLMobqMpuWXGaAtio/j1ca+d7OqmJispya1Jd7+djPv+9WbPxwzE4/NJ7vc2JDfiDVn6No3jRviwwieSs5zU+3u7jHhBJ/H3ZUmOvSLJPaK5ymxfOYvitgppfCO5zAeSm+rqJu3nJXNGm2Qr726Vnnykdvv6eHt7+/q6t1PTD0iO9SjvYfD4vRHJfY/q2bfx52Tfv8f07O9tSM5tUs5eFvT+0bsedIlAjm1Ecs+4PV2a5O5ukVzlBcmNe40+mitoV5lHcZt5+2XylUWWZUtssb1V/PqM21m6QMbyzbnoprhtRHKuL8SK4rtW6XfxSKmdvSvf+vYK+T4WyXFbHjcx9zbyBK3qzx8vYpo+n59mWolvH9MO29w7W/mU24vbsiT3pPCk9IvkyrFfHKS5+RS3IcktYMG6tHaBtTRbSX17qtz7zGDu7e7eHNlWJPe7WhDtUwg++pthfrZyLsn9GTeQY8WYQP5rbHFVsRXJOeUrbxbFsdrwxTEp6eLJnq047im3dypIxpHcuDUEi2crp5FcNZXkRG/oobklKG5Dbz/bhJnpEWk2NyzcjNqfB7Dsu+4NXxNxu49qi7rbbUNy9xVJLh27cfH3ZiQ3ds9F9u1aYPsxvmt/b0VyTqFcppx8HZEoSVgdxtPMc7qIf94skHPg9sJGNorkWl9+NgXoFcmNzlcO0Ny1e+DIBIrb0NvPzlem7mtG/KF2F27nvX0qRhl1dicsd7v7kK8cQXK6JdNfL0NyIhmz/GScmpvyiuRYr7smi8LSZUxmO5IbvenJOJIbd+/SN5IrJ327SB9vgp9OmqtKb0cJ5TL3SViHuanjcPui1TYNya28lmDJfOWXvRRwNMn93ozk/kxQdpE9nZa9TRnTFduR3PSNHJuU3kIct122cszW1KuTHFt8Sm4XkntCc+nEcowNvf1tXqPbGe55uY0N41efykrvFskdJl+Z2gfwDJHc9+xAbj7JfU/r12+LTsZpb78Zyf2d2hmbQG4pjtswkBvN7euSHPOC5JpJuck2MURzUyluU28/L6ZhI0rGvaJ2f0hODps3IrkFA1imC/HU0RMjSe73hiT3Z2K/vj8qnZ3Ut4stSe5nlkU+WwLmK8mN5fYVSa70jOSmh3JDNJdOF+12GHefLVisuSm1373JV3Jq24zkVshX/vr1cc+yoUmsIZL73pTkJiqbDZ1LcJs6fN0wWzl5T351jvRiYdy22crR3L4eyTEfSW46y3Vpbk7iblNvPy+UW5LkbrczhnKZyFbezJTcyrNyC3J74VRi7wXJTT7TmvVNzX19T076/N2U5H5mcFyaLeeZNw3kxnL7eiRXLj8ltyvJVdb5Mtd5c1O347j7BUluY2r3JZRTuSHJbrdDlZ7cp5PcGI5bgOT+TFd2Z2rubYb3/9mW5P5O5rglKW7jQG4st69GcmsEclNJbjGWM6vpZy4t2djbzwrliuVIbmOh76knHHe/U5K7rb1X85Lc/nUYkvue1a0n7dXW7+3/bEpy49+VpSMXefoXyI3k9tVIrnxJkpPDoNmp7K05bk4ot1zhyebU7kco12Qrb2K53Ookt+iuJ7eJJDeK45YguT+zlN3s9DWvTIuXnfg8PVWJldKL2/jWHDdO7LVIjvlKcqUPbm9zkpvTc+/uO3L7JnbqBcd1zqnS6csjhHLs6e5nS5CcxXITOe97npzZ7e3t7WNu233//fvftu6eeWDjmwdy40K5tUhuFY6bTHJLhnJLjI5vR3L3Vr4y/T4Ixy3v7mdkKzfGoqFcrf+38SRXjGO3/UO5hbz997fXMc1K2J7jRom9Eskxf0luf5bL0h0wwwNky2x4sofU2e7K/l5bxD7mWNjdF/xgno+Pt7Ek9ycj+POH/jwqROtD3193V3Yh3+NnU+zP7cXPDtif5NbhuOkk51UoJ6hWN8/ibUTFJdQ+R2y9dPQ+hzTYxmKrx51A2dWiyn7gBdxJjrWlXuWF2mL7oWy2tdh7S822lnqs2OuQHPOZ5EqPbGLtRxGx5zyrcDl0YoQD2Ebskym7WkjZS5AclH0isb1X9iokx0qvSY7t3xN28HtzF0/MPDJ1B2/vgwc4prLnktyW3r6Csj2gdq/FLsaQXLq3si8vYBTlXg6gPE1P8EfZe3j71cR2P4XgxMo+n9i7Se38NKb2qno82aImZbK9lb0QyZWn6QnedIVNqd2bUG5HZa/wOOdDU6Hs4yv7pRya3MPjyQ4eTLCc6zYf60l9Ob5RbOsAfOH2jXvCuZW9lrvvP3/t7e6TssudlV2+irKP0LNHsByvKHj2eowX/rLdlX05fFfYuCd4Ijbb2Nt74gFeTNmsL5R7u317pWzmhY2fpmdvbuMTle32cs4SrGnjl6MbxeYOwA8PsLm3h7LXUDbrWc+fFlC2B8ou0bM3V/ZKYl8ObhRse5vwwSjKzR0AlL3bkIZB2VA2HNp2l7aNAAAgAElEQVROJOeBUZT72cSORrGHA4CyT6vs8izKrk6tbPaqyr4cu3V26Qm7ewC2pwPYj+XKPRwAlL2Pstmeyj61Q3s9ZV8O7QHKXW1iL6PYx+3t7gG8UPZeYu+s7LPZuB8ODcr2ieR2Moqd3N7eRrFTT9hZ2QzK3kPsUyu7PKey2S7KXs+hXQ5sFLv1hLbU7Bw9wQdlb0/t+3oAKPtEYpendGjrK/tyXKPYsSfsaRQ7Sg1lQ9lQNpR9NGUvTXLlKXrCjn2h9KgrQNlQNpT9kmK/FLVfjtoX9jWJjtRsW5PwReydlL2TA2BQ9obK9kRsKPvwYl8O2jw7u729WG7nnrCzsksoG8p+0SGNd8pmr6Psy/JGUW5oEvvZxC5G4Y/UWyqbQdnnEdtDZZdQ9rGVfTmkUXjAcTsYBfOgJ3RZjp1S2dvZ+LmUzc6t7PLcyvaY5LpGwV7fJHqMotzIJPwQ+1zKrs6tbLaXstkZlF2dU9mbDdov6xgF27T/7+z3thG79MTbb8xy51Q2g7Kh7Bcftm+n7MtKrVNuaBL7HXi1YV/whuN6HB+UvbqymT/KZlA2lL2gstd3aJe1jGKt5vHIJDbsC8wnsaHsdckdyj6Rsv0Uu9zBoa0n1eVYfcGr/r+d2KVfYm/l+M6pbAZlQ9n+SF1W2zi0FaW6rNk8bIO22bUnNFKv2ReYv2KfStnbOL5zKrvP2TMo++WVvYnYlwM1j3/Ovp/lFha79FnsUym7grLXF5udx8bL0zu0bZR9OU5f8NEkBoxiwRCf+dj/1+8LpZdis5VtHMr2Uuz1lV1C2ccguXVdQOmnSazt+Dzt/7som0HZryg2819s+w2h7IMp+7J+8yzRPt56vVWlPqnYpcdiQ9knUvaQv4dDW4LithP7skXzzG0frzsClXpRq2CHEXsTZXsuNjuLjUPZUPYSUm8o9mWb5pnTPqXnJvHIKqa/I/O9/w+TO5T9cs5+Y2WzEym7hLIPRnLDzTPRLMoDdIRH5D7RLI4h9trKLqHs11V2dTRlMyj7oGJfNmye8e1THoTjlhW7PIzYD6VmUPZLKfuh54OyoWxvlX3Z1ipGNBB7aBGemcQTqUf0hvJAHWFVZZdQ9osquzqsshmUfURlXzZ3AS4tNPQ9f03iWV9wsotnUnsudgmxR/i+g0sNG5+tbDi0jcS+7NE8j5rowRe8Ngk3sYfefVBq/8WGsscqu4KyX0/ZLyg2c7LxGVJvJfZlP6uwxHa6yO+e4GYVo6QujyA1lA1lv7rYbFUbh7IPSXLjrKJ8Ca/XlvqkYi8r9UnFhrKh7NeTei+xLwdon8N0hEVp7khSQ9mLil3BxqFsKPsAJNdqHnYKZ7+YVbCDiQ1lQ9lQNpTtpdgXv9uHHc0kulKzc0gNZS8kdgUbh7Kh7MOQ3Mz2YYd09j1Ss1NIfVKxoWwo+5UHry+g7MvWzePYQD3fqw6E5cQ+p9RQ9hHFhrJPJHZ5HBu/7GEVj5uo/wvVwTBW6vJ1pT6p2CVsHMqGsj2Q+rKf6+s5wGIQ1QHhKHX5WlKfVGw228ahbCgbyj4uyT00i+eojgoGsSE2pIbYkHpXsS/et091aJxT6grKhrKhbCjbD7EvfrdP9QKA1BAbYkNqKPsMJDeygaoXwjnFZhAbUkNsKPtkJOfYQtUL4pxiQ9kQG1JD2WcjucdNVL00zik1lA1lQ9kQ+3QkBwAAAAAgOQAAAAAAyQEAAAAASA4AAAAAyQEAAAAASA4AAAAAQHIAAAAAAJIDAAAAAJAcAAAAAIDkAAAAAJAcAAAAAIDkAAAAAAAkBwAAAAAgOQAAAAAAyQEAAAAASA4AAAAAyQEAAAAASA4AAAAAQHIAAAAAAJIDAAAAAJDcKyPL0AbAAcHSAo3wWihSBpIzjXG7Xj9v9+zhJbeb8OG3W/oqzZXerp/X27KGcP3337u/8j7VXX1J5mAu0hYOo2aFNHvBPv9UaiP9E477999/jz8+Kxpdj6bs+lvFS4md1Spl7uZyEO1PI7midswSn8Ny8gYTlvDvv9cX8QKfWuzbcr6v4K3oq8S1wLf5lzS2cBAYPS+ras9x/beFJ2aZ/uuiet+REYGv40bjR2b5frFrX/1vupxvODLJ0QYaFnQSyX26N/LWYNQJfC43gvPZWEByi6r6tUiueIlIznJm/17HKPtVSK4Re0wk99okJ9rn85amd9Et0iVJ7l9/Se4qBz1pehN2sZjrY6m/HeW8JPeZck1f//U5zl7a76UCWvgaz5IQrzAnx03zxnV9+xw7pDk6yfWIPUKlL01yBWG27HNYzy9GcvfGFBinuRdJwYLkHsrEPj0edq2D20lsm5im9vDXcf366CQ3UewzkJyVty1a7VM8JbluNFwcgeTE1BnrbQPxcTEsX8WYy5+GHuwjyRXdSx7q9bAkJ1ju2qvoHs2w59pjXunXleQK5mC47JCzlxnNyrT7dZ9y2BDJHUr+h2I/cGVDvsHnmH48yTGb1m5NFrcQke+nrj3sITl5BZ3eVX+RRUrppwydPz0cSd5sg65f9F2Gsp+fVVYLIV6Z3Xmu51pUt89PbQCpTHNmOvStr5eRoC7Mun5+8gYpPgmuVovuV/vQR3LZ1VKzuET87ZNUnbbe/LgkJ7LUHUW3bV1ar9BqNtQG3DxEM12z5oqruKLwzNJN/xN2nF11c9i2zOomyfpt+qjeXiSqLYUa5XBpC9kCpgEan8BkUZr6pHZkZkB8Jz97L7ZQZY+t9ghP0x0y5SlNP1NW0f75UCSX2vNRxfv7u5Tj3qq47JJc2pnezazvpI4z3jv5vWtPK2Q8h/mpeL/QBQuZkTkzNQxXLfCnnuFXyU9lLEV3yj9zqGHdmuRIYW3z+mm7RiMbsIUDklyqSI4qumPrRPdmSNzRXvHeuqSZ/Pcp6WWRHHdqurysbcvax/fY9GFJLjPKaCuUqX5Nqu1Yz8U3nfZJ+x2H32Kbftqy1R7hm05i3La8lnSemx+ufDzJXQdeXDTBVYzypeY7JCeu+KRz+bKXm+9kVzlSvPpnFUUroGfqdy7BVfV7SVOfn1LIq76MC2RsgzuEm7yojgXZEMldTet8ytbZyXd0Se7aCPRpOvG/WiKqV/rmBya5myE5o+iurSvtEUV326CQ9VpXM8uXqj/sqF8XkvtUMnVsmZJcy6YPS3IV5SktvPHzN20EdgMo1V6NfVRX04a+lqD2i637adtWe4Q3nYQaetp0mKFE0CFIbuDFMx2gZXryqk1y1hXSSEQbMjlGvOrGTg/QFUwzSA3LJcNcuzy5UdyMseskZ6E1n+lAVkREWV+D3tSjeOuIIKDYbzDYUTZ3zHfG0zPk9aXYPKvV6NV+82OnKz9biu7auuD+u1Yr620DbQtMDxP5B0xd4dEUfovkdBqqa8uE5No2fVySuzY0ZbQlu6OMVRiptjMkd9VcoH9IzSTOzVPL7xeb+GzLVnuEN52kbegNrWeemMNiJHc1jZaSIId2mmZ0oxv4bijN5EB9Jbm0bavEKq7MjNk+TRtJWdPbZ9pO7ehZnZ7wnjYfbx2V/Cl2a5WOsrPbtclWpJascszS/+bHJbmUjHGVogds/WZJ3G0DM7PFrnIvEaPV7OrT3hFtkjMt0bblBzZ9XJK7Nf1aTUcpvbEmc6kFpg1wM13gkyR6qurd0xrEYbGrHlvtEV53Epbe3ouKjmRN7/EkWzmB5PrtmJp3d1RwbcXtn/JHEqHc1B4xvvaSe9tLE6tIK5uouzWnDSeQ66+2sZCGvFksIh+2T095EGOQ1/+XlJdkvW9+VJIr0n+byVctVI+tk+6sdkLrtkEnz+/phiFtkiv6midt+fiWTR+a5D7tRkgb3rqSZmFNA9yawmvdBXRDeLtgvl9sQnKWrfYI3/UNhbqlCQh82dljKZKj5Sj3ZhaDdBoaC93EPfrGfUeM5FjVGbe0SU6keO62bd1UgaZlLMyUd9Huke7lOwZJTnh/TXJU7Hvvmx9wxxNe5ConSO+VregeW+8klHva4N7eN8qqVfOW5DpjcWPLlORaNn14kiP+uZAqZmRMoqiMNYO6W2vkovOVqa97CTwmubat9gjf9g0s0xneomsXr5GupCSgnEAPydm7gfWlbH0mOdYiuVaIQkexzc8sldO3hORaDsRq0CYTlo3aYmlDksuMQJ1A9NPotfXmR97WK7VyMf22/m97k+2eNmDyrp8mNamuud79qkhsF54QP2bbMmuX5PmToJru7VXPbW1wVtAMpHHjugGo9nV3UFd7u2L68Zxc21Z7hCeiFentk5YJK9/oza4Ck6orr70947Oyqb5FcjfbbK79G6X5XHiS9bh/m+RuHedApXYgObIar+Um9+GIbh9NP1s1oK0ijWvvmx+W5MwyqQFPruyiY7Z92jOrLz7pfkG7LhEZRXJtW35JklPm3FJf1lr5LfVNSC5tu8ebKbguDiR2s4TAttUe4U3HL652Q5nw1Zt9iC9T+oBFTVnGz2qgc1ZZbyTHr7CP8ThSJMdaEyhZt6KiL5K7qm0+Mz40ek5yWSsrQM898YPkVNn7LS1uDyO51psfj+SuRQ1mK9xOUVJb7yW5rvbk4m+qZD0E9rfwhNq0bcuvSHJ6CkVauUFPJDdMcrfGXlJv5yj7xSaqtGy1R3iz9EmugudH9mg7lu0yZrNn30guszu0Yio6T5E+nZOzW+sIJNeeitVUb3fwf4kYOvpXG164kJxVr+LFlHWH5Jr1voTkruTTW++bH3gJQVeCHlvvnYTv155cU9v4FzmX4RE1DJBc15ZfkeQGqypGzsmp4rqrt9Wm/WK3+qmx1QdzcqYanNi8KLXyZw/UyyQPQBhaOX/aqa/2PEynKLfPl2Rp6vUSArlAzB6o9/g+kmq8VnTamTmkK9m7tWOED03R7usk/UKrK1vjn+6bvxTJ9dg6GQLVlpw91l7a/uzmVVJrgOS6tvyCJJd1V3NTkruSWJ5UVxLtNwN37ugLb4IZR7E7/TRtYtaW8LqT2CvDzAK5T11Cf0ySS+nuPbrSShTZMSPrvUty1nYvN0FpN2vDiGzAvfjj+Jodmz57tvJgZmUw+2x2xiAG8IzkWttjksLk9LrTcTxtdTSvz8icHNnoR5h/981fiuR6bJ3w1qfZ98FugyK96d3s5MXZ7XrtG1V7SnJdW349kstMkE1HrGSpWGZlXHQDpNYain+bZbP+ZisHxFaq7Nhqj/CE5O5V2399Ctl9MYXLtF6gtqbNmkMazDHBxSdZWUR7R3OQsB65FpouCzMWutK9/r1CYTZglTuadIruVLsUhdqrlVpS5lB40h7gN8uPM28Wg5sJaMPjldnOh91oeGu/+WuRXNfWK7MPxI1Yv9UGzOoqGZ2AvXrVOg/SlbYtvxbJsSIlx2Myo1m9kQczp0gW13/thYKN9u8krffp3a6kz8XOmuUSlq32CK87iVF5SsS9PT5P+wAkp47IVsuIPpv01b+ft3tDAJ29K8UVV1luejORr969r9n65PPqZTCn1kB8WqUDlvduNrDVMqtzVq8O1ZVy3ygNvSFM/e3bjnM2n+0NNa9SoNsnra68GiWqUWznzV+L5Lq2rg8SvjX7v3Ta4CbrNm7W8KD+w/XzX5+39aITrrYtvw7JdRaM6L3KbqnZWZT9qwy9MX1Gt+n993qXp07Y/uJYYtNtvait9ghP/Ra/9Eo5PfOK4C9T+0HnQAF2HVhZZDoKueLK2jfSu0l8+nsWc/bZ3xfa7F+3CY1eJSden5Fc2q5Zpq3zud8GzfZiL7Pd+pVkodN7ezf99pu/GMl1bJ06jdtAG1Rt82+2rvdqHnqI5Dq2/IIkR9ZypK3OKKS9Wl6PWbs0tntq8e+/Hh8r2i+2UWXLVnuEN53E2Hlm72nljyVcJjaRkszasEEtobpmVT/JmSs+mz6tWq65j0h6eTpby/T734pe3yfqbkUGv33UTlpNITndOp/3Hb19az26WhVzY5TklGLJaWKtN38xkuvYukli99h2o722+Rd9vchbkuvY8ouR3GfrNDytHVVSKqW9df9ka594Lq+3q+4Xu1Glbas9wpOjdj4VUdq7/PgzcrtM/mYh1se1/5Zm7Mm32iUU9V+K9l/8Pa6D9YndBSnOYh35Rj3vaYvu0D/6XqluF+b9my+Jrq2zLMueaK9j7G7m5I31z7Llw4FlRDua0TqG3nfxURl/yFafCN+14tSnWqpLBSxoFw9CAQAADsx4Y+My9u/rOIHRwnu1UzdIbjnUYXvqYbAOAMD2fv7m7ZZe6wufeeUAQXLLQRwamjEm8vOfaA8AOCvJ3W7XFwrkxgm/az04SG5dkGq53cohAQDY3c/LdWWf7JTCe7dAECS3pCnc2mXkAACcMJihZYgnE76wVpaB5F4vmMv4wdAZwjgAeDGk6YjS0izNTiv8UP0pSA4AAAAAQHIAAAAAAJIDAAAAQHIAAAAAAJIDAAAAAJAcAAAAAIDkAAAAAAAkBwAAAAAgOQAAAAAAyQEAAAAgOQAAAAAAyQEAAAAASA4AAAAAQHIAAAAAsDnJlRwuFyZxc+ACK9cXpVz7fAeW52Vfe+h/1nyXPC43lNSSepWnWfL4BlYqvK6Iw2+dT3hr0VJxMrEPUEexPeYrerGeteV7cKnN48s438+tujXLnKYZTXL5RSBMnr7r5dJcE16SZQR+9GLhulYR9cpdXi7qH+tdgoUlDC+R6QWJ+HkrBJfLmK7jKF+4lAxrOIbwojDLXsNN1bQMYi51EI+3kZw7BksVHSsdsnTLUWxP6sqfxXs0dtR1JVuLzeSbDPqrZHMr7m2WZI6NTCG5MORe4KlVxFE5m+Qi9++tTHKsdvZBGHTknktyjhImYW5uu6nd8d7grruWoxuWT8izyOsFy8sc1poWGP1NqpqlRNwQfBzn1Ld7SI5F0WPvOGjp1FHsQXLSn20/ItmZ5ELuzaTTzIdjlrOSnJT9ko9yGwcnORZewlIOdfM9SI7edlO7iy7BiMc5k9xynXUVkpv6zskBozcrjstl3x4pf9DjDNxJbldM82evQHLcq+VPO7cnJBddZgyEppJc/dS4HrzFVRJx083jKOFvIX+ryihieoDGP9Juo0wiK/vL9PfEZVHCY+e8vlxdJjKE9a/8OeqT+qM8j0N5r0j1GnHbdUmuDuhLJXdkSUJJrpGhdr316+VtKc3L6yu1hPJPzSSOEJU3oyDW+kYJ/3894OQNy+2uEX5dsLrzXy5MvRPTUscRUZ3Rfl5LEwqpu/Lx68ncC5dH30XZiPq7sRDxx0Q2IjGcpHluZZqkvrplWouRXFt50uj5s4wOylraWA2BLlE8SsSY5VG7b/A/57v4+1w5Nu5SLB231Ef6XRPJxZaxS+9Y6l5A+7L0EqYduKPgf26aaHuSM/5Mdz5q7VXLs9lGQUVItATq66VwU3nb2UnFx8aVqHtTkqNPKJPF7UGLXYoBjTJB3bWkJNKVKZLTHdpuBv7NWEXjOX1pfUPdBXSnmNosJf9bqd9pM5Ljr1JegkjkNuQkBqcgHWuEengnZrIi6TZExp/wdBmIP+TmI04kCb9aXpar65X2hfS1MV6415WfiehK/hivSnKhTuGUcWweGTKL5BoZapITv4QtKfXLmytz0yKmCcmgTg6pGXc5vA/KSwTJiVbdYhqBa1L5fK2YWHi1RMtKXj22f6DyyUYI8mZYH5u7xIluK9VUcTVoOKXdTrpJOqa1GMk1z1PKK5Vuc6MDNb+R80BWK91JRNmGslMTAaLLVtpth1oRtXaq47b6dOCnXpl3dWmyjbEnSu1xk/a+EJ9B2kF+W7ZqtBfJxS0/Q61d6qrxbLZR6AyG8YakswTSXnPbDViKbxwZJTnrCYH54uJiG2Fp19LK4P4tIdZZm4LVDNJRlfI/jL50oi05UXYUzmsWVg+wcnlluSHJce2WXKSce+AwKaVHlq8bSuFy0Tr1R4FmwKSsXzVpUoD14DjizZbzy/JQNji/TN6tbou4ZC2S419SX4hURxK3WZXkbCOrnx7z1w0pyREZxEQWFzvSUsZCO9bLh8qyuIS8CfOS5ExE40WK6kM10ORz+2IYUF/MBd5gtM/dnbJo89hSVKMkuZSPaL8mgEteVj3yiUYo63CY2QxQ34W79DiXmTJ+FVNxRL/h1EMZ3n76PqRJxNXx0iRHTFspr+TOvhaufif+7ozP1sZlKcy4ljA0lTBPReRWJO4UWSKKgCoPtk+fmYGcpDui4476lD2INk8IyZEuzf0cI8U3TV8WPoO0g/p23Z7x9jUo2p9xndsk11g78QChtARiFITkpGS6cSLeePwmwk9QN0AVTx1ZQ3Jts0vChdPyhuSEdxGc1HStUrqv0PAf6dCkGUhfUa7ZvLRNclza+c1SX1mySaHMrDm5UpGxtIREO2P1B9FNJaXlF8N6fJhAogT+8vUAkYlWUA2uJgYCk8e3SS4kCf7gUprbrElypUrZWWNeZvq27OCNDFJe2VaJfuHYvDy50kgYMCsBHgsPGfBGiOU3YzInJ14m2MAhCE0yrV/x2ItUayxeMrS1r+bkuvJRo6cMIAfyke4spchGqCFSr+HIO+R5a07OXM2W8fYXNSa1TTtUTZLrF1M/JzKvy+wZjKciSisqL5JRjIhyJFvmm9fVNwalMjFGxx31NZcnQURIjnTp+jvMKjA1fVkloBkdDZfaecbbk1ye50mkaIqSHLMGt0KdTF1m+ztDcjkdAZi8b9J1A43iiSNrSK5rdkuZdofkmMoN0a5VmgxFKT8iHbppBurEw1ZfsUkuXqRZZPPmG0Vyprqy1IQXqeZSTke+eZPD4C8oGiHnMOqKbIsWlmbyApW4rI/kYumHxL1440Y697MdyYXSnGMZzZIxmJJBmxBvAvV6OekS5EpDAiEXp5GBX57ztGQpH2aTXNjTfCtNRYbmSfSx0icrEm+0TwpPbPkiPkbjI7bYZoBmVKilKfPk0gyqhSelhpPw0Z5ljkFFB3v5QiQXcCSWcOoN1bOkpPqBpYrTekhuUERdrhHJRJkRkffyXeoyG5KLm+mGXIlu1BeqpqHJNdPViU3W5mKPO62+3G4H25VsS3Im22yTXKuTNSFPj1FcLI4ynYVYJ3UDRPHUkTU36H1CvgrJlYbJmq6l6EpYt9GIcW2hpSZmRKcvbZNcvkiziEBxWhpumlEEkUU+cZNi4//IPi0Xz4QmqDXmpIWmUx9JqFcmaUevEnb9JMfMvWJ9m3y7dCVTlJfoPL6aQjMy6Jo//vq65ob6gOZKJWF8aS/Nqh+RXJL6f6W6LG5XV8YbkFwdSsZxHKjxV9Ty3eLVLO2r3tGRz6w8ix4ygFyLeBHG0m84TM6EMZvk9NUL8X5IvH3cHq10SS4PL9ZElKOIlbIibtZW3xBTEzuU1XfSlUbHVH1BS0MWyZEunbSn2GySI+1gkVy4PclFkcyKt+fkWhZlPFuPUfSQXGR7c+oGiOKpI2tu0POEYCWSawaqTdcqyZSUFIR06MbBN2qNZbqzeelhkpveLGIeL0g2IjnGGC2PMDWfai4lloN6RXKBGSTyDI9ESa3e5GqTnKb3KjWJNRDJmXvl+jbJhoUnypfHFskRGWiiP9IcHFRN0GmuNCQXKHmI7US8yiFK1MTXHiTX8r0DJEe0bxKbLfnC+g9KXQ8YgAWXME7KSBjLkOGUcd0ZgzbJBZ1h00Ik1wg3RHK5kLYMnEiOiKitKJIkR0RkCXe7O5S0h7QBLJJrqa+yF2+QSM7q0tYUm9WXaTvsTXKVzgk+Jjnj2XqM4jnJUTdAFE8dGSW5zhPWIjmajtZdS+et+DvrIM906MbBNxFuKMsvmpdWNwyfktyoZsnjcFJp0uTCE9Lfcz1ZIV89UBKSoT1TI9d2p5LVqUmpvIAiuUtzb9kGKgUc2enK1m3ilZcQXMgSAuUFIx1Mk0GJJrlSv7d6vcR6+YR0r6Q3DK2v53+r3UsTBWxOcrHJokWDJGdpX7v+tnydd+1lACVf2FQwlb2GUzaJNfUVpZ3gUi5LclS4IZJTCnaL5KiIoZ53iLoiklLHLf19ovmp7I3WW8kNSciUpkiX5nxlrT6zSI62gwckVzZ1zKrpuyRX24L20Y1RmFLAZyRH3YBSfKjyci132m92K5FcTutEdNfSs4smJqMd2jQDGeHQdEopJ6yaqfKHJDeqWaqqs6/OZiSn9uUJNXsHTROpP8ohXaimMyMy56sSwNoLqICo0nUkRsORWDTQ8EQs6ltZEJbqNvm61ZVcrPopTE69yDqzxrQJyek5Ofn2AZEyaZFc1OQhdRPGzY5KpZz2lEsOmoIetinJBY3XGyQ5S/u2T2/kU7YZBclzBshVU4W5yFDYhiONP6BrLZh5erxQCVpDclS4xySXKJILXUhOuxbu3WUJLRFR6lV42nzbXR15cSOrmFoMbg1kWuqrSJvTwhPSpdWorinIlO1nkVx+8YTklOkQP9MlOe6SRJNQoxA6ZOFzkqNuQLoP6eiII2tu0GN2q5BcqZxtYnetUrqv2PR62qFNM5AEAPVhoaqIlCuJnpGce7PkQVA204Wbk5zw6VEgXksuAWgmaXhpMP9IFJbV/42iC00A1H9QMl2iOAxU4Yn4SqASX6Fad1UbINE5Jx2eS1cUyue4V97WK1RbHqkYTiz4SOgSAiJDzgsXdMqpkbJhaHOllpA3IQ/Gc9KlKHdHqkovCDckuaZG6ELz7DbJWdrnqsj75JN/6iwhaE1Y8UbgW6wkqr25Xm3DiVr3UU1ScpsJl0rxkawnEe5RupLneZS5h7GriGpBnF4Ho0VMpIjceW69Ws7a1ovquK0+NejXbd7QVGPsiRqrRiR/GSakFM20gw8kZ7yy9jM9JBdrz0aMohTfcCA56ga4bUfcY0WWIyM36Jrd8iQXBGbWNNGLs6SOuRVr1SYt90yagcxplG0akLVbz9OVI5qFePw9SE5Ovaup8vJCKxHER0Eue7mYbKajwdj0KG5eoZ4EzZtGy/IAACAASURBVMVCCm1YajFimNCMX8U4h6if+Y/RRhs0583TQ5N+aRaDhypdGeaB2c2ZSxnEtMs0VxoJRRPS3Q5jnYgj38xDkvBen+QiUicSDZIc1T6LjLNoyReHVjVFPwOI9Z6xymvFURizjuG07qOaRF6x1GaRdGqvEW6w8CQR66QDvSVW4CxilURhVKolA42IiTGFcOtlY2Ldseqhlo7bzW56RJhXFk2ZLp3ognA6po2agXHTDj6QHH85Rv1MD8k1shCLF+u6WPCc5IgbkC2XSMU3jozeoGN2y5Mc13SYkLc1OubhUqjcV7MYPGzGA3Gru4SdZuGEFebPC09GNEtJPP7KJDfIAcMxEP2ocyZNs5NVSdqBbFiu/tyTtyH32uY8COvl+x6ZD4ja3SqfXmmq7h2EYH4e39KRpkc+5tJLy7ydMM3bTdhuAv17uYFwDobx1BTbIho/Su6iL9n+DJrB9Xm96svZk17yoCVLL/evftDijKbKSpdvPDKTpy22Q09nZRO8tF6q6dCsnTGkQ7GyGt8u7s1SbrSEYHUce49bYCkzCFVg9Loi5qEaqOZQ9xEQbx5m7jVkfdjr2s2w9Dr15QGSA/wkOTUzFb+uiLmclzqL6zw4dlnV4R/JtZshCfzvoyA5wFOWC6au/TxQKDfpiFJgD4SX8CwRd/mgSrndDLHD+dkgOQAY6mwnEJFBzQAAkgMAAAAAkBwAAAAAgOQAAAAAkFwPyniz6dc8JotRMHkBbAkY3ECztP0Be9qAh2hJxyVYDpexl5pKfiBwEh+ki4wmuXjhI2ofgB65GB+r5rI8Qc3EayNYbIUetYXDU2d7D5bhWmjTYxO/10hIlZSO6nbYeGTzbWpWdWSiXXrNll2OIuj4bb22qxhNSLHqwUjucjkXy9lB9/JdzZ0aFqKR/LJUF6ZH0rTO1gbJeYDOWa4guQ7J9ZttHB3EyR1lTg4k57nvi1bs5GRbUvYsPbYQjfC9g6OFXh4kB5J7RZI7jjce3Wf5meBJlJdxlDC+i64aw+dxFCV6WB/FLJe/lYn1Z2oh9UfydxbF9R3lVSyK5FkG9e/1U8ztVJcht+N/T7aiEvV6pXg7Zp6s3pAPaagQOd/GQnzExMtLcesGEm0lr+SiE2HE12iD8Gccx4xo0L0qyQ07VXoY8Wywy0XvVqQMmRqj+FMcMamjRlPGOusLmFIm321Z2bTcgFZcwbY0Xncj7/Rq00dVfzMnOeu/q1ZPuuK0SY50/9zcf38nrlTCnXkuhK/sbtvyVYrkjPo61pH02D+9miVRq6qh9h7mAm5UUWw1cFcpi3Sp+oExf2nqobj5Mtut8XZpzNZ24SKSU92gbP4oW8KWQkV9Jf+rebS/JKePAuD7UURyL2u5HbvYo19eIfZx0KcJ8j+X+s9kUKc+EsdVBM0BPOYkSX3w+8Dt1BM3IoJEH1cqj3VRZ2Xb5wI3QsT61eSlQW5eXR7HEogLcnncPPEK+ugLfcUxt355BZJLxHHHSee+xBiDi/glToxZN9aZiIOYhHKlmTJi8pFlQl7FNO1e3fRRdS5QFKozcLV5yu3U+/pii+TksULx5j33qWlJSfhRO0ZW0m3bXVOfKqElsKyDNpHVIcjV4aW9lb4yJH2amzCm5oFdpSzk0ORROtRDBfoRrePOjdkSHRrCV28flMaImtMbiPmoI2nkgarLHYq1Lsld4jwRbZ+osz2DnOXidKicf5bLc8r5oYhlLuXmBzKWzckV/LIyF6c08DOZEr3LtWGzpqF6bxddwqRMNusrF3NkCgtrqfO4xxqMEIxvultW8tJSnAXIglrcMlKHywdCoiDhPMdMw9bCJYE6HLX+LN7fEfQOMnXAKX+TY7eekWzZilPrIWx9UcxPpTRDuaGQ2AqOavUbkuOdv/5ch83ka+oT7m7KzmB5AlHHOgKhbowaY8D1zbt9nMf66D9tndxT5fzDWhx+4Kj2ADUbxCWzTMgrkmv1atJHxQGY3Dzlaa9BLWlgzpSO+VZPuX3aXIvkxOGY8jgifhxumfhC8UoltMuRbmt3TX1uDPE91DpoE1nNSq42dkFJjluDPF42EAZGG7ijlKVIrn6rvKSi1j+WTOy+3CI53UZEh5Tk6m6QBPQkbO0vqflYJCcefQCSkzwTaOouha+Km89KRf0qLCuV8Cy3GZ+ps6RymgRm5k7x4O3UCbGb5fvFuzB5mltziqltDTnlxLLtHxOZBWPqylwfudqYjDpiXl+hToveEw8HmUoFQmGROs26GcnSoLuSmr6I0XLZDOWaIa4dEjdhrLxLs++5ygbosJl8TecJ1Mh33vhAiMa6SqTGGKhjbCMdvBLrTOTYRSm6MydHTcgrkmv1atJHG/NMmpPmAuMOxGf5g0hOHZqXc+MOWOXRFrVmTi7XYndyAlL2JGjOQDe+h15Lm6hlS+bqxi5IJ1OtnGijog3cUcpiJMdaxi3fKu9GcropGh1aJBdX1skEgTxiMWybDyW5jUuMJ6crjdCaZ1iehJfIZK0jdaI6hzogNkwYHSvLQXh9J3vSNyQDpnjwdko7bKujWHJj0coC8r4hT4vkIj4s40N2dSagHPJ3T91UfcG0izlBMtqd5HoGmSbgDBsriGQ434xkadCt5KrHs/FFkpwcypEhrt2QZkxdBvL5pgvVdwjLJmymX5OfiPONuSXOsotYZdjiqu3GGmPsHIdqWWdIcjYdkqMm5Fe60u7VPX2Uq4UTAgd3VuqA0Us3dLZJjg8Mcv0r/7I35ShW4Ylyzla3tbo21zr1PcQ6aBN10+jq6sYuwoDDcKM8A9kcVGsauM/VLpSPb3ko0R2JIXRIrtGhRXJ5ZR2l3fgDYj42yW2t+aVITg6f+R8UT/MP2UUjbh99zdRlCTksuHlArgYu8eDtYuv48U38fS49ms7GmYTqMMmF+n0jJf5lmOS0x4saj7K/H+gdZJqAU75faEiKjmRJWEPHs/GliXPoELcbEgsGVS0QtufkdABMvta4m7C593S5gziO5VI5e7BrjLFDctQ6E2tiokNy1IQ8JjnaR7V58lfPjaTK4TM5u8wezcmJlICoVDDf9pDkcsntTbdtD0a41qnvIdZBm8hqh56rRW9RCQflwkQzGcowDbwayUVVy0OJmdUgGSa5RocdkiOmLOJPlQkx5tOakzsmydWD/TjJE0lypRnt1kqUUIVCYTPKUZfFHZLjbROr7Fc8eDvdhTab16hfiVGF5zKX8ITkIv2+Oa9PysvgAckFpot4RHI9g0wTcDLaC+zBvhXDE1lykvClQ9xuQ9KoOekhuWqQ5OKKtuu0sL1xMo07SGxj7ERyxDqfkBw1IZ8jOdJH9bsGkuSUpKWWtOTn4gV9sYJhO5bIo8ji2pFK+EtyEfFatp4kyTW+h1gHbaIesidX23nqQAc8hAuaBl6b5KioeRxeSLfrkJzR4SOSE11fvCgxn5cgOT1Oj0h5ZNTjaprcrbqMxvmN1QU622euC1u3U8OmcjM/UT9QakcZrZpxj7Tn7iO5uDFm9a3hSE6bRED9/P4k1x1kNgGnUF6sa8CskawVw9PxLOnqdIjbR3KhUX6+McnFJl0UmcLagETxwVC6suVChkiOmpDf6cqEBLFlpXe5IKbeyFAGvXNRVmQTkZjQH6nbJBe350qV7IkKu6jvodZBm6g10UFnMLskF5PwLpcPHFbKoiTXEVX4so5boyXLzRTEAMnVt7YSFlHTc5Ljk1wZqAR9LpZWyLIrvq4sCGvPFoSWmmRFTw8/cK9AUtfyuqR9O2VKgvu22U4zvIR6mCXWAahqqZwXY3WEMCMyQdZBYtpqmORCRaFB5R3Jtcdi2oBzUWifG5IjI1k7hjeWXVok1wxxB0jOpEm2JTntrhNRWnbRlXO2bXdIjho7eZeSzrPLt6Mm5DPJkT6qzDNSYbqUQhdphLoyqf49oR2mlKnMUnvTSE46CKL35bBYKTUlOdJt7a6ptG75HmIdtImszJS5eoDk+OOkTagORxp4XZIjouYBXwagOmjLremXiF1IrqZzafLUfCJuDDVBHD1dGQWB1jJP3aqysyCKAmUNYRyRIthIFOKpEsOWj9FDgFjU8/LrwtbteNuF9Y+hKMXcolwn1vmYmD9ZL2i7cEbuklwkpx9jXtYecl3zqICLP0xy3AJU1aFHJNczyCybybb6b7JRSGaZ6XRla4SoFho2JEeHuN2QmNTk0dotm+TI1xYkudzYk64BDYIgbFZCXfrTlcQ6rXep9UpeXxykTEzIZ5IjfVTUA/H+puqBwkj2Zb2iqDFz0+58CVYQmhVi4hJVVxzGoTerBKVKKMmRbtvpmoGWQPkeah20ifJGueTqAZILRSvHzaiSNHCfUvLFSI6Kavlq263JNiI6fERylbFsYj58BqAW89Akx6noEuntD6IwKuWHjOdwVS0Z/zFoBjnio7BvSx1NWmqMxR1L0rmdrOSItltHUDapN27YzUtELOiQHFPjuZi8JF8eNzwnpwQNicn4QHI9g0wTcIr/EBqkI1kS1uhbRWIdSUNy9hC3HRI3q6uEl7BnenRjk6+16vJFuybT9lVoxqriJ37/MNdBpbHtLsk11mk5szwkE8exdADEhHwmuaaPSjECNckqTDVoFkMbM7cKfkrRHqrWLGnqzuwStN0RX5q6ADXwarptu2uqfEbzObUO0kTUk1NP1T8nF2lr0HV0TQN3lRIsUYZg3q8RtSS+uu3WpNkmtuKGSM6sgaTmk/BirvIgJDcElrOeHlNZm7C39x103UuXKpXu6a5/3GTZBSP9l7wEe/LSLO97cwdBfUD/IDMwOxbkF1OKEdsjWRp06/GsiIEakqND3G5IbEKJkIfxIRlqhE2NC/ma+sQiuctSowRW9dt2V4G9GrS2rC6drcELkD7KymFJB3blLimVkZ9zrw5jKPvcWTu8Z4PfYT3NELcmKJ/MCfQ036CBLF2GQERtnskG2shlaEJnoYn57Gfwy9fx5qHKcefVSyH2e5/ZlUhuYJCph3MhrUOxBvs06JZ2EYf1oI6SHB0Qd8aOOjATS/VI0yc8a2wGjs3X1CcWyS2+2vpVbRtYPiouR3SycRkl33f7Cz3b5GANklOTb69FCbR09lQk1zvIHB4Vlo/Hop2lYc9D4k64b/+BPbqUwbaBfXpOOKqTjUDiuR/iA13PDk1cYUWmGJ97N+EwW3XhCUfvCy61j8PyBcLhl7RtYHmMKPge2clyv62vDALvjplbZ9uBo59/DCxOcrzSim4GcFjAtgHgULigCYAtSE5u/BZiMgsAAJAc8JJgJdoAAACQHAAAAACA5AAAAAAAJLch4lGFFd0EHsoaPEASr6mFPD542pZaLeMG22ov9tCI2TFN/MFbxwn6AkjuPMjHHf0TtosNO1u0M7De9k583RrQ0PvVvC33Xj6wWnWSntVe0cPmiw8m/kDHnNzn0RdAcge3iWhUD35OctElQatuHo6vutInOVaJaffUuzbJtdvrZCQ3ss+jL4DkTgWQHPACJFeNsdmXIzkAJPcSSCLRb8so4nvr53EkM9VxxPJIbDsfR4k4Z0b8Jq4Qv/ON8Msk0vsf8C+W5sfEchfiMtWX6g+jhKl9IsWjy/ovMUrwNxy9ct1qLTYKzev/xPxwg8YIqG74H3Ojz4g6e/OROBuh/opA2XPlfiBvol+X7zwvrL5+byVAbbWlbhkSyRkBa5IbtPmYeUdyjZotlfd2TCNNLA+44L2/0+epbXhM23FXRbVp5nFoeTsVyfVdDZJ7MZLTx6GF8pwsvhFxJc6J5BsZlqHYYt8cjSev0KeJBvrnylymDiaLCMnJXfpjfZ4Vf0Kpt+7Xn2Jf4G2gzhBJlBaoQpOLPIKBGAHRjfiGGqfIH8vGgvRH6uw98632lXuGK+ZNIn2su7RZpptAnpsXU/tW7WUJGBhDJzYv29M3kiOdlaq86uuYjTRRc2hOp88T2/AXRsFURZHQLKPeTp8o1adQkNyLRbzm9DAWXuroTJ56W1tzPcjjOzLmPK3DpMGLK/JYnYhb/1yPhgNhQmEd1ok78aOEyyRojIWfqp7ncu/fnH+Yy4OHa4dSsooF9X/KaOlDNoCHJHdJ8iRQJ8oJ5QTq+Mi8JEZAdVN/hZXC70sdNifTkY/k5xxho+3IA9dP3kQc5CdOP2S8rkKeDJ6zRB0Srmw1pyRHBZQ2f7FtPhedJfTsrFjaWYnKKQeajkmkyfWhkDnt88IKqIPwN44zCqYqEprLqber9CV18H5pXQ2SezHoA9PK5uDDhJycLY5M1Gd60+M7I/GzOGu6lGafyBM31XG5iRnoq5EiP4E2lwFCaG5d5eL0mk1OQAc0ycnTKsNGOfXgPJE6oEbQ6EZ6PpaXRm/mgEvro9gEdz1X7mnj5k1kuFXyQ9TUnFyZMO33Qm2rMSE5S0B1sG1p2XykD3X3iuRoZyUqb38uZLZ7cK4zO6TPSxOJSGt6ikbBtorClrer1DHPIkUbxNbVILkXQy6H8aEwAD4Kj80QVngrlbAWBq+8WK6u1iO++tv8i/wm+sSZhuTUdeYw8DyJ1EjYxHp5jHzlpiSXa3Ii1RWNTzRGQHQT1CNeOQ4JLypWS8w99Uf6bqUeL7eu3A3kTTgV5cTylZTJRZJcY6tNJNcRMLRtXneWyC//SDsrUXkScIR2x6TSxJIUYrvP99iGn2gUTIXSUjTeTrRJ0tA+vfp1SU6f9ZvPH3iWi7vsvFzV8alxrEJk+i0TCe6YKYNX/ooaP78w1l+8mFqtxrOpn+QHiXoGieRCTMrtQnJUoVUzhKVGQHQjfuQrBJjRtfZ05iNtEkzGDd0rd0vc0TcRk1OynkRFcnISR/aAxHjChuQ6Aoa2zVcqD+HZnBztrETlZoaRdkwqTSlFKrsmQm3DXxgFU6EMVRtvpzxX3ESAzdWvS3KJ+sYCA8/EevgSFTvheo6iVjQTmg0vUSKQk136S36UZmCIreUEtKkE8ouJGR0HJJJLyICxfkSeE5LLxV/KACS3C8lFPSRnjMDWTaKOE6r/MVaiv6o+inSOO9TRfPvKvXwBfROWqHOCpa3WlBzGSRmRSC63I7mOgKFt85VaM+1dJNd0VqLy9ucqemukEXGvzstaJkIdhMfQCqZCGZIz3k55rqhxg6QJQHJjSW6JdM2KJFfW2hW6pmkIyjplINM8ZqyamISGnrUIiT8pK2s7AfWduBk8UpJTn24eyeUbzBNZ8bxDcM+ejYbmv3Sb5FTz54mZkbXGti3dKB3mvSMlkxMyhQmhNyOXzpuoGpSgiedULiMyglOSswUUt6M2H+rqPK9IjnbWPpKjHdNaLFd/Q3bOton4nafsKJgKZUjOeDvqucokP+Z6wXkkl8dyZQhLzP5t8uBa8W+ZyJkq4ZX4uCau/6I8EP9M3iqJ+VVlbUU835fH7Z3g1DPKpH5I1XxZ3bLiQ4q4VBcJkmPqsXUwJO6cxPkyLsCk7EWNdZCY3h3qAmRp8KJyif+YWCSnKI1P36qeHjUkl+sCNENyek4uNj1NFaltuAJnizOwY9prouddKKejoTxZY6TTJjmjUENyxAiIbpIgbAb9IV9RGeiVAeQjXX2ZG14wV+a7rq4ibyIdtSC5UqQZNdWphL1t34EtOyE5avOS1hPP0ni0s/aRHO2YVBqZ3WU9JkIdRO7rcrlGwVSoZmZRezvtuWI1SqFXH2YT1lkkF/PBjMxoRKKDCJXnylnVBi98OZPz67w5I93p62/Un8nsX8gbjxfpRrxzha2RXiQ/F3eL9CvIi2JRssgbvlaW0EkYq1vzlwl54au5eIG4U+UY4/pN41AtIVBz6fovsoPUV4TRRfGYITnRU2LjIYL6lUkmnxcp1++tUv/17QJxK1GyLlJiPCHaGNx5Sa5iQ19ejeSIQnVZWWMERDclV3CkPUIQRU01OvlIbfdYq7tGQq9kO0/MNW+SSPHU4I3zl3z/QFVX2vYd2LITkqM2z0KxMtS3rTtJZ+0jOdoxqTSy11c9JkJsg118jeqIgolQDckZb2c8V20YAbOuDo8S1c0hOSbIhxs5H/BphxiK3prIEXcuLjMkZxoliPQIiLNeHCjWZJGgs5I+Lpf/JM0UvrxxLnxwJJ7GWz8KpH/jt2b8n5Av6mwunp+vNAnrUE/K6zyN+YvqIPz3IK5aJEfm5sWPgVVSxzPkUW4Wg4dqJU6sfCu/XBL5hmV4DcnJeFimCsW/KkBWAXb9T9zE9HFZ2RG0CLVNiG0SADxuL+NQ/9QiubIO6hm9XP1XJgbEZ7qsVX1bXMJWIDmp4LgitdONERDd5FyJgdAP4z8SF9d8pBYQmzoOcuXOJZbkTRLbVEu+pqr+SM3JJdxCxTYopPDEEtDkN4jNi9qVxLsdT5rO2ktytGNSaSq9WKxtItQ2Qm/XEbQUbJUMUW8n2yQxEpGrD5OXnRXJSTuvOYSRQXUiapOYZC5OQoTkmuSc+MbFDKYqMidnUZLc/DRIxC3p6F0GbTEnRCYau6x/rP+YX0r1EsLZNRcv6g06pNk+NKcciORzNvQN6zQPOq9Utm64YQIkaAZ2Mh4OpKNn5g8ywOYrZkVvED9EagmRGbPXoXb9SxyGOhGrfuDXBFFI/kRILuHpAT560fkC9V9hTvxrYjmX2PTM3JD/EK4Tfv5/e9eu5ToKBKWAhICAiAT0/3+56icgy74znvFrtuqc3evRA8tqoKC7oS4N2ivBeG4w4vGW64ISfuXLnVvDM5dD5Z7jpWf1+8oPLO3mXa9H/cdjTb+rtC8U43XjrReEf+VH3br6U1bt/ojkMq2NsXRbG/8QlRE1CT3ldSQ5e1F59aI4x4cITIJ8/Ncw+pFh8V5Wz1KJPUE3ahZ20SfayUyKLsJ408XA3STn82EeuYTYD8gEO8cmU3gmKck4GGbQTZzLydzYOjnXa4JO3mnAMpBcSlKXzF/Q/y16fdYxjN+d5EOC4QAA+AHJybDOsoYX4h9eRFp88sWsI/3V3gudkdzS53yp1GIkVxba43QkOZmDhYnkVv86iXnIDJFKJ5JbbKooJLdOzwbcRXI+H64cfSn9QJ9gNzK45oWWeQbdnPHIOj4512soVSHpKGUvo6XkyUfsA1d/gf3bjB79wfxuCdqC5AAA+AHJNVs30mROvndbQng+Wc8Lz6ak+6N+Z0kymO8kp9kpi1JhNpJjhprS5/g8fWcnOZ2rURe38rVN89uadcMS5eMetl8M3E9yfT682zOtwwE1C+fJBHYZi3WnGXST4DWHSIfJubAdc6MtQo6023XgFKNqQQ71F9i/Tff+5A0bmOT8bil5BckBAHA/ycmuNUn2s0mt8EB8La2sw1607EQMgbYp5QBK5u2GO8nRHbThJ/VWdCuR3Bp5hN4kG8PG8mVJdf+ONpBcWwNtFSupLeSharyxLOeaJCk6c3w8jRcDPyE5nw/vgwZ9yXJAzEJ1ovmkneZs0wx6JjmfnMtcn/6/254wuSvXkIsmkWRdQJyj16KaOQeCSc7vlqtBcgAA/ITkNtLRkI6EttF3YY7oYcikWUdBs3D2D2ue3JW8EVagDpI+rElW4ug2QExJ03ZI9B3DyvGqQ/wgPRxN9HRvcwvD8eNJl1fDssCB9UOS6/Ph/V0TWfkBMQtfJ2yl9p9m0DPJDZPzxmRmHswyJp7IjHwv2PwF/i95TG2ZCscI57srrA0AwI9Ibkwj8ryjMT/JFy+1G0lj/VwvrPeLbdxK5OTuKR2Ku72rZ981pesjSE7mSON8WHaj8gNCciHUlmVPiEibO5V5Bj2TnE/O6158jXIbq9VMM7kl1irZ9uIvsH8l8WSf/8sWi6X2u6VkkBwAAD8kuZto5Recg+U7z9WwpeOjSM62qe3zYbWuHRCSI5nIwARVYog8CRtn0DPJ+eScP0QeEsmywcMSAlo7nLq/QP9tKnzFroMWbYtk1qhkhb4AkgMA4IEk9ys7GuTvdFQguWeg3jwwLjWylQPXZ9DdAXBj2eDlVeOc3h0Bbbobs/YfjS3jQ0YIFVYB/hTJvaBGF2iJvgnSPr2qEQLmn4n8mBWldYFqJfCnSA74P08F1mUVzyHweQgUBwXJASA5ALhBcxkU97GdwWP8/iA5ACQHAMCrkWkT/Vj2f0vhbKCaYpQduWPaj9JW2DlGW8Ca5FyOsh2fbEfO64j2U7qj9v5visV2MJYrU0wYBgEfQXLtC2JCvpsWrWsq5zmXiKcBwDsgqixCXBLvJltkIxkWhloTf4xL10tQDQVeOUL3VhOd81Mk2UCiLFWVEUn/Ki4L1q0CH0JyX1koEC2KzVsWnqYjzJmR/yDOgo1LnotxBHIi2r2fLt/ur+o94/janaAVw6LHoFUipMY6abw2UpYfrqJ+WHidRiaeY64KtK+Q6MhtvNYk2x7cS8h0KqsYYql8iXAc7/hQVqRDA59Acl/RWZhI7vyGmeT+QZwpwGBP7fZG4+Tl7HRavpup4OV8fcjCElar7cmMecDDOgORtFbZxyLbnrMysSpn5k0+V9ZAEnG9wH+H/lG1rlhcwqWzd7JkLUDO3sQ4BXhnkuuymapd6Q72mve/uPbyqJuaSIyqnEkdm/Rprpe5icCm9KNSTE1LILn4C7VMVuJ0icxRjRN4Mcl9X1PKy/nykKWuoVDlEJZbV6xQeDDJdanckvpsTU8H3nEm0E44WXTsE+/JTppai6vK6mc2MQlwLiZeIhtrA8Abz+RMNrOpALpr32aWyySfhKuVxrCGYPti0GYWbR/wrZZh3gU2tRhyhcR6qZZZFilYJDLLusb1FVO6mssv5Vf/Vjn1wf3FOAIhG9roQgcdvJ0aWbfU1DW/m3sWbbDCuuDSx+0lGMkdVb1NXDwXKmQ/7D+OtgDjWQBnNSzlGc6ukn9ttvGDgp4845lJrgQLyl2QXDJh80UEAeNS95aaXTZwE0VpJTyRQW9mdZOYfg9YG7qrTU4NsD6RvX+lB/mtROi7ohb3tYH7GsS3Sc5kM1nWpqNTHwAAF8FJREFUi3ZbSquTHO1pGEeSEx2x7CQXibuCMlQX2PRimDCPapm86y8ri/HYn/7Xnt9MpMnfsuXXq8x3tpW4EqbM5XRy9QCSs4EMDWICD81t0CG7SFJt2P9W8STSybFdnXWw0vjGVRIT9lGOPvRB1dvFxQN/TVr7zjk6jFJ9pqCycQ819vr99IiaTnqR7Uf78Tx7K5+J5ArZp9T1nOTWLGA6q3u1yEsSXQg1G13cSS6srhLPKsnvM5sbRaDvvvmKq+Mh4A7hFzam4YFKqFfo7xs93H1Ri7vawJ0N4tuGMdlM/77kI3PZPXAkOeY/VTx1TU3bgskFNnsxXtNGtUyWVOXyhORe0kLYZbbd1Oz5et/7nSp65RtDfBLJ+QiEeyy2iw06nOSIwNa4HZS5+2CFNr+k38E3LpO70lW9XVw8rM3kwttAbupEmKUFH8Vx7FNL3+v3ynIxykzhg0lO2ep0JleWwZVC2ZhUGXYiq5udqsu6DSRH/+Xhjri9Cz6O5PhBf05ye+3e21lYT6dGVyIJ5z3cfVGLtyY5k83k7+NRmTsipKnX0V2pb0xJro7P2AU2ezFswKNaZnbPiKze2acH6ekTOXOZrUPIkRxrhf1tWUZG6n7L3eLm6psder2K7n8V9dWJ0rpfWPIYptTv4Lhn/7L9tYqf4EHegmkgI5Lwuwl80NFncvpmLpW5hQuzVBu9cSI5V/Xm76MKwy9HdHbLzB5UfOY8iPwUY8ucU92pJXssWD/U3MjPoG7Wsvf1eZsixuKQ3X/GEFsuPiLm0XLRI+ar7Z7dMWDtFergyrXvSvuH2h2/v0ly+ZTkmlBW0tjFIjN+0aeQUzwPH0hOCtIRkOyqnd7CaTmRnL1q9cffeueD631IPHCDHuwzpiJ4Cb1m7V/guQ7+5WrVPPYn1iGEdCzl2+O4LC08nzznHEmoGkWwTscrr4YjPGqRCHl+JJkT5tw7Rr1Vf7e+EW4Z8nt7MxnawBAgeQrJqWym9HCp1OIkxxZbtiPJxXiF5Ir+vxfDr/yolrlPEVV/UwcYNYfl2Q3EuuHS+s8LlC29mz5wi+bMa4k6+s80V9/BoeeynuarYz8ehzPtwrwE6mw0TGnfQQuPhi9zVfYYHkdyNgKxQUwfdDjJmQT7rMxtgxWpmVQL1mnAO6t6m7i4khyVZPs82+vkyeCSc14eG5Jtg7HdRJHstkinbsYgBXNzs9KILPaT3SHbOPLs3topaK1W9kK6Z3cIWK/Rq/vsyrXv4ruqP+kvuitTPI/J0clA5hWJQT6dXKZi/1Er/86R5DZ2XWeWnVBSDO9GcvaqzR9/450PrvdeH7pBtejBPpJdyq5wzSnoNWvdK4TlOgzBALbqaP5tsw6B+onVpK/usboFmUo5ec5DJCEE6d2l0xkqr4UjpBtIFKqQzmnM1GjSJewdY+COUcRK9sPyu+Wi/fOSV7nAm0lvA2OA5Ckkp7KZ9H0xbN2rJK487vw52MYxOf2F5q5kS5rDV806FiM5yge1TGVDi9vx7nrPVn8e6bmTHNXbFrMSPM9jIq941ebrrr4rDr0sg1s5JeFMu5AKK1Ebn39HliyeOrgrQ+y98iNIzkcg0htQ+okNOi5ITuqs/TobrHSSWy5JrmuCq7j4Jckxd+akjnIaJCyPHeOMmS3uTuXGRv73IqOSIsbobtYiyr95dMHL1Caokut6EbQ2K1sh3bPrAWt5ALXv5MotFvHmu/qT/hLJ8WBmLacxOQ1Rh6JDOen1beYdLLdkIjmd3Pl9b7IUZCA5f9WeBHD9nXfX+1Afhma7bQf7uPvBcgrmmuW5DkMwoFs1X7grqZ8w8rnH6jHOk7r5OedIwn6oiQx1HLu/Ho4o/hbTWudHalJ4lXQMaj+d5Jbu8shar9axmXgbmAIkTyE5k83kcGNpeyswkltpfaisCS08WOElo5E7Z3nk/fR+Q3KvcM3rVMxKsZCjWub+auh8VInMOKhxPg1jwKV3UWnqFrkKSA8whmfSdYee1LQqUmsHz1+oh1hBsR6vydxws7FjH2c8guR8BJJtzuiDjguS0/qbhnHBOpCczjqXcTDpqt4uLn5JcpEb2j7TM19ne6z5xXwx0h5X7k7l+DIZStwJIQ1vnd2sfJefHEkuy+8crOw1aAj8UyHu2e0B62nThcmVK4PxGO0b0t2hpatDu5t9541Mt9v32Q9+t5icveqeBHD9nXfX+1AfZoMe7BN8fi8voNesYM2lsr09GJC3sz03emHcq95p9SGod/Kch0hCWvVHTiTXwxFOctxHzI8Uw5COUa1DkZlct0HTps3REGsmQxsYAiTPITl57/R9bZ/ArslJrtBf0istid2VMcmwzUhuf12DmAfl95SpGB4NXqhlzhKZLbxgX6DzmZy4Z9idp7xTF98LSdyt4uq74tCz9c3ix/NwJv1ZggTj1NL6HVKXJpKjmhkfFMefBjI8iOFkDBt0XJBc3SdvabVfZ4MVJzm6cS9h8ao9qnq7uPglye3j1lRk4yjtKYhqQ36ssSOvX3F3qrbNqhtTLTYxdTdrWXzXKvdX9sST/faDlemkW1kLGcYDHrBOw9Ky6d2I1fOqd3XHL3Cnu1JfdU8CuP7Ou+t9qg9uUB+/uX2skVpOwVyzPNdhCgZss/kHkktGBHdafQhwnDznIZKwaQBmJrkxHKHBSb54fiSq7/sjawdqnH4kOePaNDST3gaO7+QJJDd1g3WqLfVikHahhVnP/uhXyXKqC03NWSLzBRFr5aOS2oHkykK70XaSy+x+M95XV9/BoeeekDQ2tOlC+qrVVlX4d5yQHG2K+6gkvGkgk9ds247ooOOC5Eh0M5T+q2Sw0tsOlRXcXTmreru4+AnJNeb4pbojcG8C5YHduRl7/8buTnWSW9VbKz/E3axCcnbyjORmK8tgWKxshRwmvfr/HG1YPHe4ltYld/mTAt8jubodXnVPArj+zrvr/VAftNlu29E+fSQqOQVzzfJchykYsM3mPyG5e63uq1Xz2XPOkQRezX9JcmM4Qra0kiHo4ZHWXDhZ7MskZ81kTNo4vJPnktzJa/ubiNIW4hhy9Jomk1s2MfOXuXnc1Tc79Kq7K2WvdzVnWge+IJrkw7JEbevrDY8kR7tNPPSnD+OU+hV/VG+Std0oazuqet90jtW+vvKpxnZ3qpOcDkGswrubld0sfrI7gtrs5hVftNYgs7IV0rsZD1jXMbFo6nDFXapLVocnBb43kMvmMatHf/yNd95d71N98GZrTrijfXpOwVyzPNdhCgZss/lPSO5eq1fxNkincvGccyRhr6oyFBCS08o7hSNkuC3rEQ6PlEL0jpHd/ZKldJXkejPpbWB+JyC5R4FcZiVqGrSEHHUml1sJ5sXjqGO17thdfZ27llRL6FP5VKtGHSWc2ad8Ubx4HKb07+gkp1/Gf76NiyqHfbD1uHWM7anGzppGqO5UJ7m2alBY3r67Wfehy3CyO2Sd5LqVew2Ks6926GYsYJ2pDi3ppMNt+6y5Jl+y6k8KfHNEszfsJBld+qo9CeD6O++u96k+eLPlic1kHyM5yymYa9bme4aOwYDhmTz5hPMWnOTutjp5h2SPmpPnnCMJmWuoPlqvvGM4QjID2It1fCRNSNrPVn6ntAg1rddJbmwm2gYO7+SFJNf+9iCSXWaSTmYhx6DriZaFbVKDBGn7PgLu6usOPYozZo/6kq/Ooo6SvzTE5ERhm+uIfUcnudo5L73N/hEtLOuy5D9lbDOEkxy9+sX2qhvcrBt3eXWMGEsMuS/yiMeg9dFXO3SHFrD2gPRFh8vfRQ+pTd+rDPAtUIt1s8ir/vc7H1zvY32YbDDbx0jOcwqmmrX1HJMhGDA8ilcq7hAGJ9K9Vuc8V44fXj7nFElgx1PtPZxV3jEcwQTPcbNw8Uhh9dcctcMM+QbJ9WbibeDwTl5Hcn8f7n07TCnK7MQbnXStnjnezsqsVz179dwhUbs/4I1eUf4rG8zXL9uwu1nbxcnWzm9sx8LrjS+pX/EkX3lS4Cu2rve88zM3/sEGVwxS/2GvevnX2L3X7ZesXq8XMUcSDs9y0XndrJc+pK9fdsmcmOFHS2NQxT980lEWqOwBwN9G+dSOuubXe9BBch+O8Eb7AAIA8BDkD10a0t4hfAGS+3BUuKgAAHjX/ukNwhcgOQAAAODPAiQHAAAAgOQAAAAAACT3J9F+XfyqYkXT/8fYMP6n4L5t0f4v9kzhI/NfQHJfq/u/niM0rW4r/yg9Ibvko419RAx4y++J+3rxP7HbUy7/Op6WdH/LeGEfBpL74uj+9wdF1/44sxKWwn20sUFyILn3/+nxX8fDT1YrvbAPA8l9eRwyi7S7pL2quKtm+6XcuzaDlPpAieThk2zOzNeY3LxddJC2V8l74GXGNkOJaapILI3mVYvxoFeNz4PeK3UmhrGEkhPcly+cuCedZJA9mORGIx8mIFYR3JR8cRcCpsPWF3CJ1WrBscq8/mfLsyWt8Yk7obz/8LPjNPTTT/brcilWb0+bR9LekS+3PszeGkjuDcf2vD/bINLelrhElXMnFXfTbB/l3uPaFdFGkXqSm1/jICCvcvN2UVlX3gHVdORV8h54lbHdUGJm2nt+Ni9ZdImyubpIK+uBq3VmKGG/OUIG7mWI1jTpQ6BNE93I/VyfsklF2E25X0Py3P1irjlxvyGFoURREQkhHqrMu/xs3w43snBduHJ8YyE86qasp+NL8/RWTt5cpHe6127tw9IS4/NfAUjuG/3eINLeFpdzVzUv0Wzvcu9UD3zvVdqFOakijsrNDwLy4q70i+ivFkcdebgrX2rsbqhFhHG3g3nFosUlVFrMolR2rc6EoQRyApV32LZGFdSu7rR+IyHnp5kX7WURG29kST6kwci9AfocXytCE92JLA2/Lp3ktAwZwxb5X15ESqm9j6N6eDbzJYqQjhyvx+PdXWk93RbWengrJ28uioiAFsUXPX/zFpDcN/q9QaS9cb2towZLF0BnuXeREBjcE0nftUrJDwLyQ0yOLjJZga4jD5J7qbHdUGJPOjyZVy3Kk4A0aIzcrDNeQghvMkvXDrhc22zwRkLOt4JSc5pVTdszcn2u/mhrZNKXL6ORewOcHpjHK1k6/X6x1Bwb7XStt9UU2y57hFfauj/bTHJXjjvJ9Z5uDl/OzcPfHPNb8YG6azuA5N6139u6IESblW5ds73LvQt0hJtZ27oN9SsOAvLaS9pFdZ/708i268iD5F5qbDeUmHk/N5s3yh+RP0pEgk2Zb9SZXkIJbxJy/RfJ3UjI+RbJzWlW0jO+7Pd7IxM5+HUw8tAAOyNLRXBVmn6x1Rx5f7ShspRIF7Eiz9wjvBjzsw0kZxLg10iu93Sd5C6aR39zfQrAfVhezcUJkvsEkst6WOW/vb2a3DsrCKrwTF5SqTaIk74kDgLy0uyHi2oOe73oOvIgudeSnBlKzMwkN5o3rvwHu62Z6kiRksc61+vMWEJJ6/oGv3siuTykEjg7HRNyPE1B67/cw8kG1dMstISaqczN06z2cqiMEjkvXbIULE9Dr+xP8cDe3hqZ9NnrYOShAfrVWhE6yS3XSW4mjLnKvBrXyEwYzav6Ccl5T9dJ7qJ59Dd3IDkayT9/R3mQ3L0kFzdRuGczds32Se7ddk9WIjTG49yUQUBe5ebtIr5LozuiJAeSe6mx3VBVvFTLwbw6uuU7ikftyyXJjXXGS6Bur77DAH8kOcqMon8tpaa/lzEhZ0pT8MQKSjYgkd+wsEzwkIOj2TmUv1BXuZrG9qos2wvTK/0pHu63M+1SEgjtRu4N0KYsVhGc5Kpqep6RHJfYXNx4qjJvYevGAuVFvcVCcvIWLo47yfWeruvEXTaP+a06ybWyvWK9BUjuXpIzOfdoovWi2T7IvbdqgbdEIvarklzdhzmVte5NQF7k5v2iGErLXM9MR54l7xMWV71qJueGouWwe998MG9b3WKZw+37v62ES5Ib64yXEKJY+eX2HUhOEgfykFKznSXkTGkKnlgROZN0P0q9n5eQpaJX81vEZkGeZkWPeRp5eIpHwhsZfdhtNhrZzzWbzVpF6CLa1PDTckpyUiKnXG9jj/AODdmfbVtCzpIIue7DrX1wcXrcSa73dH0md9k8/M11kqM+bB9D7O3i6a8AJHe3uzJoFEbs5Zrto9y7pRQ0PmsxZ5KHj6OAvMjN+0VNI3VdR56JMy5YK/ciknNDbTmEpOe6ebeqFmPWW2W2suQTd+VYZ6wEs/LL7TuQnCWC9JSa7SwhZ0pT8MQKLidqhMdLYMbqucTSLMJAckMuRLVZxhM8GN7I9jnlbqDJyHaueGxdK0InOWq14dxdybeH6nMXM/hbNGR7tm0fVwd+0ZUoLaR4eryTnPd0neROmodXaic57sPyS14BSO7+jvDgebhwRIxSSrOQvJ/xDyw33y5k3/0Ilsm9FvUyTXZWyjoaqHyhzlgJ7T2sO7ork6yOGlJqzsh/jOz0xIqJ5LyEHuQRkkuBOryB5E5yIZKt0XpsWz5rZGpkOTd42C710W5Z73B1be9YpdtQh4nAz47f7unOmsfla6mvegUguftJDvjfgXr1usa/WWfiquNyyX2KnAHuKTWnJDekKfTEipnkrIQDycUllZZGkjvJhdCneLmRU/w/VfAXNQ+QHEgOeAPkZVmXe9e1vXudyfJ8vLilyIeeUnNOcpamkMdcnInkvISB5OjQmvSrijgnyzFPg9bhlO0Vy6cvjZz+N4lfPyO5HzUPkNzboSBA9r8c3JQf7L335nWmBUqMiZLgWFqhBAtLqblCcpamkMdcnInkvIROcpxmFULdD/O+Ialy0Yc8Db5FnuKjjPzZ+GH253u+OZAcAADSRVEALch0juJorafUXCE5S1OYcnEmkvMSOslxmlWliyUBZ1k0Y2PK0+DEPn0KAADJAQDwGyP5y2yDG0PzMU1hvnsu9IJN2zRpaNfvR8IVAJIDAOBFCBBPAEByAACA5AAAJAcAwIehwpsIgOQAAAAAACQHAAAAACA5AAAeifKFSJvrz9CC7fTdRe7t1opBrEAFQHIAADwO6QvqNqvtAcIi5//eyKnOMtI39n7BXkIASA4AgEfiC1OpieS+cH2Ztp0HyQEgOQAAngvxUu7/51mXCnfLGZED31TAO5OaXFYZb5bGK3KD7/CYqp7d9EbV/3aV8KWYbrjJgfs5JjkWcCs5ge8AkBwAAL8CFbkOvK2WCXfrbC2IVLfMskh7YDUZ76iKO2VZwxKaTsbi/sda+1HR/3aV8LpEKp5JUsvxc/QdTbawDHHBQjwAJAcAwK8gxK4C4MLdTHILs1geSY4+JZV+JpJbmbtMXTWIOAEfbavpf7tKeF3WJhoGLgeu5/g7mONENjzCLABIDgCA34BICTSXCG2mchP5QIzTTI57j2wkx4I5tpEl63/TZXKUymOSc5VwJkBRETc5cN70Wb+DOY50CmATACQHAMAvgbgpRpMUUOFuph6R8l5HkkvKV0pyIhvnBRUhLBWT26mOSc5VwsUzOsmBC9tl+g6VPShhCQnLCQCQHAAAv4MY26Lypi7czceZq/a/jiS3dpJbTklu2XTGJiRnKuFCcnWUA1cipe+ISYOBJa0rrAKA5AAA+BWUhadeLvPd3ZVVuY3mWI3SQ9agLKUkJ7SVy0xycpQ8key4dJVwCd4xmXY58CbfwSLhnLTSbq81AACQHAAA38Hq5OPC3UJyodRIfBPWXDgHchUZ755duc/SJglxkU3dj1Y+KvrfphJelzXvxzmdReXAW9i/g6ZwnF1JHBpirXFBXA4AyQEA8DsQP6EuIVDhbp7RpWXh6FkN+7/srsyhy4Czz5ECbrpQrpMcHRXPJul/u0r4PkFbF9knxeTA+RypkotI+H6SzqyYyAEgOQAAHoBBRoeIzP7qmSAXMt7nk66D/vel4LiX044FNEzjAJAcAACPhs3oAAAkBwAASA4AQHIAAHwKIAAOgOQAAAAAACQHAAAAACA5AAAAAADJAQAAAABIDgAAAADJAQAAAABIDgAAAABAcgAAAAAAkgMAAAAAkBwAAAAAgOQAAAAAkBwAAAAAgOQAAAAAACQHAAAAACA5AAAAAADJAQAAAABIDgAAAABAcgAAAMBfx3+oEHjenwWZZgAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "6b5e8e8f",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________\n",
    "\n",
    "# Advanced CNN using Roboflow\n",
    "\n",
    "Read the web page 'How to Train YOLOv8 Object Detection on a Custom Dataset' \n",
    "\n",
    "> https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/\n",
    "\n",
    "Then create your own Roboflow project, with some data of your choice found on the site using\n",
    "\n",
    "> https://universe.roboflow.com/\n",
    "\n",
    "and clone an existing project. The Roboflow 'pipeline'<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) includes many steps\n",
    "\n",
    "![roboflow-2.png](attachment:roboflow-2.png)\n",
    "\n",
    "here you only want to look into the _process_ and _train_ steps and take a peek into the _collect_, _organize_ and _labelling_ step, which is normally the most time-consuming part of any ML project.\n",
    "\n",
    "The current computer vision/ML projects ohn Roboflow include object detection, classification, instance segmentation and semantic segmentation, and you are free to choose between the different computer vision-related concepts, but object detection and classification would probably be the best choice for this journal (instance and semantic segmentation are highly complicated processes.)\n",
    "\n",
    "Train and test a model and make documentation of the process for the journal, using images, and learning graphs. etc. from the site. \n",
    "\n",
    "(Notice, that there is no obvious way of getting hands-on any code behind the 'pipeline' in Roboflow, if, say you want to elaborate on the Yolov models on your own computer.)\n",
    "\n",
    "<a name=\"cite_note-1\"></a>[1][^](#cite_ref-1)  <span style=\"font-family:'Courier New'\">https://2486075003-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-M6S9nPJhEX9FYH6clfW%2Fuploads%2FfHpPTWNdCVR9qHQDeskF%2FScreen%20Shot%202022-08-24%20at%2012.35.36%20PM.png?alt=media&token=623927fe-3099-4ccd-8aaa-890bf5c0b03b</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5db2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: goto Roboflow, and create a CNN project..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcff2e6",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-\n",
    "2023-04-10| CEF, initial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc_position": {
   "height": "616px",
   "left": "0px",
   "right": "20px",
   "top": "106px",
   "width": "213px"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "fdca30083b677063cb48173af8214056b55c1526d8cb4ee9ed2f266c85962ab6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
